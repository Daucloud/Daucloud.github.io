<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>高代选讲期末重点整理</title>
      <link href="/2024/06/18/%E9%AB%98%E4%BB%A3%E9%80%89%E8%AE%B2%E6%9C%9F%E6%9C%AB%E9%87%8D%E7%82%B9%E6%95%B4%E7%90%86/"/>
      <url>/2024/06/18/%E9%AB%98%E4%BB%A3%E9%80%89%E8%AE%B2%E6%9C%9F%E6%9C%AB%E9%87%8D%E7%82%B9%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<blockquote><p>此文为宗正宇老师在最后一节课所划六个重点的整理，权作期末预习(x)</p></blockquote><h1 id="求特征多项式和极小多项式"><a href="#求特征多项式和极小多项式" class="headerlink" title="求特征多项式和极小多项式"></a>求特征多项式和极小多项式</h1><h2 id="特征多项式"><a href="#特征多项式" class="headerlink" title="特征多项式"></a>特征多项式</h2><p>$f(\lambda)=\left|\lambda I-A\right|$</p><h2 id="极小多项式"><a href="#极小多项式" class="headerlink" title="极小多项式"></a>极小多项式</h2><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法 1"></a>方法 1</h3><p><strong>定理 7.4</strong></p><script type="math/tex; mode=display">极小多项式m(\lambda)和特征多项式f(\lambda)含有完全相同的根集和不可约因子集</script><p>一般来说考试涉及的矩阵并不复杂，维数也较小，因此可以根据定理 7.4，从小到大尝试$f(\lambda)$的因式$a(\lambda)$是否满足$a(A)=O$以确定最小多项式</p><h3 id="方法-2"><a href="#方法-2" class="headerlink" title="方法 2"></a>方法 2</h3><p>求出矩阵$A$的 Jordan 标准型，则$(\lambda-\lambda_i)$项在$m(\lambda)$中的重数即为特征值为$\lambda_i$的 Jordan 块的最大阶数；对于广义 Jordan 块，可以直接取为相应的不可约因式</p><h1 id="计算复方阵的-Jordan-标准型及可逆矩阵-P-s-t-P-1-AP-J"><a href="#计算复方阵的-Jordan-标准型及可逆矩阵-P-s-t-P-1-AP-J" class="headerlink" title="计算复方阵的 Jordan 标准型及可逆矩阵$P,s.t.P^{-1}AP=J$"></a>计算复方阵的 Jordan 标准型及可逆矩阵$P,s.t.P^{-1}AP=J$</h1><h2 id="Jordan-标准型计算"><a href="#Jordan-标准型计算" class="headerlink" title="Jordan 标准型计算"></a>Jordan 标准型计算</h2><h3 id="方法-1-1"><a href="#方法-1-1" class="headerlink" title="方法 1"></a>方法 1</h3><ol><li>求出特征多项式$f(\lambda)$\</li><li>则$\lambda_i$的个数为$r_i=n-rank(A-\lambda_iI)$，也即$f(\lambda)$中$(\lambda-\lambda_i)$的次数</li><li>阶数为$t$的 Jordan 块的个数为$r_i(t)=rank(A-\lambda_iI)^{t+1}+rank(A-\lambda_iI)^{t-1}-2rank(A-\lambda_iI)^t$<blockquote><p>解释：注意到$rank(A-\lambda_iI)^m$中仅含有原本阶数大于$m$的$\lambda_i$的 Jordan 块，故$rank(A-\lambda_iI)^m-rank(A-\lambda_iI)^{m+1}$恰好为阶数大于$m$的$\lambda_i$的 Jordan 块的个数，因此$\left(rank(A-\lambda_iI)^{m-1}-rank(A-\lambda_iI)^m\right)-\left(rank(A-\lambda_iI)^m-rank(A-\lambda_iI)^{m+1}\right)$为阶数大于$m-1$的$\lambda_i$的 Jordan 块的个数和阶数大于$m$的 Jordan 块的个数之差，换言之即为阶数为$m$的 Jordan 块的个数</p></blockquote></li></ol><ul><li>一些 tricks:<ol><li>由于考试涉及的 Jordan 块通常维数不大，而$m(\lambda_i)$则确定了各$\lambda_i$的 Jordan 块的最大阶数，通过此往往就可求出 Jordan 标准型</li><li>可以通过计算$dim Ker(\lambda_iI-A)$来获知$\lambda_i$的 Jordan 块的个数（$Ker(\lambda_iI-A)$中的每个向量都可以作为一条 Jordan 链的终结）</li></ol></li></ul><h3 id="方法-2-1"><a href="#方法-2-1" class="headerlink" title="方法 2"></a>方法 2</h3><p>求出$A$的初等因子组，则 Jordan 标准型为</p><script type="math/tex; mode=display">J=\begin{bmatrix}J\left(p_1^{k_1}\right)&&\\&\ddots&\\&&J\left(p_s^{k_s}\right)\end{bmatrix}</script><blockquote><p>注：该方阵称为第三种相似标准型，除此之外，还有第一类相似标准型（有理标准型）和第二类相似标准型（初等因子友阵型）:</p></blockquote><script type="math/tex; mode=display">C=\begin{bmatrix}C\left(d_1\right)&&\\&\ddots&\\&&C\left(d_r\right)\end{bmatrix}\\G=\begin{bmatrix}C\left(p_1^{k_1}\right)&&\\&\ddots&\\&&C\left(p_s^{k_s}\right)\end{bmatrix}</script><h2 id="可逆矩阵-P-的求取"><a href="#可逆矩阵-P-的求取" class="headerlink" title="可逆矩阵 P 的求取"></a>可逆矩阵 P 的求取</h2><h3 id="方法-1-2"><a href="#方法-1-2" class="headerlink" title="方法 1"></a>方法 1</h3><ol><li>求出$Ker(A-\lambda_iI)$，其中的向量可以作为$\lambda_i$的 Jordan 链的终结</li><li>但是需要注意，并不是每个向量都可以形成阶数正确的 Jordan 块。譬如，设$Ker\left(A-\lambda_iI\right)$由$\alpha_1$和$\alpha_2$张成。如果一个 Jordan 块为二阶，那么我们需要设此 Jordan 链的终结为$s\alpha_1+t\alpha_2$，并确保$\left(A-\lambda_iI\right)x=s\alpha_1+t\alpha_2$有解（通过考虑增广矩阵$\begin{bmatrix}A&amp;s\alpha_1+t\alpha_2\end{bmatrix}$）</li></ol><blockquote><p>举一道例题，便于理解<br><em>问：求此复数域上方阵的 Jordan 标准型和可逆矩阵 P：</em></p><script type="math/tex; mode=display">\begin{bmatrix}1&-3& 0&3\\-2&-6& 0&13\\0&-3& 1&3\\-1&-4& 0&8\\\end{bmatrix}</script><p><em>解:</em></p><p>$$f(\lambda)=(\lambda-1)^4,m(\lambda)=(\lambda-1)^3,故 Jordan 标准型为\begin{bmatrix}</p></blockquote><pre><code>1&amp;0&amp;0&amp;0\\1&amp;1&amp;0&amp;0\\0&amp;1&amp;1&amp;0\\0&amp;0&amp;0&amp;1\\</code></pre><p>\end{bmatrix}$$</p><blockquote><p>$$Ker(A-I)=span\left(\begin{bmatrix}</p></blockquote><pre><code>3\\1\\0\\1</code></pre><p>\end{bmatrix},\begin{bmatrix}<br>0\0\1\0<br>\end{bmatrix}\right)$$</p><blockquote><p>$$取\alpha_4=\begin{bmatrix}</p></blockquote><pre><code>3\\1\\0\\1</code></pre><p>\end{bmatrix}，设\alpha_3=\begin{bmatrix}<br>3s\s\t\s<br>\end{bmatrix}$$</p><blockquote><p>$$欲使(A-I)\alpha_2=\alpha_3 有解，考虑增广矩阵\begin{bmatrix}</p></blockquote><pre><code>0&amp;-3&amp;0&amp;3&amp;3s\\-2&amp;-7&amp;0&amp;13&amp;s\\0&amp;-3&amp;0&amp;3&amp;t\\-1&amp;-4&amp;0&amp;7&amp;s\\</code></pre><p>\end{bmatrix}，可知 3s=t$$</p><blockquote><p>$$不妨取 s=1，故\alpha_3=\begin{bmatrix}</p></blockquote><pre><code>3\\1\\3\\1</code></pre><p>\end{bmatrix}，并解得\alpha_2=\begin{bmatrix}<br>3u+3\v-1\0\0<br>\end{bmatrix}$$</p><blockquote><script type="math/tex; mode=display">再令(A-I)\alpha_1=\alpha_2有解</script><script type="math/tex; mode=display">考虑增广矩阵\begin{bmatrix}</script></blockquote><pre><code>0&amp;-3&amp;0&amp;3&amp;3u+3\\-2&amp;-7&amp;0&amp;13&amp;u-1\\0&amp;-3&amp;0&amp;3&amp;v\\-1&amp;-4&amp;0&amp;7&amp;w\\</code></pre><p>\end{bmatrix}，因此有 3u+3=v,2w-\frac{v}{3}=u-1$$</p><blockquote><script type="math/tex; mode=display">不妨取u=1,v=6,w=1故有\alpha_2=\begin{bmatrix}</script></blockquote><pre><code>6\\0\\6\\1</code></pre><p>\end{bmatrix},\alpha_1=\begin{bmatrix}<br>7\-2\0\0<br>\end{bmatrix}$$</p><blockquote><p>$$故知 P=\begin{bmatrix}</p></blockquote><pre><code>7&amp;6&amp;3&amp;3\\-2&amp;0&amp;1&amp;1\\0&amp;6&amp;3&amp;0\\0&amp;1&amp;1&amp;1</code></pre><p>\end{bmatrix}$$</p><h3 id="方法-2-2"><a href="#方法-2-2" class="headerlink" title="方法 2"></a>方法 2</h3><p>由于$\lambda I-A$和$\lambda I-J$有相同的 Smith 标准型，而 Smith 标准型有固定的算法求解，因此可以求$P_1^{-1}\left(\lambda I-A\right)P_1=P_2^{-1}\left(\lambda I-J\right)P_2=S$，则$P=P_2P_1^{-1}$</p><h1 id="lambda-矩阵对角化；初等因子、不变因子和-Jordan-标准型的关系"><a href="#lambda-矩阵对角化；初等因子、不变因子和-Jordan-标准型的关系" class="headerlink" title="$\lambda-$矩阵对角化；初等因子、不变因子和 Jordan 标准型的关系"></a>$\lambda-$矩阵对角化；初等因子、不变因子和 Jordan 标准型的关系</h1><h2 id="lambda-矩阵对角化"><a href="#lambda-矩阵对角化" class="headerlink" title="$\lambda-$矩阵对角化"></a>$\lambda-$矩阵对角化</h2><p>课本有详尽的算法。简而言之即是不断降低$a_{11}$的次数<br><img src="https://img.picgo.net/2024/06/18/2024061812322236777062ff2476a40.jpg" alt="3663133d2ba182b3d2aca82db4134e2.jpg"><br><img src="https://img.picgo.net/2024/06/18/202406181233913c52ac3b6584e7b8d.jpg" alt="9b55172bde8fb4579a9083312526854.jpg"></p><h2 id="初等因子、不变因子与-Jordan-标准型的关系"><a href="#初等因子、不变因子与-Jordan-标准型的关系" class="headerlink" title="初等因子、不变因子与 Jordan 标准型的关系"></a>初等因子、不变因子与 Jordan 标准型的关系</h2><h3 id="三种基本因子的概念"><a href="#三种基本因子的概念" class="headerlink" title="三种基本因子的概念"></a>三种基本因子的概念</h3><ul><li>对于$\lambda-$矩阵 1. 不变因子：Smith 标准型对角线上的所有非零元素：$d_1(\lambda),…,d_r(\lambda)$ 2. 行列式因子：所有 k 阶非零子行列式的首 1 最大公因子：$D_1,…,D_r$ 3. 初等因子：不变因子的准素因子全体（1 不计入）<blockquote><p>不变因子和行列式因子互相决定：$D<em>k=\prod</em>{i=1}^kd<em>i,d_k=\frac{D_k}{D</em>{k-1}}$</p></blockquote></li><li>对于数字矩阵，其三种基本因子指的是$\lambda I-A$的三种基本因子（但是 1 均不计入）</li></ul><h3 id="从初等因子求-Jordan-标准型"><a href="#从初等因子求-Jordan-标准型" class="headerlink" title="从初等因子求 Jordan 标准型"></a>从初等因子求 Jordan 标准型</h3><script type="math/tex; mode=display">J=\begin{bmatrix}J\left(p_1^{k_1}\right)&&\\&\ddots&\\&&J\left(p_s^{k_s}\right)\end{bmatrix}</script><h1 id="Gram-Schmidt-正交化"><a href="#Gram-Schmidt-正交化" class="headerlink" title="Gram-Schmidt 正交化"></a>Gram-Schmidt 正交化</h1><p>设$A=\begin{bmatrix}v_1&amp;v_2&amp;\cdots&amp;v_n\end{bmatrix}$<br>则其正交化结果$Q=\begin{bmatrix}w_1&amp;w_2&amp;\cdots&amp;w_n\end{bmatrix}$<br>其中：</p><script type="math/tex; mode=display">\begin{aligned}&w_1=v_1\\&w_2=v_2-\frac{\left<w_1,v_2\right>}{\left<w_1,w_1\right>}w_1\\&w_3=v_3-\frac{\left<w_1,v_3\right>}{\left<w_1,w_1\right>}w_1-\frac{\left<w_2,v_3\right>}{\left<w_2,w_2\right>}w_2\\&\vdots\\&w_n=v_n-\frac{\left<w_1,v_n\right>}{\left<w_1,w_1\right>}w_1-\frac{\left<w_2,v_n\right>}{\left<w_2,w_2\right>}w_2-\cdots-\frac{\left<w_n,v_n\right>}{\left<w_n,w_n\right>}w_n\\\end{aligned}</script><p>上学期的内容，不过多赘述</p><h1 id="规范方阵谱分解"><a href="#规范方阵谱分解" class="headerlink" title="规范方阵谱分解"></a>规范方阵谱分解</h1><ol><li>求出规范方阵 A 的特征多项式，并解出特征值则$U^{-1}AU=diag(\lambda_{1},\cdots,\lambda_n)$</li><li>求出$Ker(\lambda_iI-A)$的一组基，对其施行 Gram-Schimidt 正交化</li><li>将上一步得到的所有向量排列起来，即得到酉方阵$U$</li></ol><h1 id="Jordan-标准型的应用和可对角化条件"><a href="#Jordan-标准型的应用和可对角化条件" class="headerlink" title="Jordan 标准型的应用和可对角化条件"></a>Jordan 标准型的应用和可对角化条件</h1><p>设 A 为 n 阶复方阵，则如下命题等价：</p><ol><li>A 在$\mathbb{C}$可对角化</li><li>A 的几何重数为 n（有 n 个线性无关的特征向量）</li><li>A 在复数域上的初等因子均为 1 次（即初等因子均无重根）</li><li>A 的不变因子均无重根</li><li>A 的极小多项式无重根</li><li>$\forall c\in\mathbb{C},rank(cI-A)=rank\left((cI-A)^2\right)$<br>此外，以上命题的一个充分条件是$f(\lambda)$无重根</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Chapter2 Using 🤗 Transformers</title>
      <link href="/2024/05/14/NLP-Course-of-Hugging-Face/"/>
      <url>/2024/05/14/NLP-Course-of-Hugging-Face/</url>
      
        <content type="html"><![CDATA[<h1 id="Behind-the-pipeline"><a href="#Behind-the-pipeline" class="headerlink" title="Behind the pipeline"></a>Behind the pipeline</h1><ul><li><code>pipeline()</code> groups the “preprocessing-pass the inputs through the model-postprocessing” together<br><img src="https://cdn.jsdelivr.net/gh/Daucloud/imagecdn/test/202404291430930.png" alt="|525"><h2 id="Preprocessing-with-a-tokenizer"><a href="#Preprocessing-with-a-tokenizer" class="headerlink" title="Preprocessing with a tokenizer"></a>Preprocessing with a tokenizer</h2></li><li>convert the raw context into vectors with a tokenizer:<ol><li>splitting the input into tokens(words, subwords, symbols like punctuation)</li><li>each token to an integer</li><li>adding additional inputs that maybe useful to the model<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">checkpoint=<span class="string">"checkoutName"</span></span><br><span class="line">tokenizer=AutoTokenizer.from_pretrained(checkpoint)<span class="comment"># all the preprocessing needs to be done exactly the same way as when the model was pretrained</span></span><br><span class="line">raw_input=<span class="string">"context"</span></span><br><span class="line">inputs=tokenizer(raw_inputs,padding=<span class="literal">True</span>,truncation=<span class="literal">True</span>,return_tensors=<span class="string">"pt"</span>)</span><br><span class="line"><span class="comment"># as for the return_tensors, pt: Pytorch, tf: TensorFlow, np: Numpy, jax: JAX</span></span><br><span class="line">print(inputs)</span><br><span class="line"><span class="comment">### the results, is a dictionary, which contains two key-value pair</span></span><br><span class="line">&#123;</span><br><span class="line"> <span class="string">"input_ids"</span>: tensor([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]), <span class="comment"># the unique identifiers for each token</span></span><br><span class="line"> <span class="string">"attention_mask"</span>: tensor([<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Going-through-the-model"><a href="#Going-through-the-model" class="headerlink" title="Going through the model"></a>Going through the model</h2></li></ol></li><li>convert input IDs into logits<br><img src="https://raw.githubusercontent.com/Daucloud/imagecdn/main/test/202405140923963.png" alt="image.png"></li><li>🤗 provides the <code>AutoModel</code> class, which corresponds to the hidden states step<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel</span><br><span class="line">checkout=<span class="string">"checkoutname"</span></span><br><span class="line">model=AutoModel.from_pretrained(checkout)</span><br><span class="line">outputs=(**inputs)</span><br><span class="line"><span class="comment"># the Automodel converts the inputs(seen last part) into a three-dimensional vector:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">1. batch size: the number of sequences processed at a time</span></span><br><span class="line"><span class="string">2. sequence length</span></span><br><span class="line"><span class="string">3. Hidden size: usually very large(768, 3072, even more)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print(outputs.last_hidden_state.size) <span class="comment"># torch.Size([2,16,768])</span></span><br></pre></td></tr></table></figure></li><li>There are also some other architectures for specific tasks. You can understand them as <code>AutoModel</code> followed by the head<ul><li>ForCausalLM</li><li>ForMaskedLM</li><li>ForMultipleChoice</li><li>ForQuestionAnswering</li><li>ForSequenceClassification</li><li>ForTokenClassification</li><li>other 🤗<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line">checkout=<span class="string">"checkoutName"</span></span><br><span class="line">model=AutoModelForSequenceClassification.from_pretrained(checkout)</span><br><span class="line">outputs=(**inputs)</span><br><span class="line">print(outputs.logits.shape) <span class="comment">#torch.Size([2,2]), the size is much smaller than the results of AutoModel</span></span><br></pre></td></tr></table></figure><h2 id="Postprocessing-the-output"><a href="#Postprocessing-the-output" class="headerlink" title="Postprocessing the output"></a>Postprocessing the output</h2></li></ul></li><li>Convert the logits into predictions , aka,  convert the raw, unnormalized scores to probabilties which can be understood by human</li><li>Usually we will use a SoftMax Layer to work on this<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">predictions=torch.nn.functional.softmax(outputs.logits, dim=<span class="number">-1</span>)</span><br><span class="line">print(predictions)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[5.0e-2,9.5e-1],[1.0e-1,9.0-1]],grad_fn=&lt;SoftmaxBackward&gt;)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># we can inspect the id2label the following way:</span></span><br><span class="line">model.config.id2label <span class="comment"># &#123;0:'NEGATIVE',1:'POSITIVE'&#125;</span></span><br></pre></td></tr></table></figure><h1 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h1></li><li><code>AutoClass</code> and it relatives mentioned above are simple wrappers which can automatically guess the architechture from your checkpoint<h2 id="Creating-A-Transformer"><a href="#Creating-A-Transformer" class="headerlink" title="Creating A Transformer"></a>Creating A Transformer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertConfig, BertModel, </span><br><span class="line">config=BertConfig()</span><br><span class="line">model=BertModel(config)<span class="comment"># in this way, you will get a randomly initialized bert model, which will output gibberish</span></span><br><span class="line">model=BertModel.from_pretrained(<span class="string">"bert-base-cased"</span>)<span class="comment"># the model is instantiate with the checkpoint trained by the bert team, which is less time-consuming and more environment-friendly; you can also replace the BertModel with AutoClass. Actually, it is more suggested to use AutoModel rather than a specific model, which will also work even if the architechture is different</span></span><br></pre></td></tr></table></figure></li><li>once you used the checkpoint, the weights are downloaded and cached to <code>~/.cache/huggingface/transformers</code></li><li>you can use the <code>save_pretrained</code> method to save the model to your disk:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.save_pretrained(<span class="string">"path/to/directory"</span>)</span><br><span class="line">!ls path/to/directory</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">config.json pytorch_model.bin</span></span><br><span class="line"><span class="string"># the two files go hand in hand, the `config.json` contains the attributes necessary to build the architechture, and the `pytorch_model.bin` contains the weights(checkpoints) which are the parameters of your model</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h1 id="Tokenizers"><a href="#Tokenizers" class="headerlink" title="Tokenizers"></a>Tokenizers</h1><h2 id="Algorithms-for-tokenization"><a href="#Algorithms-for-tokenization" class="headerlink" title="Algorithms for tokenization"></a>Algorithms for tokenization</h2><h3 id="Word-Based"><a href="#Word-Based" class="headerlink" title="Word-Based"></a>Word-Based</h3></li></ul><ol><li>split the the words according to the given marks, such as spaces, punctuations…</li><li>map each word to an ID. The ID is determined through the vocabulary, which is usually very large. For example, the size of a English vocabulary may be as large as 500,000</li><li>As for the words that are not in the vocabulary, they are often represented by the unkown token such as <code>[UNK]</code> or  <code>&lt;unk&gt;</code>. However, as you can imagine, this will lose imformation, so it is sensible to avoid the unknown tokens as much as possible<h3 id="Character-Based"><a href="#Character-Based" class="headerlink" title="Character-Based"></a>Character-Based</h3></li><li>split the sentences by the characters<blockquote><p>This method will defintely reduce the amount of unknown tokens. However, is will also cause the vocabulary to be too large and the results less meaningful</p><h3 id="Subword-Tokenization"><a href="#Subword-Tokenization" class="headerlink" title="Subword Tokenization"></a>Subword Tokenization</h3></blockquote></li><li>representing the rare words with the composite of frequently used words<blockquote><p>this will spare the space while remaining the semantic meanings as much as possible, which is especially useful to agglutinative languages such as Turkish</p></blockquote></li></ol><p><em>examples</em>:<br>-<br>Byte -level BPE: GPT-2</p><ul><li>WordPiece: BERT</li><li>SentencePiece or Unigram: multilingual models<h2 id="Loading-and-Saving"><a href="#Loading-and-Saving" class="headerlink" title="Loading and Saving"></a>Loading and Saving</h2></li><li>nearly the same as loading and saving the models: use <code>from_pretrained</code> and <code>save_pretrained</code></li><li>the things that cached<ul><li>algorithms: similar to <em>architechture</em> of the models</li><li>vocabularay: similar to <em>weights</em>of the models<h2 id="Encoding"><a href="#Encoding" class="headerlink" title="Encoding"></a>Encoding</h2></li></ul></li><li>there are two steps while encoding<h3 id="tokenization"><a href="#tokenization" class="headerlink" title="tokenization"></a>tokenization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer=AutoTokenizer.from_pretrained(<span class="string">"modelName"</span>)</span><br><span class="line">tokens=tokenizer.tokenize(<span class="string">"sequence"</span>)</span><br><span class="line"></span><br><span class="line">print(tokens) <span class="comment"># print the results of spilting</span></span><br></pre></td></tr></table></figure><h3 id="From-tokens-to-input-IDs"><a href="#From-tokens-to-input-IDs" class="headerlink" title="From tokens to input IDs"></a>From tokens to input IDs</h3></li><li>the models can only accept the tensors as inputs, so we need to map our tokens in to numbers</li><li>use the vocabulary to work on this, which is a dictionary mapping the tokens to numbers<h1 id="Handling-Multiple-Sequences"><a href="#Handling-Multiple-Sequences" class="headerlink" title="Handling Multiple Sequences"></a>Handling Multiple Sequences</h1><h2 id="Models-expect-a-batch-of-inputs"><a href="#Models-expect-a-batch-of-inputs" class="headerlink" title="Models expect a batch of inputs"></a>Models expect a batch of inputs</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">model=AutoModelForSequenceClassification.from_pretrained(<span class="string">"checkpointName"</span>)</span><br><span class="line">tokenizer=AutoTokenizer.from_pretrained(<span class="string">"checkpointName"</span>)</span><br><span class="line"></span><br><span class="line">tokens=tokenizer.tokenize(<span class="string">"raw inputs"</span>)</span><br><span class="line">token_ids=tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line">input_ids=torch.tensor(token_ids)</span><br><span class="line"></span><br><span class="line">model(input_ids)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">model([input_ids]) <span class="comment"># this line will succeed</span></span><br></pre></td></tr></table></figure><h2 id="Padding-the-inputs"><a href="#Padding-the-inputs" class="headerlink" title="Padding the inputs"></a>Padding the inputs</h2></li><li>the batch of inputs must have the same lengths to get through the model, for the shape of tensor is rectangle. This is why will introduce the <code>padding_id</code> to pad the inputs. You can use the attributes <code>pad_token_id</code> of a tokenizer to get access it<h2 id="Attention-masks"><a href="#Attention-masks" class="headerlink" title="Attention masks"></a>Attention masks</h2></li><li>the key features of transformers is the attention layers that contextualize each token. As a consequence, the padding ids will also make a difference to the output, which is not expected.</li><li>The Attention masks comes for this. The value 0 represents ignoring the corresponding tokens and 1 do the opposite<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">batched_ids = [</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, <span class="number">200</span>],</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, tokenizer.pad_token_id],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">attention_mask = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))</span><br><span class="line">print(outputs.logits)</span><br></pre></td></tr></table></figure><h2 id="Longer-sequences"><a href="#Longer-sequences" class="headerlink" title="Longer sequences"></a>Longer sequences</h2></li><li>there exists a limit to the length with all transformer models. If you want to pass a sequence longer than the limits, you should <strong>truncate</strong> your sequence or change to a model allowing longer inputs.<h1 id="Putting-it-all-together"><a href="#Putting-it-all-together" class="headerlink" title="Putting it all together"></a>Putting it all together</h1><h2 id="put-the-tokenization-steps-together"><a href="#put-the-tokenization-steps-together" class="headerlink" title="put the tokenization steps together"></a>put the tokenization steps together</h2></li><li>As a review, there are three steps we need to do to tokenize the raw inputs: <code>tokenizer.tokenize()</code>—&gt;<code>tokenizer.convert_tokens_to_ids()</code>—&gt;<code>torch.tensor()</code></li><li>For convenience, the <code>🤗 transformers</code> actually provides a high-level function to put all these steps together, which is <code>tokenizer()</code> itself<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">tokenizer=AutoTokenizer.from_pretrained(<span class="string">"checkpointName"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line">sequence=<span class="string">"1"</span></span><br><span class="line">input1=tokenizer(sequence) <span class="comment"># valid</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line">sequences=[<span class="string">"1"</span>,<span class="string">"2"</span>]</span><br><span class="line">input2=tokenizer(sequneces)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 pad</span></span><br><span class="line">input3=tokenizer(sequences, padding=<span class="string">"longest"</span>) <span class="comment"># pad to the maximum sequece length</span></span><br><span class="line">input4=tokenizer(sequences, padding=<span class="string">"max_length"</span>) <span class="comment"># pad to the model maximum length</span></span><br><span class="line">input5=tokenizer(sequences, padding=<span class="string">"max_length"</span>, max_length=<span class="number">8</span>) <span class="comment"># pad to the specified maximum length</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 truncate</span></span><br><span class="line">input6=tokenizer(sequences, truncation=<span class="literal">True</span>) <span class="comment"># truncate the sequences longer than the model limit</span></span><br><span class="line">input7=tokenizer(sequences, max_length=<span class="number">8</span>, truncation=<span class="literal">True</span>) <span class="comment"># truncate the sequences longer than the specified length</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5 switch the type of tensors returned</span></span><br><span class="line">input8=tokenizer(sequences, padding=<span class="literal">True</span>, return_tensors=<span class="string">"pt"</span>) <span class="comment"># pt stands for the Pytorch, tf for TensorFlow, np for NumPy</span></span><br></pre></td></tr></table></figure><h2 id="Special-words"><a href="#Special-words" class="headerlink" title="Special words"></a>Special words</h2></li><li>Some models add the special token such as <code>[CLS]</code> at the beginning, some add the special token such as <code>[SEP]</code> at the end, some add both, and some add none.<h2 id="Wrapping-up-From-tokenizer-to-model"><a href="#Wrapping-up-From-tokenizer-to-model" class="headerlink" title="Wrapping up: From tokenizer to model"></a>Wrapping up: From tokenizer to model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transfomers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"></span><br><span class="line">tokenizer=Autokenizer.form_pretrained(<span class="string">"checkpointName"</span>)</span><br><span class="line">model=AutoModel.form_pretrained(<span class="string">"checkpointName"</span>)</span><br><span class="line"></span><br><span class="line">sequence=<span class="string">"1"</span></span><br><span class="line">Input=tokenizer(sequence)</span><br><span class="line">Output=model(**Input)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> CS Learing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 🤗 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Chapter1 Transformer Models</title>
      <link href="/2024/05/10/NLP-Course-of-Hugging-Face/"/>
      <url>/2024/05/10/NLP-Course-of-Hugging-Face/</url>
      
        <content type="html"><![CDATA[<blockquote><p>最近打算入门 NLP，在自学 🤗 的<a href="https://huggingface.co/learn/nlp-course/chapter1/1" target="_blank" rel="noopener">NLP Course</a>，但是感觉自己过于摆烂了。于是打算边学边做笔记，争取在期末之前把本课程学完</p></blockquote><h1 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline()"></a><code>Pipeline()</code></h1><ul><li>the most basic object in the 🤗 Transformers llibrary</li><li>It can:<ul><li>feature-extraction(get a vector representing the text)</li><li>fill-task</li><li>ner(entity recognition)</li><li>question-answering</li><li>sentiment-analysis</li><li>summarization</li><li>text-generation</li><li>translation</li><li>zero-shot classification</li></ul></li></ul><h2 id="Zero-shot-Classification"><a href="#Zero-shot-Classification" class="headerlink" title="Zero-shot Classification"></a>Zero-shot Classification</h2><ul><li>you can casually assign the labels</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">classifier = pipeline(<span class="string">"zero-shot-classification"</span>)</span><br><span class="line">classifier(</span><br><span class="line">   <span class="string">"I play Genshin Impact!"</span>,</span><br><span class="line">   candidate_labels=[<span class="string">"op"</span>,<span class="string">"2-dimensional"</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="Text-Generation"><a href="#Text-Generation" class="headerlink" title="Text Generation"></a>Text Generation</h2><ul><li>involves randomness</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">generator=pipeline(<span class="string">"text-generation"</span>)</span><br><span class="line">generator(</span><br><span class="line">  <span class="string">"prompts"</span>,</span><br><span class="line">  max_length=<span class="number">15</span>, <span class="comment">#the max length of the text</span></span><br><span class="line">  num_return_sequence=<span class="number">3</span> <span class="comment">#How many texts are gonna generated</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="Mask-filling"><a href="#Mask-filling" class="headerlink" title="Mask filling"></a>Mask filling</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">unmasker=pipeline(<span class="string">"fill-mask"</span>)</span><br><span class="line">unmasker(<span class="string">"I play &lt;mask&gt; Impact!"</span>,top_k=<span class="number">2</span>) <span class="comment">#top_k decides the time it does;&lt;mask&gt; depends on what model you are using</span></span><br></pre></td></tr></table></figure><h2 id="Named-entity-recognition"><a href="#Named-entity-recognition" class="headerlink" title="Named entity recognition"></a>Named entity recognition</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ner=pipeline(<span class="string">"ner"</span>,grouped_entities=<span class="literal">True</span>)<span class="comment">#'grouped_entities=True' is used to enable the model to put multi-words together</span></span><br></pre></td></tr></table></figure><h2 id="Question-answering"><a href="#Question-answering" class="headerlink" title="Question answering"></a>Question answering</h2><ul><li>answer a question using given context</li><li>extracting answers from the context instead of generating answers</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">question_answerer=pipeline(<span class="string">"question-answering"</span>)</span><br><span class="line">question_answerer(</span><br><span class="line">  question=<span class="string">"where do I work?"</span>,</span><br><span class="line">  context=<span class="string">"My name is Sylvain and I work at Hugging Face in Brooklyn"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="Summarization"><a href="#Summarization" class="headerlink" title="Summarization"></a>Summarization</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">summarizer=pipeline(<span class="string">"summarization"</span>)</span><br><span class="line">summarizer(</span><br><span class="line">   <span class="string">"""</span></span><br><span class="line"><span class="string">context</span></span><br><span class="line"><span class="string">   """</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="Translation"><a href="#Translation" class="headerlink" title="Translation"></a>Translation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">translator=pipeline(<span class="string">"translation"</span>,model=<span class="string">"modelName"</span>)</span><br><span class="line">translator(<span class="string">"contex"</span>)</span><br></pre></td></tr></table></figure><h1 id="Brief-intro-to-Transformers"><a href="#Brief-intro-to-Transformers" class="headerlink" title="Brief intro to Transformers"></a>Brief intro to Transformers</h1><h2 id="General-categorization"><a href="#General-categorization" class="headerlink" title="General categorization"></a>General categorization</h2><ul><li>GPT-like: auto-regressive</li><li>BERT-like: auto-encoding</li><li>BART/T5-like: sequence-to-sequence</li></ul><h2 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h2><h3 id="Pretraning"><a href="#Pretraning" class="headerlink" title="Pretraning"></a>Pretraning</h3><ul><li>the act of traing a model from scratch<br><img src="https://github.com/daucloud/imagecdn/raw/main/test/202404251741650.png" alt=""></li></ul><h3 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a>Fine-tuning</h3><ul><li>training on the top of pretrained models with a dataset specific to the target task<br><img src="https://github.com/daucloud/imagecdn/raw/main/test/202404251746195.png" alt="image.png"></li></ul><h2 id="General-architecture"><a href="#General-architecture" class="headerlink" title="General architecture"></a>General architecture</h2><ul><li>Transformers model is generally composed of two blocks:<ul><li>Encoder: receives inputs and builds representations for them</li><li>Decoder: use the outputs of encoder to generate target outputs</li></ul></li><li>Various tasks requires different blocks:<ul><li>Encoder-only: sentence classfication; NER</li><li>Decoder-only: text generation</li><li>Encoder-decoder models or sequence-to-sequence models: translation or summarization</li></ul></li></ul><h2 id="Attention-layer"><a href="#Attention-layer" class="headerlink" title="Attention layer"></a>Attention layer</h2><ul><li>pay specific attention to certain words while ignoring others more or less</li></ul><h2 id="Original-Architechture"><a href="#Original-Architechture" class="headerlink" title="Original Architechture"></a>Original Architechture</h2><ul><li>the encoder translate all the words</li><li>the decoder is only allowed to translate by the past words; but later it can get all the outpus of encoer to better translate the word<br><img src="https://github.com/daucloud/imagecdn/raw/main/test/202404271755969.png" alt="image.png"></li></ul>]]></content>
      
      
      <categories>
          
          <category> CS Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 🤗 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>漫步杂记</title>
      <link href="/2024/03/17/%E6%BC%AB%E6%AD%A5%E6%9D%82%E8%AE%B0/"/>
      <url>/2024/03/17/%E6%BC%AB%E6%AD%A5%E6%9D%82%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>喜欢在少人的夜，一个人漫无目的地在校园里走走停停。瘫在宿舍打了一下午的游戏，头脑昏昏沉沉。于是掷下手机，抄起外套，择宿舍楼西边的一条僻幽小径，开始在入夜的园子里随意游荡。正值冬春之交，北京的风已然收起了冬日硌人的棱角，佐以夜的些许清冷的寒意，迎面吹来，令人感到舒适又清醒。享受着风的感觉，心灵逐渐从日常生活的繁琐和世俗意义的压力中抽离，柔软的触角不知不觉向四周延伸开来：落叶随风移动时摩擦地面的簌簌声、校河反射路灯的粼粼波光、道旁宿舍楼中通亮的灯、黑夜中轻柔摇动的柳条、奇形怪状的光秃枝丫、含苞待放的粉色蓓蕾……心灵仿佛褪去了老成的外壳，对司空见惯的一切重又兴起了孩童般的好奇，目之所及、耳之所闻都是再新奇不过的事物。</p><p>肆意的外界体验唤醒了丰盈的内在感受，骨髓深处的孤独感一点点渗了出来，悄然漫上了心头。我是一个孤独的人，内心深处，始终有一部分是无法对人敞开、甚至我自己也不甚了然的。很多时候，我对这种孤独感是异常恐惧的：在某些难眠的深夜，我会被潮水般涌出的孤独窒塞住呼吸。我渴望有一名真正的灵魂知己能够分享一切、倾吐所有。可或是不敢，或是无缘，终究没能找到。但是，在这个随意漫步的晚上，我却逐渐学会了享受孤独。独自一人，可以随心所欲地停步和起步、随心所欲地蹲下来端详道旁的枯草、随心所欲地拾起脚边的枝条。原来孤独并不是什么可怕的事情。每个人生来孤独，这是一个人身为他自己而不是其他的什么人所必然带来的独一无二的体验。我当然仍渴望觅到一名知己，但我已学会拥抱这份孤独。</p><p>踱步到大礼堂北边的河道时，突然有一阵极猛烈的风顺着我前进的方向吹来。我立住片刻，感受着风在耳边的呼啸声和强风下紧贴身体的衣物。随后我转过身，张开双臂，闭目体会寒冷的气流冲击面颊时的微痛。我逆风走了几步，随后又转过身去，顺风走了起来。我越走越快，越走越跳跃，越走越轻盈。我忍不住笑了起来，一开始是微笑，后来是咧嘴笑，最后竟演变成了哈哈大笑。我一边笑，一边大声说：“我还活着！”后来风停了，我还在大声笑着。周围没什么人，见证者只有河水、柳树和礼堂。</p><p>回宿舍时，我走的是学堂路。学堂路人流不小，走在道路的边缘，时时有自行车从身旁驰过。我饶有兴致地看着各式各样的自行车远去的身影，听着偶尔传来的一两句玩笑和谈话声，发散的思绪重又回到了日常生活，只是多上了几分释然、坦荡和宁静。漫步的趣味，当真言之不尽。</p><blockquote><p>注：背景图片来自<a href="https://mapio.net/pic/p-3220490/" target="_blank" rel="noopener">https://mapio.net/pic/p-3220490/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八的东西 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>可以判断素数的正则表达式</title>
      <link href="/2024/02/01/%E5%8F%AF%E4%BB%A5%E5%8C%B9%E9%85%8D%E5%90%88%E6%95%B0%E7%9A%84%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
      <url>/2024/02/01/%E5%8F%AF%E4%BB%A5%E5%8C%B9%E9%85%8D%E5%90%88%E6%95%B0%E7%9A%84%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>最近读到一个可以判断素数的正则表达式（匹配成功则不是素数）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;^1?$|^(11+?)\1+$&#x2F;</span><br></pre></td></tr></table></figure><br>这么精炼，颇有些出乎我的意料。因为在我的印象中大多数正则表达式都十分丑陋，比如匹配邮箱的正则表达式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(?:[a-z0-9!#$%&amp;&#39;*+&#x2F;&#x3D;?^_&#96;&#123;|&#125;~-]+(?:\.[a-z0-9!#$%&amp;&#39;*+&#x2F;&#x3D;?^_&#96;&#123;|&#125;~-]+)*|&quot;(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*&quot;)@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.)&#123;3&#125;(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])</span><br></pre></td></tr></table></figure></p><p>然而，仔细阅读了一下原理后却发现它的确可以，不过需要一步预处理，即把十进制数字转换为“1的数组”：0为空字符串，1为1，2为11，3为111……<br>至此，就可以开始我们的匹配：</p><ul><li><code>/^1?$/</code>很好理解，匹配到是空字符串（0）或 1，自然不是合数；</li><li><code>/^(11+?)\1+$/</code>则正是该表达式的精妙所在。其利用到了正则表达式匹配时<strong>回溯</strong>的特性。让我们具体解释一下：</li></ul><ol><li><code>(11+?)</code>匹配的是<strong>至少两个1</strong>，但是因为进行的是懒惰匹配，所以最开始匹配到的是<code>11</code>，并被捕获到了分组<code>\1</code>中。后面部分中，如果<code>11</code>重复了一次或者更多次，那么就是合数。这又是为什么呢？其实非常容易理解：如果数字$a$匹配成功，意味着$a$化作的“1的数组”<strong>恰好</strong>由<strong>不少于</strong>2个<code>11</code>组成，也就是，$\exists b&gt;1\land b \in \mathbb N, a=2b$，这自然意味着$a$是一个合数(在这种情况下，$a$还是偶数)。</li><li>如果<code>11</code>匹配失败了呢？这就是本法最核心的部分了：匹配器会进行回溯，对下一个满足<code>(11+?)</code>的<code>111</code>进行<code>\1+</code>匹配的尝试。同理上一步，如果匹配成功，该数字大于3且含有一个因数3，自然是合数。</li><li>接下来，“匹配 - 回溯”不断进行。如果$n$是合数，会在匹配不超过$\sqrt{n}$次后成功；反之，会一直匹配到第$n$次，最后匹配失败。</li></ol><p>可见，<code>/^1?$|^(11+?)\1+$/</code>的实现想法是非常朴素的：列举比$n$小的所有正整数$i$，判断$n$是否可以被$i$整除。比如：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//判断一个自然数是否为素数</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isPrime</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>)<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">2</span>)<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= <span class="built_in">sqrt</span>(n); i++)<span class="keyword">if</span> (n % i == <span class="number">0</span>)<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>但是，得到这么精炼的表达式的关键实则在于对于正则表达式匹配机制的深刻理解(懒惰匹配和回溯法的结合)，而这正是值得我们深思和学习的地方。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>生成对抗网络</title>
      <link href="/2024/01/18/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
      <url>/2024/01/18/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="生成对抗网络"><a href="#生成对抗网络" class="headerlink" title="生成对抗网络"></a>生成对抗网络</h1><p><del>本文是数据科学导论期末大作业展示的报告，拿来随便水一篇博客hh</del></p><h2 id="0-引言"><a href="#0-引言" class="headerlink" title="0    引言"></a>0    引言</h2><p>生成对抗网络 (Generative Adversarial Network, GAN) 是一种十分流行的机器学习模型。自2014年Ian Goodfellow等人首次提出以来，GAN迅速在学术界和工业界引发了热烈的反响，许多有影响力的工作层出不穷。图一是累积的GAN论文数量，其火热程度可见一斑。</p><p><img src="https://github.com/daucloud/imagecdn/raw/main/test/202401181914378.png" alt=""></p><center>Figure 1: Cumulative number of GAN papers</center><p>本文将对GAN做简要介绍，主要包括GAN原始模型、经典变体和应用成就。</p><h2 id="1-原始模型"><a href="#1-原始模型" class="headerlink" title="1    原始模型"></a>1    原始模型</h2><h3 id="1-1-基本思想"><a href="#1-1-基本思想" class="headerlink" title="1.1    基本思想"></a>1.1    基本思想</h3><p>GAN全称为生成对抗网络。顾名思义，其在本质上是一种生成模型，它最突出的特点是使用判别器与生成器进行对抗式训练，后者负责产生贴近真实的数据，前者则负责努力辨别生成数据和真实数据。当训练迭代至一定次数后，生成器产生的数据便足够逼真，训练目的因而得到实现。</p><p>对此，GAN的原始论文<sup><a href="#fn_1" id="reffn_1">1</a></sup>中给出了一个通俗的解释：</p><blockquote><p>生成器是“假钞制造团伙”，负责制造能在市场上流通的假钞；判别器则是“警察”，负责识破假钞。假钞制造团伙和警察之间不断竞争，警察的鉴别能力和团伙的造假能力都不断提升，最终，造假团伙制造的假钞足以以假乱真：“假钞变成了真钞”。</p></blockquote><p><strong>GAN就是在制造足以以假乱真的假钞。</strong></p><h3 id="1-2-基本步骤"><a href="#1-2-基本步骤" class="headerlink" title="1.2    基本步骤"></a>1.2    基本步骤</h3><p>概括来说，GAN的训练主要分为以下三步：</p><ol><li><strong>固定生成器$G$，训练判别器$D$：</strong> 使 $D$ 尽可能准确地识别出样本究竟是由生成器生成的（来自 <script type="math/tex">p_z(z)</script>）还是来自真实数据集（来自 <script type="math/tex">p_{data}(x)</script>）</li><li><strong>固定判别器$D$，训练生成器$G$：</strong> 使 $G$ 生成的样本($G(z)$)尽可能贴近真实样本，即让生成器$G$骗过判别器$D$</li><li><strong>迭代：</strong> 循环1. 2.至一定次数，此时生成器$G$产生的样本足以“以假乱真”，判别器$D$辨认真实样本和生成样本成功的概率都为$\frac1 2$，二者达到了纳什平衡<sup><a href="#fn_2" id="reffn_2">2</a></sup>。</li></ol><blockquote><p>训练示意图：<img src="https://github.com/daucloud/imagecdn/raw/main/test/202401181914603.png" alt=""></p><center>Figure 2: 训练过程示意图</center><p>图中，$z$轴是输入生成器的随机噪声，其通过生成器(箭头)映射到样本集$x$轴。黑色散点是真实数据集；绿色实线是生成样本集，蓝色虚线是判别器（越远离$x$轴，其值越接近于1，即其认为该样本来自真实数据集的概率越大）。</p><p>(a)中，判别器曲线(蓝色虚线)较为颠簸，判别效果较差，因此(a)到(b)首先对判别器进行训练；(b)到(c)则对生成器进行训练，使得生成样本曲线(绿色实线)更贴近真实样本集(黑色散点)。当迭代此二步到一定次数后，绿色实线和黑色散点接近重合，判别器曲线也恒为$1 \over 2$，即无法将生成样本和真实数据进行区分。</p></blockquote><h3 id="1-3-具体算法"><a href="#1-3-具体算法" class="headerlink" title="1.3    具体算法"></a>1.3    具体算法</h3><p><strong>记号说明</strong></p><ul><li>生成器和判别器均为多层神经网络：$G(z;\theta _g)$和$D(x;\theta _d)$</li><li>$G$通过参数$\theta_g$将噪声$z$映射为生成样本$G(z;\theta _g)$</li><li>$D$通过参数$\theta_d$将样本映射为$[0,1]$的标量$D(x;\theta _d)$，其值越接近1表示为真实数据的概率越大</li><li>目标，求解值函数$V(D,G)$的极小极大值，即：</li></ul><script type="math/tex; mode=display">\min_G\max_D V(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[\log D(x)]+\mathbb{E}_{z\sim p_z(z)}[\log(1-D(G(z)))]</script><p><strong>具体求解步骤</strong></p><p>for 1 to n do<br>&emsp;&emsp;for 1 to k do<br>&emsp;&emsp;&emsp;&emsp;从噪声集取出m个样本${z^{(1)},z^{(2)},…,z^{(m)}}$;<br>&emsp;&emsp;&emsp;&emsp;从真实数据集中取出m个样本${x^{(1)},x^{(2)},…,x^{(m)}}$;<br>&emsp;&emsp;&emsp;&emsp;使用梯度上升法<sup><a href="#fn_3" id="reffn_3">3</a></sup>更新判别器的参数$\theta_d$：</p><script type="math/tex; mode=display">\nabla_{\theta_d}\frac{1}{m}\sum_{i=1}^{m}[\log D(x^{(i)})+\log D(1-D(G(z^{(i)})))].</script><p>&emsp;&emsp;end for<br>&emsp;&emsp;从噪声集中取出m个样本${z^{(1)},z^{(2)},…,z^{(m)}}$;<br>&emsp;&emsp;使用梯度下降法更新生成器的参数$\theta_g$</p><script type="math/tex; mode=display">\nabla_{\theta_d}\frac{1}{m}\sum_{i=1}^{m}[\log (1-D(G(z^{(i)})))].</script><p>end for</p><blockquote><p>k是一个超参数；之所以更新k次$D$才更新1次$G$，是因为只有判别器足够好，生成器的更新才准确有效。</p></blockquote><h2 id="2-经典变体"><a href="#2-经典变体" class="headerlink" title="2    经典变体"></a>2    经典变体</h2><p>GAN作为风靡一时的机器学习模型，自然少不了各种改进和变体。本部分选取了三种较有代表性和影响力的变体进行介绍：CGAN、DCGAN和WGAN。</p><h3 id="2-1-CGAN4"><a href="#2-1-CGAN4" class="headerlink" title="2.1    CGAN4"></a>2.1    CGAN<sup><a href="#fn_4" id="reffn_4">4</a></sup></h3><p>原始GAN模型中，输入生成器的是随机噪声，因此最后产生的图像的随机性也相对较大。倘若我们想要获得具有某种特征和标签的图像，就需要从生成的一大堆样本中进行挑选，较为繁琐。</p><p>CGAN(Conditional Generative Adversarial Networks)便是为了解决这一问题而产生的。其思想也十分朴素，即向生成器和判别器的输入中添加条件信息。这样一来，生成样本不仅需要足够逼真，还要满足特定的条件才能通过判别器：</p><script type="math/tex; mode=display">\min_G\max_D V(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[\log D(x|y)]+\mathbb{E}_{z\sim p_z(z)}[\log(1-D(G(z|y)))]</script><p><img src="https://aovoc.github.io/assets/pics/cgan-arch2.PNG" alt="Cgan,icgan"></p><center>Figure 3: CGAN和GAN的对比</center><h3 id="2-2-DCGAN5"><a href="#2-2-DCGAN5" class="headerlink" title="2.2    DCGAN5"></a>2.2    DCGAN<sup><a href="#fn_5" id="reffn_5">5</a></sup></h3><p>DCGAN(Deep Convolutional Generative Adversarial Networks)全称为深度卷积对抗生成网络。顾名思义，其主要想法是将卷积神经网络(CNN)和GAN进行结合，在不改变GAN的基本原理的情况下较为有效地改善了其训练不稳定的问题。</p><p>DCGAN做出的主要改变有：</p><ul><li><p>使用卷积层和转置卷积层：引入了转置卷积层和卷积层分别作为生成器和判别器网络的主要组件。</p></li><li><p>去除全连接层：用全卷积层代替。</p></li><li>批归一化(Batch Normalization)：生成器和判别器都使用BN层。</li><li>修改激活函数：生成器输出层使用Tanh，其余层使用ReLU；判别器均使用LeakyReLU</li></ul><p><img src="https://github.com/daucloud/imagecdn/raw/main/test/202401181914603.png" alt=""></p><center>Figure 4: 生成器转置卷积层示意图</center><h3 id="2-3-WGAN6"><a href="#2-3-WGAN6" class="headerlink" title="2.3    WGAN6"></a>2.3    WGAN<sup><a href="#fn_6" id="reffn_6">6</a></sup></h3><p>WGAN(Wasserstein Generative Adversarial Networks)引入了Wasserstein距离代替原来的JS散度作为GAN的损失函数，彻底解决了GAN训练不稳定的问题，是GAN发展史上里程碑式的工作之一。</p><p><img src="https://pic1.zhimg.com/80/v2-6be6e2ef3d15c4b10c2a943e9bf4db70_1440w.jpg#width=60%" alt="img" style="zoom:67%;" /></p><center>Figure 5: WGAN算法</center><p>可见，WGAN做出的最主要改动，即是对损失函数的更换。此外，其还做出了如下改变：</p><ul><li>判别器最后一层去掉sigmoid</li><li>每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c</li><li>不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行</li></ul><p>改进虽然简单，但是成效巨大。</p><h2 id="3-应用举例"><a href="#3-应用举例" class="headerlink" title="3    应用举例"></a>3    应用举例</h2><h3 id="3-1-「Edmond-de-Belamy」"><a href="#3-1-「Edmond-de-Belamy」" class="headerlink" title="3.1    「Edmond de Belamy」"></a>3.1    「Edmond de Belamy」</h3><p>在2018年末的佳士得纽约拍卖场上，来自巴黎的艺术团队<em>Obvious</em>使用GAN生成的画作「Edmond de Belamy」以超出估价40倍的43.25万美元成交，其右下角便印有纵横GAN领域的著名公式： <script type="math/tex">\min_G\max_D \mathbb{E}_{x}[\log D(x)]+\mathbb{E}_{z}[\log(1-D(G(z)))]</script></p><p>1968年，毕加索曾说：”计算机是没有用的。它们只会告诉你答案”。但在同一场拍卖会上，没有一幅毕加索的画作成交价格超过了「Edmond de Belamy」，这不禁令人唏嘘不已。</p><p><img src="https://github.com/daucloud/imagecdn/raw/main/test/202401181922436.png" alt=""></p><center>Figure 6: 「Edmond de Belamy」</center><h3 id="3-2-This-Person-Does-Not-Exist"><a href="#3-2-This-Person-Does-Not-Exist" class="headerlink" title="3.2    This Person Does Not Exist"></a>3.2    This Person Does Not Exist</h3><p><a href="https://thispersondoesnotexist.com/" target="_blank" rel="noopener">thispersondoesnotexist.com </a>每次进入该网址，都会生成一张世界上并不存在的人脸，而这正是使用GAN进行生成的。</p><p><img src="https://github.com/daucloud/imagecdn/raw/main/test/202401181917230.png" alt=""></p><center>Figure 7: This Person Does Not Exist</center><h3 id="3-3-二次元头像生成7"><a href="#3-3-二次元头像生成7" class="headerlink" title="3.3    二次元头像生成7"></a>3.3    二次元头像生成<sup><a href="#fn_7" id="reffn_7">7</a></sup></h3><p>使用GAN，你可以随心所欲生成二次元<del>老婆</del>头像：</p><p><img src="https://github.com/daucloud/imagecdn/raw/main/test/202401181917189.png" alt=""></p><center>Figure 8: GAN生成的二次元头像</center><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4    总结"></a>4    总结</h2><p>GAN是一种应用广泛、潜力巨大的生成模型。它使用判别器和生成器进行对抗性训练，并最终产生足够逼真的图像。但GAN在训练过程中通常存在生成样本过于随机、训练不稳定等缺点，因此涌现出了诸多变体对其进行改进：CGAN、DCGAN、WGAN …… </p><p>我们期待在将来能看到更具潜力的GAN模型和更富价值的GAN应用！</p><hr><p>Footnotes: </p><p>[1]:Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014, June 10). <em>Generative Adversarial Networks</em>. arXiv.org. <a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">https://arxiv.org/abs/1406.2661</a><br>[2]:事实上，这种 “纳什平衡” 是一种理想状态，实际训练难以达到。<br>[3]:关于梯度下降/上升法，可参考<a href="https://www.zhihu.com/question/305638940/answer/1639782992" target="_blank" rel="noopener">什么是梯度下降法？ - 马同学的回答 - 知乎</a><br>[4]:Mirza, M., &amp; Osindero, S. (2014, November 6). <em>Conditional generative adversarial nets</em>. arXiv.org. <a href="https://arxiv.org/abs/1411.1784" target="_blank" rel="noopener">https://arxiv.org/abs/1411.1784</a><br>[5]:Radford, A., Metz, L., &amp; Chintala, S. (2016, January 7). <em>Unsupervised representation learning with deep convolutional generative Adversarial Networks</em>. arXiv.org. <a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="noopener">https://arxiv.org/abs/1511.06434</a><br>[6]:<a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="noopener">令人拍案叫绝的Wasserstein GAN - 郑华滨的文章 - 知乎</a><br>[7]:Jin, Y., Zhang, J., Li, M., Tian, Y., Zhu, H., &amp; Fang, Z. (2017, August 18). <em>Towards the automatic anime characters creation with Generative Adversarial Networks</em>. arXiv.org. <a href="https://arxiv.org/abs/1708.05509" target="_blank" rel="noopener">https://arxiv.org/abs/1708.05509</a></p>]]></content>
      
      
      <categories>
          
          <category> cs learning </category>
          
          <category> intro to data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>雪</title>
      <link href="/2023/12/11/%E9%9B%AA/"/>
      <url>/2023/12/11/%E9%9B%AA/</url>
      
        <content type="html"><![CDATA[<h1 id="雪"><a href="#雪" class="headerlink" title="雪"></a>雪</h1><p>本来是不喜欢冬天的。</p><p>原来顶喜欢的季节是秋天。漫步街头，总能与一片飘然坠于身前的落叶不期而遇。爱极了这种意料之外的邂逅。</p><p>后来听过luna的<a href="https://www.bilibili.com/video/BV1ka4y1E7ik/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bd539b5a62c295726bece82272cc6c5a" target="_blank" rel="noopener"> あの夏のいつかは (在那個夏日的某天)</a>，夏天在我心中的地位渐渐变得与秋天不分伯仲了。每次看这首曲子的pv，都深为夏的热烈激动不已：“正是无数个微小片刻，汇集在一起造就这热烈的我”。生命本该如此。</p><p>但是，提到“冬”这个字，浮现在脑海中的总是凛冽的风、凋零的树、蜷缩的人……讨厌这种万事万物都被压抑的感觉：生命似乎被严酷的冬冰封了起来，待到来年春暖花开的时节，才能重焕生机。冬天少了点灵魂。</p><p>后来才渐渐发现我错了。冬不是没有灵魂；相反，冬的灵魂甚至比其他三个季节都更加可爱——这正是那名为“雪”的精灵。雪洁白、轻盈、古灵精怪，冬天的沉闷因为她的到来一扫而空。“忽如一夜春风来，千树万树梨花开。”被雪点缀后的世界，焕发出不亚于春的勃勃生机。人们不再蜷缩在被窝，校园里随处可见飞舞的雪球、奇形怪状的雪人和创意百出的雪地上的图案。冬天原来是这样一个趣味盎然的季节。</p><p>独身骑行在清华园内，走走停停，停停走走，一时将诸多ddl抛在脑后，只觉心灵也因这白茫茫的雪的世界变得一片通透。终于明白原来没有一个季节是不值得喜爱的，生命中也没有一个时刻是不值得热爱的。人活在世，本该如此。</p><p>本该这样喜欢冬天啊。</p><p><img src="/img/2023-12-11-雪/snow1.jpg" alt="snow1"></p><p><img src="/img/2023-12-11-雪/snow2.jpg" alt="snow2"></p><p><img src="/img/2023-12-11-雪/snow.jpg" alt="snow"></p>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八的东西 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello,World!</title>
      <link href="/2023/12/09/Hello-World/"/>
      <url>/2023/12/09/Hello-World/</url>
      
        <content type="html"><![CDATA[<p>建好了博客，第一件事当然是发一篇 Hello,world! (bushi</p><p><del>实际上没想好要写啥，所以随便水一篇博客</del></p><p>放首DT：</p><iframe allow="autoplay *; encrypted-media *; fullscreen *; clipboard-write" frameborder="0" height="175" style="width:100%;max-width:660px;overflow:hidden;border-radius:10px;" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-storage-access-by-user-activation allow-top-navigation-by-user-activation" src="https://embed.music.apple.com/cn/album/%E6%B5%81%E6%B2%99/1416149926?i=1416149940"></iframe>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八的东西 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
