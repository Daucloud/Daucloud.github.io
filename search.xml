<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>可以判断素数的正则表达式</title>
      <link href="/2024/02/01/%E5%8F%AF%E4%BB%A5%E5%8C%B9%E9%85%8D%E5%90%88%E6%95%B0%E7%9A%84%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
      <url>/2024/02/01/%E5%8F%AF%E4%BB%A5%E5%8C%B9%E9%85%8D%E5%90%88%E6%95%B0%E7%9A%84%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>最近读到一个可以判断素数的正则表达式（匹配成功则不是素数）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;^1?$|^(11+?)\1+$&#x2F;</span><br></pre></td></tr></table></figure><br>这么精炼，颇有些出乎我的意料。因为在我的印象中大多数正则表达式都十分丑陋，比如匹配邮箱的正则表达式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(?:[a-z0-9!#$%&amp;&#39;*+&#x2F;&#x3D;?^_&#96;&#123;|&#125;~-]+(?:\.[a-z0-9!#$%&amp;&#39;*+&#x2F;&#x3D;?^_&#96;&#123;|&#125;~-]+)*|&quot;(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*&quot;)@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.)&#123;3&#125;(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])</span><br></pre></td></tr></table></figure></p><p>然而，仔细阅读了一下原理后却发现它的确可以，不过需要一步预处理，即把十进制数字转换为“1的数组”：0为空字符串，1为1，2为11，3为111……<br>至此，就可以开始我们的匹配：</p><ul><li><code>/^1?$/</code>很好理解，匹配到是空字符串（0）或 1，自然不是合数；</li><li><code>/^(11+?)\1+$/</code>则正是该表达式的精妙所在。其利用到了正则表达式匹配时<strong>回溯</strong>的特性。让我们具体解释一下：</li></ul><ol><li><code>(11+?)</code>匹配的是<strong>至少两个1</strong>，但是因为进行的是懒惰匹配，所以最开始匹配到的是<code>11</code>，并被捕获到了分组<code>\1</code>中。后面部分中，如果<code>11</code>重复了一次或者更多次，那么就是合数。这又是为什么呢？其实非常容易理解：如果数字$a$匹配成功，意味着$a$化作的“1的数组”<strong>恰好</strong>由<strong>不少于</strong>2个<code>11</code>组成，也就是，$\exists b&gt;1\land b \in \mathbb N, a=2b$，这自然意味着$a$是一个合数(在这种情况下，$a$还是偶数)。</li><li>如果<code>11</code>匹配失败了呢？这就是本法最核心的部分了：匹配器会进行回溯，对下一个满足<code>(11+?)</code>的<code>111</code>进行<code>\1+</code>匹配的尝试。同理上一步，如果匹配成功，该数字大于3且含有一个因数3，自然是合数。</li><li>接下来，“匹配 - 回溯”不断进行。如果$n$是合数，会在匹配不超过$\sqrt{n}$次后成功；反之，会一直匹配到第$n$次，最后匹配失败。</li></ol><p>可见，<code>/^1?$|^(11+?)\1+$/</code>的实现想法是非常朴素的：列举比$n$小的所有正整数$i$，判断$n$是否可以被$i$整除。比如：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//判断一个自然数是否为素数</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isPrime</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>)<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">2</span>)<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= <span class="built_in">sqrt</span>(n); i++)<span class="keyword">if</span> (n % i == <span class="number">0</span>)<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>但是，得到这么精炼的表达式的关键实则在于对于正则表达式匹配机制的深刻理解(懒惰匹配和回溯法的结合)，而这正是值得我们深思和学习的地方。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>生成对抗网络</title>
      <link href="/2024/01/18/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
      <url>/2024/01/18/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="生成对抗网络"><a href="#生成对抗网络" class="headerlink" title="生成对抗网络"></a>生成对抗网络</h1><p><del>本文是数据科学导论期末大作业展示的报告，拿来随便水一篇博客hh</del></p><h2 id="0-引言"><a href="#0-引言" class="headerlink" title="0    引言"></a>0    引言</h2><p>生成对抗网络 (Generative Adversarial Network, GAN) 是一种十分流行的机器学习模型。自2014年Ian Goodfellow等人首次提出以来，GAN迅速在学术界和工业界引发了热烈的反响，许多有影响力的工作层出不穷。图一是累积的GAN论文数量，其火热程度可见一斑。</p><p><img src="https://cdn.jsdelivr.net/gh/Daucloud/imagecdn/test/202401181914378.png" alt=""></p><center>Figure 1: Cumulative number of GAN papers</center><p>本文将对GAN做简要介绍，主要包括GAN原始模型、经典变体和应用成就。</p><h2 id="1-原始模型"><a href="#1-原始模型" class="headerlink" title="1    原始模型"></a>1    原始模型</h2><h3 id="1-1-基本思想"><a href="#1-1-基本思想" class="headerlink" title="1.1    基本思想"></a>1.1    基本思想</h3><p>GAN全称为生成对抗网络。顾名思义，其在本质上是一种生成模型，它最突出的特点是使用判别器与生成器进行对抗式训练，后者负责产生贴近真实的数据，前者则负责努力辨别生成数据和真实数据。当训练迭代至一定次数后，生成器产生的数据便足够逼真，训练目的因而得到实现。</p><p>对此，GAN的原始论文<sup><a href="#fn_1" id="reffn_1">1</a></sup>中给出了一个通俗的解释：</p><blockquote><p>生成器是“假钞制造团伙”，负责制造能在市场上流通的假钞；判别器则是“警察”，负责识破假钞。假钞制造团伙和警察之间不断竞争，警察的鉴别能力和团伙的造假能力都不断提升，最终，造假团伙制造的假钞足以以假乱真：“假钞变成了真钞”。</p></blockquote><p><strong>GAN就是在制造足以以假乱真的假钞。</strong></p><h3 id="1-2-基本步骤"><a href="#1-2-基本步骤" class="headerlink" title="1.2    基本步骤"></a>1.2    基本步骤</h3><p>概括来说，GAN的训练主要分为以下三步：</p><ol><li><strong>固定生成器$G$，训练判别器$D$：</strong> 使 $D$ 尽可能准确地识别出样本究竟是由生成器生成的（来自 <script type="math/tex">p_z(z)</script>）还是来自真实数据集（来自 <script type="math/tex">p_{data}(x)</script>）</li><li><strong>固定判别器$D$，训练生成器$G$：</strong> 使 $G$ 生成的样本($G(z)$)尽可能贴近真实样本，即让生成器$G$骗过判别器$D$</li><li><strong>迭代：</strong> 循环1. 2.至一定次数，此时生成器$G$产生的样本足以“以假乱真”，判别器$D$辨认真实样本和生成样本成功的概率都为$\frac1 2$，二者达到了纳什平衡<sup><a href="#fn_2" id="reffn_2">2</a></sup>。</li></ol><blockquote><p>训练示意图：<img src="https://cdn.jsdelivr.net/gh/Daucloud/imagecdn/test/202401181914603.png" alt=""></p><center>Figure 2: 训练过程示意图</center><p>图中，$z$轴是输入生成器的随机噪声，其通过生成器(箭头)映射到样本集$x$轴。黑色散点是真实数据集；绿色实线是生成样本集，蓝色虚线是判别器（越远离$x$轴，其值越接近于1，即其认为该样本来自真实数据集的概率越大）。</p><p>(a)中，判别器曲线(蓝色虚线)较为颠簸，判别效果较差，因此(a)到(b)首先对判别器进行训练；(b)到(c)则对生成器进行训练，使得生成样本曲线(绿色实线)更贴近真实样本集(黑色散点)。当迭代此二步到一定次数后，绿色实线和黑色散点接近重合，判别器曲线也恒为$1 \over 2$，即无法将生成样本和真实数据进行区分。</p></blockquote><h3 id="1-3-具体算法"><a href="#1-3-具体算法" class="headerlink" title="1.3    具体算法"></a>1.3    具体算法</h3><p><strong>记号说明</strong></p><ul><li>生成器和判别器均为多层神经网络：$G(z;\theta _g)$和$D(x;\theta _d)$</li><li>$G$通过参数$\theta_g$将噪声$z$映射为生成样本$G(z;\theta _g)$</li><li>$D$通过参数$\theta_d$将样本映射为$[0,1]$的标量$D(x;\theta _d)$，其值越接近1表示为真实数据的概率越大</li><li>目标，求解值函数$V(D,G)$的极小极大值，即：</li></ul><script type="math/tex; mode=display">\min_G\max_D V(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[\log D(x)]+\mathbb{E}_{z\sim p_z(z)}[\log(1-D(G(z)))]</script><p><strong>具体求解步骤</strong></p><p>for 1 to n do<br>&emsp;&emsp;for 1 to k do<br>&emsp;&emsp;&emsp;&emsp;从噪声集取出m个样本${z^{(1)},z^{(2)},…,z^{(m)}}$;<br>&emsp;&emsp;&emsp;&emsp;从真实数据集中取出m个样本${x^{(1)},x^{(2)},…,x^{(m)}}$;<br>&emsp;&emsp;&emsp;&emsp;使用梯度上升法<sup><a href="#fn_3" id="reffn_3">3</a></sup>更新判别器的参数$\theta_d$：</p><script type="math/tex; mode=display">\nabla_{\theta_d}\frac{1}{m}\sum_{i=1}^{m}[\log D(x^{(i)})+\log D(1-D(G(z^{(i)})))].</script><p>&emsp;&emsp;end for<br>&emsp;&emsp;从噪声集中取出m个样本${z^{(1)},z^{(2)},…,z^{(m)}}$;<br>&emsp;&emsp;使用梯度下降法更新生成器的参数$\theta_g$</p><script type="math/tex; mode=display">\nabla_{\theta_d}\frac{1}{m}\sum_{i=1}^{m}[\log (1-D(G(z^{(i)})))].</script><p>end for</p><blockquote><p>k是一个超参数；之所以更新k次$D$才更新1次$G$，是因为只有判别器足够好，生成器的更新才准确有效。</p></blockquote><h2 id="2-经典变体"><a href="#2-经典变体" class="headerlink" title="2    经典变体"></a>2    经典变体</h2><p>GAN作为风靡一时的机器学习模型，自然少不了各种改进和变体。本部分选取了三种较有代表性和影响力的变体进行介绍：CGAN、DCGAN和WGAN。</p><h3 id="2-1-CGAN4"><a href="#2-1-CGAN4" class="headerlink" title="2.1    CGAN4"></a>2.1    CGAN<sup><a href="#fn_4" id="reffn_4">4</a></sup></h3><p>原始GAN模型中，输入生成器的是随机噪声，因此最后产生的图像的随机性也相对较大。倘若我们想要获得具有某种特征和标签的图像，就需要从生成的一大堆样本中进行挑选，较为繁琐。</p><p>CGAN(Conditional Generative Adversarial Networks)便是为了解决这一问题而产生的。其思想也十分朴素，即向生成器和判别器的输入中添加条件信息。这样一来，生成样本不仅需要足够逼真，还要满足特定的条件才能通过判别器：</p><script type="math/tex; mode=display">\min_G\max_D V(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[\log D(x|y)]+\mathbb{E}_{z\sim p_z(z)}[\log(1-D(G(z|y)))]</script><p><img src="https://aovoc.github.io/assets/pics/cgan-arch2.PNG" alt="Cgan,icgan"></p><center>Figure 3: CGAN和GAN的对比</center><h3 id="2-2-DCGAN5"><a href="#2-2-DCGAN5" class="headerlink" title="2.2    DCGAN5"></a>2.2    DCGAN<sup><a href="#fn_5" id="reffn_5">5</a></sup></h3><p>DCGAN(Deep Convolutional Generative Adversarial Networks)全称为深度卷积对抗生成网络。顾名思义，其主要想法是将卷积神经网络(CNN)和GAN进行结合，在不改变GAN的基本原理的情况下较为有效地改善了其训练不稳定的问题。</p><p>DCGAN做出的主要改变有：</p><ul><li><p>使用卷积层和转置卷积层：引入了转置卷积层和卷积层分别作为生成器和判别器网络的主要组件。</p></li><li><p>去除全连接层：用全卷积层代替。</p></li><li>批归一化(Batch Normalization)：生成器和判别器都使用BN层。</li><li>修改激活函数：生成器输出层使用Tanh，其余层使用ReLU；判别器均使用LeakyReLU</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Daucloud/imagecdn/test/202401181914603.png" alt=""></p><center>Figure 4: 生成器转置卷积层示意图</center><h3 id="2-3-WGAN6"><a href="#2-3-WGAN6" class="headerlink" title="2.3    WGAN6"></a>2.3    WGAN<sup><a href="#fn_6" id="reffn_6">6</a></sup></h3><p>WGAN(Wasserstein Generative Adversarial Networks)引入了Wasserstein距离代替原来的JS散度作为GAN的损失函数，彻底解决了GAN训练不稳定的问题，是GAN发展史上里程碑式的工作之一。</p><p><img src="https://pic1.zhimg.com/80/v2-6be6e2ef3d15c4b10c2a943e9bf4db70_1440w.jpg#width=60%" alt="img" style="zoom:67%;" /></p><center>Figure 5: WGAN算法</center><p>可见，WGAN做出的最主要改动，即是对损失函数的更换。此外，其还做出了如下改变：</p><ul><li>判别器最后一层去掉sigmoid</li><li>每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c</li><li>不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行</li></ul><p>改进虽然简单，但是成效巨大。</p><h2 id="3-应用举例"><a href="#3-应用举例" class="headerlink" title="3    应用举例"></a>3    应用举例</h2><h3 id="3-1-「Edmond-de-Belamy」"><a href="#3-1-「Edmond-de-Belamy」" class="headerlink" title="3.1    「Edmond de Belamy」"></a>3.1    「Edmond de Belamy」</h3><p>在2018年末的佳士得纽约拍卖场上，来自巴黎的艺术团队<em>Obvious</em>使用GAN生成的画作「Edmond de Belamy」以超出估价40倍的43.25万美元成交，其右下角便印有纵横GAN领域的著名公式： <script type="math/tex">\min_G\max_D \mathbb{E}_{x}[\log D(x)]+\mathbb{E}_{z}[\log(1-D(G(z)))]</script></p><p>1968年，毕加索曾说：”计算机是没有用的。它们只会告诉你答案”。但在同一场拍卖会上，没有一幅毕加索的画作成交价格超过了「Edmond de Belamy」，这不禁令人唏嘘不已。</p><p><img src="https://cdn.jsdelivr.net/gh/Daucloud/imagecdn/test/202401181922436.png" alt=""></p><center>Figure 6: 「Edmond de Belamy」</center><h3 id="3-2-This-Person-Does-Not-Exist"><a href="#3-2-This-Person-Does-Not-Exist" class="headerlink" title="3.2    This Person Does Not Exist"></a>3.2    This Person Does Not Exist</h3><p><a href="https://thispersondoesnotexist.com/" target="_blank" rel="noopener">thispersondoesnotexist.com </a>每次进入该网址，都会生成一张世界上并不存在的人脸，而这正是使用GAN进行生成的。</p><p><img src="https://cdn.jsdelivr.net/gh/Daucloud/imagecdn/test/202401181917230.png" alt=""></p><center>Figure 7: This Person Does Not Exist</center><h3 id="3-3-二次元头像生成7"><a href="#3-3-二次元头像生成7" class="headerlink" title="3.3    二次元头像生成7"></a>3.3    二次元头像生成<sup><a href="#fn_7" id="reffn_7">7</a></sup></h3><p>使用GAN，你可以随心所欲生成二次元<del>老婆</del>头像：</p><p><img src="https://cdn.jsdelivr.net/gh/Daucloud/imagecdn/test/202401181917189.png" alt=""></p><center>Figure 8: GAN生成的二次元头像</center><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4    总结"></a>4    总结</h2><p>GAN是一种应用广泛、潜力巨大的生成模型。它使用判别器和生成器进行对抗性训练，并最终产生足够逼真的图像。但GAN在训练过程中通常存在生成样本过于随机、训练不稳定等缺点，因此涌现出了诸多变体对其进行改进：CGAN、DCGAN、WGAN …… </p><p>我们期待在将来能看到更具潜力的GAN模型和更富价值的GAN应用！</p><hr><p>Footnotes: </p><p>[1]:Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014, June 10). <em>Generative Adversarial Networks</em>. arXiv.org. <a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">https://arxiv.org/abs/1406.2661</a><br>[2]:事实上，这种 “纳什平衡” 是一种理想状态，实际训练难以达到。<br>[3]:关于梯度下降/上升法，可参考<a href="https://www.zhihu.com/question/305638940/answer/1639782992" target="_blank" rel="noopener">什么是梯度下降法？ - 马同学的回答 - 知乎</a><br>[4]:Mirza, M., &amp; Osindero, S. (2014, November 6). <em>Conditional generative adversarial nets</em>. arXiv.org. <a href="https://arxiv.org/abs/1411.1784" target="_blank" rel="noopener">https://arxiv.org/abs/1411.1784</a><br>[5]:Radford, A., Metz, L., &amp; Chintala, S. (2016, January 7). <em>Unsupervised representation learning with deep convolutional generative Adversarial Networks</em>. arXiv.org. <a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="noopener">https://arxiv.org/abs/1511.06434</a><br>[6]:<a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="noopener">令人拍案叫绝的Wasserstein GAN - 郑华滨的文章 - 知乎</a><br>[7]:Jin, Y., Zhang, J., Li, M., Tian, Y., Zhu, H., &amp; Fang, Z. (2017, August 18). <em>Towards the automatic anime characters creation with Generative Adversarial Networks</em>. arXiv.org. <a href="https://arxiv.org/abs/1708.05509" target="_blank" rel="noopener">https://arxiv.org/abs/1708.05509</a></p>]]></content>
      
      
      <categories>
          
          <category> cs learning </category>
          
          <category> intro to data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>雪</title>
      <link href="/2023/12/11/%E9%9B%AA/"/>
      <url>/2023/12/11/%E9%9B%AA/</url>
      
        <content type="html"><![CDATA[<h1 id="雪"><a href="#雪" class="headerlink" title="雪"></a>雪</h1><p>本来是不喜欢冬天的。</p><p>原来顶喜欢的季节是秋天。漫步街头，总能与一片飘然坠于身前的落叶不期而遇。爱极了这种意料之外的邂逅。</p><p>后来听过luna的<a href="https://www.bilibili.com/video/BV1ka4y1E7ik/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bd539b5a62c295726bece82272cc6c5a" target="_blank" rel="noopener"> あの夏のいつかは (在那個夏日的某天)</a>，夏天在我心中的地位渐渐变得与秋天不分伯仲了。每次看这首曲子的pv，都深为夏的热烈激动不已：“正是无数个微小片刻，汇集在一起造就这热烈的我”。生命本该如此。</p><p>但是，提到“冬”这个字，浮现在脑海中的总是凛冽的风、凋零的树、蜷缩的人……讨厌这种万事万物都被压抑的感觉：生命似乎被严酷的冬冰封了起来，待到来年春暖花开的时节，才能重焕生机。冬天少了点灵魂。</p><p>后来才渐渐发现我错了。冬不是没有灵魂；相反，冬的灵魂甚至比其他三个季节都更加可爱——这正是那名为“雪”的精灵。雪洁白、轻盈、古灵精怪，冬天的沉闷因为她的到来一扫而空。“忽如一夜春风来，千树万树梨花开。”被雪点缀后的世界，焕发出不亚于春的勃勃生机。人们不再蜷缩在被窝，校园里随处可见飞舞的雪球、奇形怪状的雪人和创意百出的雪地上的图案。冬天原来是这样一个趣味盎然的季节。</p><p>独身骑行在清华园内，走走停停，停停走走，一时将诸多ddl抛在脑后，只觉心灵也因这白茫茫的雪的世界变得一片通透。终于明白原来没有一个季节是不值得喜爱的，生命中也没有一个时刻是不值得热爱的。人活在世，本该如此。</p><p>本该这样喜欢冬天啊。</p><p><img src="/img/2023-12-11-雪/snow1.jpg" alt="snow1"></p><p><img src="/img/2023-12-11-雪/snow2.jpg" alt="snow2"></p><p><img src="/img/2023-12-11-雪/snow.jpg" alt="snow"></p>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八的东西 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello,World!</title>
      <link href="/2023/12/09/Hello-World/"/>
      <url>/2023/12/09/Hello-World/</url>
      
        <content type="html"><![CDATA[<p>建好了博客，第一件事当然是发一篇 Hello,world! (bushi</p><p><del>实际上没想好要写啥，所以随便水一篇博客</del></p><p>放首DT：</p><iframe allow="autoplay *; encrypted-media *; fullscreen *; clipboard-write" frameborder="0" height="175" style="width:100%;max-width:660px;overflow:hidden;border-radius:10px;" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-storage-access-by-user-activation allow-top-navigation-by-user-activation" src="https://embed.music.apple.com/cn/album/%E6%B5%81%E6%B2%99/1416149926?i=1416149940"></iframe>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八的东西 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
