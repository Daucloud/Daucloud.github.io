<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>é«˜ä»£é€‰è®²æœŸæœ«é‡ç‚¹æ•´ç†</title>
      <link href="/2024/06/18/%E9%AB%98%E4%BB%A3%E9%80%89%E8%AE%B2%E6%9C%9F%E6%9C%AB%E9%87%8D%E7%82%B9%E6%95%B4%E7%90%86/"/>
      <url>/2024/06/18/%E9%AB%98%E4%BB%A3%E9%80%89%E8%AE%B2%E6%9C%9F%E6%9C%AB%E9%87%8D%E7%82%B9%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<blockquote><p>æ­¤æ–‡ä¸ºå®—æ­£å®‡è€å¸ˆåœ¨æœ€åä¸€èŠ‚è¯¾æ‰€åˆ’å…­ä¸ªé‡ç‚¹çš„æ•´ç†ï¼Œæƒä½œæœŸæœ«é¢„ä¹ (x)</p></blockquote><h1 id="æ±‚ç‰¹å¾å¤šé¡¹å¼å’Œæå°å¤šé¡¹å¼">æ±‚ç‰¹å¾å¤šé¡¹å¼å’Œæå°å¤šé¡¹å¼</h1><h2 id="ç‰¹å¾å¤šé¡¹å¼">ç‰¹å¾å¤šé¡¹å¼</h2><p><span class="math inline">\(f(\lambda)=\left|\lambdaI-A\right|\)</span></p><h2 id="æå°å¤šé¡¹å¼">æå°å¤šé¡¹å¼</h2><h3 id="æ–¹æ³•-1">æ–¹æ³• 1</h3><p><strong>å®šç† 7.4</strong> <spanclass="math display">\[æå°å¤šé¡¹å¼m(\lambda)å’Œç‰¹å¾å¤šé¡¹å¼f(\lambda)å«æœ‰å®Œå…¨ç›¸åŒçš„æ ¹é›†å’Œä¸å¯çº¦å› å­é›†\]</span>ä¸€èˆ¬æ¥è¯´è€ƒè¯•æ¶‰åŠçš„çŸ©é˜µå¹¶ä¸å¤æ‚ï¼Œç»´æ•°ä¹Ÿè¾ƒå°ï¼Œå› æ­¤å¯ä»¥æ ¹æ®å®šç†7.4ï¼Œä»å°åˆ°å¤§å°è¯•<spanclass="math inline">\(f(\lambda)\)</span>çš„å› å¼<spanclass="math inline">\(a(\lambda)\)</span>æ˜¯å¦æ»¡è¶³<spanclass="math inline">\(a(A)=O\)</span>ä»¥ç¡®å®šæœ€å°å¤šé¡¹å¼</p><h3 id="æ–¹æ³•-2">æ–¹æ³• 2</h3><p>æ±‚å‡ºçŸ©é˜µ<span class="math inline">\(A\)</span>çš„ Jordanæ ‡å‡†å‹ï¼Œåˆ™<spanclass="math inline">\((\lambda-\lambda_i)\)</span>é¡¹åœ¨<spanclass="math inline">\(m(\lambda)\)</span>ä¸­çš„é‡æ•°å³ä¸ºç‰¹å¾å€¼ä¸º<spanclass="math inline">\(\lambda_i\)</span>çš„ Jordan å—çš„æœ€å¤§é˜¶æ•°ï¼›å¯¹äºå¹¿ä¹‰Jordan å—ï¼Œå¯ä»¥ç›´æ¥å–ä¸ºç›¸åº”çš„ä¸å¯çº¦å› å¼</p><h1 id="è®¡ç®—å¤æ–¹é˜µçš„-jordan-æ ‡å‡†å‹åŠå¯é€†çŸ©é˜µps.t.p-1apj">è®¡ç®—å¤æ–¹é˜µçš„Jordan æ ‡å‡†å‹åŠå¯é€†çŸ©é˜µ<spanclass="math inline">\(P,s.t.P^{-1}AP=J\)</span></h1><h2 id="jordan-æ ‡å‡†å‹è®¡ç®—">Jordan æ ‡å‡†å‹è®¡ç®—</h2><h3 id="æ–¹æ³•-1-1">æ–¹æ³• 1</h3><ol type="1"><li>æ±‚å‡ºç‰¹å¾å¤šé¡¹å¼<span class="math inline">\(f(\lambda)\)</span></li><li>åˆ™<span class="math inline">\(\lambda_i\)</span>çš„ä¸ªæ•°ä¸º<spanclass="math inline">\(r_i=n-rank(A-\lambda_iI)\)</span>ï¼Œä¹Ÿå³<spanclass="math inline">\(f(\lambda)\)</span>ä¸­<spanclass="math inline">\((\lambda-\lambda_i)\)</span>çš„æ¬¡æ•°</li><li>é˜¶æ•°ä¸º<span class="math inline">\(t\)</span>çš„ Jordanå—çš„ä¸ªæ•°ä¸º<spanclass="math inline">\(r_i(t)=rank(A-\lambda_iI)^{t+1}+rank(A-\lambda_iI)^{t-1}-2rank(A-\lambda_iI)^t\)</span>&gt; è§£é‡Šï¼šæ³¨æ„åˆ°<spanclass="math inline">\(rank(A-\lambda_iI)^m\)</span>ä¸­ä»…å«æœ‰åŸæœ¬é˜¶æ•°å¤§äº<spanclass="math inline">\(m\)</span>çš„<spanclass="math inline">\(\lambda_i\)</span>çš„ Jordan å—ï¼Œæ•…<spanclass="math inline">\(rank(A-\lambda_iI)^m-rank(A-\lambda_iI)^{m+1}\)</span>æ°å¥½ä¸ºé˜¶æ•°å¤§äº<spanclass="math inline">\(m\)</span>çš„<spanclass="math inline">\(\lambda_i\)</span>çš„ Jordan å—çš„ä¸ªæ•°ï¼Œå› æ­¤<spanclass="math inline">\(\left(rank(A-\lambda_iI)^{m-1}-rank(A-\lambda_iI)^m\right)-\left(rank(A-\lambda_iI)^m-rank(A-\lambda_iI)^{m+1}\right)\)</span>ä¸ºé˜¶æ•°å¤§äº<spanclass="math inline">\(m-1\)</span>çš„<spanclass="math inline">\(\lambda_i\)</span>çš„ Jordanå—çš„ä¸ªæ•°å’Œé˜¶æ•°å¤§äº<span class="math inline">\(m\)</span>çš„ Jordanå—çš„ä¸ªæ•°ä¹‹å·®ï¼Œæ¢è¨€ä¹‹å³ä¸ºé˜¶æ•°ä¸º<span class="math inline">\(m\)</span>çš„Jordan å—çš„ä¸ªæ•°</li></ol><ul><li>ä¸€äº› tricks:<ol type="1"><li>ç”±äºè€ƒè¯•æ¶‰åŠçš„ Jordan å—é€šå¸¸ç»´æ•°ä¸å¤§ï¼Œè€Œ<spanclass="math inline">\(m(\lambda_i)\)</span>åˆ™ç¡®å®šäº†å„<spanclass="math inline">\(\lambda_i\)</span>çš„ Jordanå—çš„æœ€å¤§é˜¶æ•°ï¼Œé€šè¿‡æ­¤å¾€å¾€å°±å¯æ±‚å‡º Jordan æ ‡å‡†å‹</li><li>å¯ä»¥é€šè¿‡è®¡ç®—<span class="math inline">\(dimKer(\lambda_iI-A)\)</span>æ¥è·çŸ¥<spanclass="math inline">\(\lambda_i\)</span>çš„ Jordan å—çš„ä¸ªæ•°ï¼ˆ<spanclass="math inline">\(Ker(\lambda_iI-A)\)</span>ä¸­çš„æ¯ä¸ªå‘é‡éƒ½å¯ä»¥ä½œä¸ºä¸€æ¡Jordan é“¾çš„ç»ˆç»“ï¼‰</li></ol></li></ul><h3 id="æ–¹æ³•-2-1">æ–¹æ³• 2</h3><p>æ±‚å‡º<span class="math inline">\(A\)</span>çš„åˆç­‰å› å­ç»„ï¼Œåˆ™ Jordanæ ‡å‡†å‹ä¸º</p><p><span class="math display">\[J=\begin{bmatrix}J\left(p_1^{k_1}\right)&amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;J\left(p_s^{k_s}\right)\end{bmatrix}\]</span></p><blockquote><p>æ³¨ï¼šè¯¥æ–¹é˜µç§°ä¸ºç¬¬ä¸‰ç§ç›¸ä¼¼æ ‡å‡†å‹ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰ç¬¬ä¸€ç±»ç›¸ä¼¼æ ‡å‡†å‹ï¼ˆæœ‰ç†æ ‡å‡†å‹ï¼‰å’Œç¬¬äºŒç±»ç›¸ä¼¼æ ‡å‡†å‹ï¼ˆåˆç­‰å› å­å‹é˜µå‹ï¼‰:</p></blockquote><p><span class="math display">\[C=\begin{bmatrix}C\left(d_1\right)&amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;C\left(d_r\right)\end{bmatrix}\\G=\begin{bmatrix}C\left(p_1^{k_1}\right)&amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;C\left(p_s^{k_s}\right)\end{bmatrix}\]</span></p><h2 id="å¯é€†çŸ©é˜µ-p-çš„æ±‚å–">å¯é€†çŸ©é˜µ P çš„æ±‚å–</h2><h3 id="æ–¹æ³•-1-2">æ–¹æ³• 1</h3><ol type="1"><li>æ±‚å‡º<spanclass="math inline">\(Ker(A-\lambda_iI)\)</span>ï¼Œå…¶ä¸­çš„å‘é‡å¯ä»¥ä½œä¸º<spanclass="math inline">\(\lambda_i\)</span>çš„ Jordan é“¾çš„ç»ˆç»“</li><li>ä½†æ˜¯éœ€è¦æ³¨æ„ï¼Œå¹¶ä¸æ˜¯æ¯ä¸ªå‘é‡éƒ½å¯ä»¥å½¢æˆé˜¶æ•°æ­£ç¡®çš„ Jordanå—ã€‚è­¬å¦‚ï¼Œè®¾<spanclass="math inline">\(Ker\left(A-\lambda_iI\right)\)</span>ç”±<spanclass="math inline">\(\alpha_1\)</span>å’Œ<spanclass="math inline">\(\alpha_2\)</span>å¼ æˆã€‚å¦‚æœä¸€ä¸ª Jordanå—ä¸ºäºŒé˜¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦è®¾æ­¤ Jordan é“¾çš„ç»ˆç»“ä¸º<spanclass="math inline">\(s\alpha_1+t\alpha_2\)</span>ï¼Œå¹¶ç¡®ä¿<spanclass="math inline">\(\left(A-\lambda_iI\right)x=s\alpha_1+t\alpha_2\)</span>æœ‰è§£ï¼ˆé€šè¿‡è€ƒè™‘å¢å¹¿çŸ©é˜µ<spanclass="math inline">\(\begin{bmatrix}A&amp;s\alpha_1+t\alpha_2\end{bmatrix}\)</span>ï¼‰</li></ol><blockquote><p>ä¸¾ä¸€é“ä¾‹é¢˜ï¼Œä¾¿äºç†è§£</p><p><em>é—®ï¼šæ±‚æ­¤å¤æ•°åŸŸä¸Šæ–¹é˜µçš„ Jordan æ ‡å‡†å‹å’Œå¯é€†çŸ©é˜µ Pï¼š</em></p><p><span class="math display">\[\begin{bmatrix}1&amp;-3&amp; 0&amp;3\\-2&amp;-6&amp; 0&amp;13\\0&amp;-3&amp; 1&amp;3\\-1&amp;-4&amp; 0&amp;8\\\end{bmatrix}\]</span></p><p><em>è§£:</em></p><p><spanclass="math display">\[f(\lambda)=(\lambda-1)^4,m(\lambda)=(\lambda-1)^3,æ•…Jordan æ ‡å‡†å‹ä¸º\begin{bmatrix}    1&amp;0&amp;0&amp;0\\    1&amp;1&amp;0&amp;0\\    0&amp;1&amp;1&amp;0\\    0&amp;0&amp;0&amp;1\\\end{bmatrix}\\Ker(A-I)=span\left(\begin{bmatrix}    3\\1\\0\\1\end{bmatrix},\begin{bmatrix}0\\0\\1\\0\end{bmatrix}\right)\\å–\alpha_4=\begin{bmatrix}    3\\1\\0\\1\end{bmatrix}ï¼Œè®¾\alpha_3=\begin{bmatrix}3s\\s\\t\\s\end{bmatrix}\\æ¬²ä½¿(A-I)\alpha_2=\alpha_3 æœ‰è§£ï¼Œè€ƒè™‘å¢å¹¿çŸ©é˜µ\begin{bmatrix}    0&amp;-3&amp;0&amp;3&amp;3s\\    -2&amp;-7&amp;0&amp;13&amp;s\\    0&amp;-3&amp;0&amp;3&amp;t\\    -1&amp;-4&amp;0&amp;7&amp;s\\\end{bmatrix}ï¼Œå¯çŸ¥ 3s=t\\ä¸å¦¨å– s=1ï¼Œæ•…\alpha_3=\begin{bmatrix}    3\\1\\3\\1\end{bmatrix}ï¼Œå¹¶è§£å¾—\alpha_2=\begin{bmatrix}3u+3\\v-1\\0\\0\end{bmatrix}\\å†ä»¤(A-I)\alpha_1=\alpha_2æœ‰è§£\\è€ƒè™‘å¢å¹¿çŸ©é˜µ\begin{bmatrix}    0&amp;-3&amp;0&amp;3&amp;3u+3\\    -2&amp;-7&amp;0&amp;13&amp;u-1\\    0&amp;-3&amp;0&amp;3&amp;v\\    -1&amp;-4&amp;0&amp;7&amp;w\\\end{bmatrix}ï¼Œæœ‰ 3u+3=v,2w-\frac{v}{3}=u-1\\ä¸å¦¨å–u=1,v=6,w=1,æ•…æœ‰\alpha_2=\begin{bmatrix}    6\\0\\6\\1\end{bmatrix},\alpha_1=\begin{bmatrix}7\\-2\\0\\0\end{bmatrix}\\æ•…çŸ¥ P=\begin{bmatrix}    7&amp;6&amp;3&amp;3\\    -2&amp;0&amp;1&amp;1\\    0&amp;6&amp;3&amp;0\\    0&amp;1&amp;1&amp;1\end{bmatrix}\]</span></p></blockquote><h3 id="æ–¹æ³•-2-2">æ–¹æ³• 2</h3><p>ç”±äº<span class="math inline">\(\lambda I-A\)</span>å’Œ<spanclass="math inline">\(\lambda I-J\)</span>æœ‰ç›¸åŒçš„ Smith æ ‡å‡†å‹ï¼Œè€ŒSmith æ ‡å‡†å‹æœ‰å›ºå®šçš„ç®—æ³•æ±‚è§£ï¼Œå› æ­¤å¯ä»¥æ±‚<spanclass="math inline">\(P_1^{-1}\left(\lambdaI-A\right)P_1=P_2^{-1}\left(\lambda I-J\right)P_2=S\)</span>ï¼Œåˆ™<spanclass="math inline">\(P=P_2P_1^{-1}\)</span></p><h1 id="lambda-çŸ©é˜µå¯¹è§’åŒ–åˆç­‰å› å­ä¸å˜å› å­å’Œ-jordan-æ ‡å‡†å‹çš„å…³ç³»"><spanclass="math inline">\(\lambda-\)</span>çŸ©é˜µå¯¹è§’åŒ–ï¼›åˆç­‰å› å­ã€ä¸å˜å› å­å’ŒJordan æ ‡å‡†å‹çš„å…³ç³»</h1><h2 id="lambda-çŸ©é˜µå¯¹è§’åŒ–"><spanclass="math inline">\(\lambda-\)</span>çŸ©é˜µå¯¹è§’åŒ–</h2><p>è¯¾æœ¬æœ‰è¯¦å°½çš„ç®—æ³•ã€‚ç®€è€Œè¨€ä¹‹å³æ˜¯ä¸æ–­é™ä½<spanclass="math inline">\(a_{11}\)</span>çš„æ¬¡æ•° <imgsrc="https://img.picgo.net/2024/06/18/2024061812322236777062ff2476a40.jpg"alt="3663133d2ba182b3d2aca82db4134e2.jpg" /> <imgsrc="https://img.picgo.net/2024/06/18/202406181233913c52ac3b6584e7b8d.jpg"alt="9b55172bde8fb4579a9083312526854.jpg" /></p><h2 id="åˆç­‰å› å­ä¸å˜å› å­ä¸-jordan-æ ‡å‡†å‹çš„å…³ç³»">åˆç­‰å› å­ã€ä¸å˜å› å­ä¸Jordan æ ‡å‡†å‹çš„å…³ç³»</h2><h3 id="ä¸‰ç§åŸºæœ¬å› å­çš„æ¦‚å¿µ">ä¸‰ç§åŸºæœ¬å› å­çš„æ¦‚å¿µ</h3><ul><li>å¯¹äº<span class="math inline">\(\lambda-\)</span>çŸ©é˜µ 1.ä¸å˜å› å­ï¼šSmith æ ‡å‡†å‹å¯¹è§’çº¿ä¸Šçš„æ‰€æœ‰éé›¶å…ƒç´ ï¼š<spanclass="math inline">\(d_1(\lambda),...,d_r(\lambda)\)</span> 2.è¡Œåˆ—å¼å› å­ï¼šæ‰€æœ‰ k é˜¶éé›¶å­è¡Œåˆ—å¼çš„é¦– 1 æœ€å¤§å…¬å› å­ï¼š<spanclass="math inline">\(D_1,...,D_r\)</span> 3.åˆç­‰å› å­ï¼šä¸å˜å› å­çš„å‡†ç´ å› å­å…¨ä½“ï¼ˆ1 ä¸è®¡å…¥ï¼‰ &gt;ä¸å˜å› å­å’Œè¡Œåˆ—å¼å› å­äº’ç›¸å†³å®šï¼š<spanclass="math inline">\(D_k=\prod_{i=1}^kd_i,d_k=\frac{D_k}{D_{k-1}}\)</span></li><li>å¯¹äºæ•°å­—çŸ©é˜µï¼Œå…¶ä¸‰ç§åŸºæœ¬å› å­æŒ‡çš„æ˜¯<spanclass="math inline">\(\lambda I-A\)</span>çš„ä¸‰ç§åŸºæœ¬å› å­ï¼ˆä½†æ˜¯ 1å‡ä¸è®¡å…¥ï¼‰</li></ul><h3 id="ä»åˆç­‰å› å­æ±‚-jordan-æ ‡å‡†å‹">ä»åˆç­‰å› å­æ±‚ Jordan æ ‡å‡†å‹</h3><p><span class="math display">\[J=\begin{bmatrix}J\left(p_1^{k_1}\right)&amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;J\left(p_s^{k_s}\right)\end{bmatrix}\]</span></p><h1 id="gram-schmidt-æ­£äº¤åŒ–">Gram-Schmidt æ­£äº¤åŒ–</h1><p>è®¾<spanclass="math inline">\(A=\begin{bmatrix}v_1&amp;v_2&amp;\cdots&amp;v_n\end{bmatrix}\)</span>åˆ™å…¶æ­£äº¤åŒ–ç»“æœ<spanclass="math inline">\(Q=\begin{bmatrix}w_1&amp;w_2&amp;\cdots&amp;w_n\end{bmatrix}\)</span>å…¶ä¸­ï¼š</p>$$<span class="math display">\[\begin{aligned}&amp;w_1=v_1\\&amp;w_2=v_2-\frac{\left&lt;w_1,v_2\right&gt;}{\left&lt;w_1,w_1\right&gt;}w_1\\&amp;w_3=v_3-\frac{\left&lt;w_1,v_3\right&gt;}{\left&lt;w_1,w_1\right&gt;}w_1-\frac{\left&lt;w_2,v_3\right&gt;}{\left&lt;w_2,w_2\right&gt;}w_2\\&amp;\vdots\\&amp;w_n=v_n-\frac{\left&lt;w_1,v_n\right&gt;}{\left&lt;w_1,w_1\right&gt;}w_1-\frac{\left&lt;w_2,v_n\right&gt;}{\left&lt;w_2,w_2\right&gt;}w_2-\cdots-\frac{\left&lt;w_n,v_n\right&gt;}{\left&lt;w_n,w_n\right&gt;}w_n\\\end{aligned}\]</span><p>$$ ä¸Šå­¦æœŸçš„å†…å®¹ï¼Œä¸è¿‡å¤šèµ˜è¿°</p><h1 id="è§„èŒƒæ–¹é˜µè°±åˆ†è§£">è§„èŒƒæ–¹é˜µè°±åˆ†è§£</h1><ol type="1"><li>æ±‚å‡ºè§„èŒƒæ–¹é˜µ A çš„ç‰¹å¾å¤šé¡¹å¼ï¼Œå¹¶è§£å‡ºç‰¹å¾å€¼åˆ™<spanclass="math inline">\(U^{-1}AU=diag(\lambda_{1},\cdots,\lambda_n)\)</span></li><li>æ±‚å‡º<spanclass="math inline">\(Ker(\lambda_iI-A)\)</span>çš„ä¸€ç»„åŸºï¼Œå¯¹å…¶æ–½è¡ŒGram-Schimidt æ­£äº¤åŒ–</li><li>å°†ä¸Šä¸€æ­¥å¾—åˆ°çš„æ‰€æœ‰å‘é‡æ’åˆ—èµ·æ¥ï¼Œå³å¾—åˆ°é…‰æ–¹é˜µ<spanclass="math inline">\(U\)</span></li></ol><h1 id="jordan-æ ‡å‡†å‹çš„åº”ç”¨å’Œå¯å¯¹è§’åŒ–æ¡ä»¶">Jordanæ ‡å‡†å‹çš„åº”ç”¨å’Œå¯å¯¹è§’åŒ–æ¡ä»¶</h1><p>è®¾ A ä¸º n é˜¶å¤æ–¹é˜µï¼Œåˆ™å¦‚ä¸‹å‘½é¢˜ç­‰ä»·ï¼š</p><ol type="1"><li>A åœ¨<span class="math inline">\(\mathbb{C}\)</span>å¯å¯¹è§’åŒ–</li><li>A çš„å‡ ä½•é‡æ•°ä¸º nï¼ˆæœ‰ n ä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡ï¼‰</li><li>A åœ¨å¤æ•°åŸŸä¸Šçš„åˆç­‰å› å­å‡ä¸º 1 æ¬¡ï¼ˆå³åˆç­‰å› å­å‡æ— é‡æ ¹ï¼‰</li><li>A çš„ä¸å˜å› å­å‡æ— é‡æ ¹</li><li>A çš„æå°å¤šé¡¹å¼æ— é‡æ ¹</li><li><span class="math inline">\(\forallc\in\mathbb{C},rank(cI-A)=rank\left((cI-A)^2\right)\)</span>æ­¤å¤–ï¼Œä»¥ä¸Šå‘½é¢˜çš„ä¸€ä¸ªå……åˆ†æ¡ä»¶æ˜¯<spanclass="math inline">\(f(\lambda)\)</span>æ— é‡æ ¹</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Chapter2 Using ğŸ¤— Transformers</title>
      <link href="/2024/05/14/NLP-Course-of-Hugging-Face/"/>
      <url>/2024/05/14/NLP-Course-of-Hugging-Face/</url>
      
        <content type="html"><![CDATA[<h1 id="behind-the-pipeline">Behind the pipeline</h1><ul><li><code>pipeline()</code> groups the "preprocessing-pass the inputsthrough the model-postprocessing" together <imgsrc="https://cdn.jsdelivr.net/gh/Daucloud/imagecdn/test/202404291430930.png"alt="|525" /> ## Preprocessing with a tokenizer</li><li>convert the raw context into vectors with a tokenizer:<ol type="1"><li>splitting the input into tokens(words, subwords, symbols likepunctuation)</li><li>each token to an integer</li><li>adding additional inputs that maybe useful to the model <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">checkpoint=<span class="string">"checkoutName"</span></span><br><span class="line">tokenizer=AutoTokenizer.from_pretrained(checkpoint)<span class="comment"># all the preprocessing needs to be done exactly the same way as when the model was pretrained</span></span><br><span class="line">raw_input=<span class="string">"context"</span></span><br><span class="line">inputs=tokenizer(raw_inputs,padding=<span class="literal">True</span>,truncation=<span class="literal">True</span>,return_tensors=<span class="string">"pt"</span>)</span><br><span class="line"><span class="comment"># as for the return_tensors, pt: Pytorch, tf: TensorFlow, np: Numpy, jax: JAX</span></span><br><span class="line">print(inputs)</span><br><span class="line"><span class="comment">### the results, is a dictionary, which contains two key-value pair</span></span><br><span class="line">&#123;</span><br><span class="line"> <span class="string">"input_ids"</span>: tensor([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]), <span class="comment"># the unique identifiers for each token</span></span><br><span class="line"> <span class="string">"attention_mask"</span>: tensor([<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> ##Going through the model</li></ol></li><li>convert input IDs into logits <imgsrc="https://raw.githubusercontent.com/Daucloud/imagecdn/main/test/202405140923963.png"alt="image.png" /></li><li>ğŸ¤— provides the <code>AutoModel</code> class, which corresponds tothe hidden states step <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel</span><br><span class="line">checkout=<span class="string">"checkoutname"</span></span><br><span class="line">model=AutoModel.from_pretrained(checkout)</span><br><span class="line">outputs=(**inputs)</span><br><span class="line"><span class="comment"># the Automodel converts the inputs(seen last part) into a three-dimensional vector:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">1. batch size: the number of sequences processed at a time</span></span><br><span class="line"><span class="string">2. sequence length</span></span><br><span class="line"><span class="string">3. Hidden size: usually very large(768, 3072, even more)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print(outputs.last_hidden_state.size) <span class="comment"># torch.Size([2,16,768])</span></span><br></pre></td></tr></table></figure></li><li>There are also some other architectures for specific tasks. You canunderstand them as <code>AutoModel</code> followed by the head<ul><li>ForCausalLM</li><li>ForMaskedLM</li><li>ForMultipleChoice</li><li>ForQuestionAnswering</li><li>ForSequenceClassification</li><li>ForTokenClassification</li><li>other ğŸ¤— <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line">checkout=<span class="string">"checkoutName"</span></span><br><span class="line">model=AutoModelForSequenceClassification.from_pretrained(checkout)</span><br><span class="line">outputs=(**inputs)</span><br><span class="line">print(outputs.logits.shape) <span class="comment">#torch.Size([2,2]), the size is much smaller than the results of AutoModel</span></span><br></pre></td></tr></table></figure> ## Postprocessing the output</li></ul></li><li>Convert the logits into predictions , aka, convert the raw,unnormalized scores to probabilties which can be understood byhuman</li><li>Usually we will use a SoftMax Layer to work on this <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">predictions=torch.nn.functional.softmax(outputs.logits, dim=<span class="number">-1</span>)</span><br><span class="line">print(predictions)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[5.0e-2,9.5e-1],[1.0e-1,9.0-1]],grad_fn=&lt;SoftmaxBackward&gt;)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># we can inspect the id2label the following way:</span></span><br><span class="line">model.config.id2label <span class="comment"># &#123;0:'NEGATIVE',1:'POSITIVE'&#125;</span></span><br></pre></td></tr></table></figure> #Models</li><li><code>AutoClass</code> and it relatives mentioned above are simplewrappers which can automatically guess the architechture from yourcheckpoint ## Creating A Transformer <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertConfig, BertModel, </span><br><span class="line">config=BertConfig()</span><br><span class="line">model=BertModel(config)<span class="comment"># in this way, you will get a randomly initialized bert model, which will output gibberish</span></span><br><span class="line">model=BertModel.from_pretrained(<span class="string">"bert-base-cased"</span>)<span class="comment"># the model is instantiate with the checkpoint trained by the bert team, which is less time-consuming and more environment-friendly; you can also replace the BertModel with AutoClass. Actually, it is more suggested to use AutoModel rather than a specific model, which will also work even if the architechture is different</span></span><br></pre></td></tr></table></figure></li><li>once you used the checkpoint, the weights are downloaded and cachedto <code>~/.cache/huggingface/transformers</code></li><li>you can use the <code>save_pretrained</code> method to save themodel to your disk: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.save_pretrained(<span class="string">"path/to/directory"</span>)</span><br><span class="line">!ls path/to/directory</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">config.json pytorch_model.bin</span></span><br><span class="line"><span class="string"># the two files go hand in hand, the `config.json` contains the attributes necessary to build the architechture, and the `pytorch_model.bin` contains the weights(checkpoints) which are the parameters of your model</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure> # Tokenizers ## Algorithms fortokenization ### Word-Based</li></ul><ol type="1"><li>split the the words according to the given marks, such as spaces,punctuations...</li><li>map each word to an ID. The ID is determined through the vocabulary,which is usually very large. For example, the size of a Englishvocabulary may be as large as 500,000</li><li>As for the words that are not in the vocabulary, they are oftenrepresented by the unkown token such as <code>[UNK]</code> or<code>&lt;unk&gt;</code>. However, as you can imagine, this will loseimformation, so it is sensible to avoid the unknown tokens as much aspossible ### Character-Based</li><li>split the sentences by the characters &gt; This method willdefintely reduce the amount of unknown tokens. However, is will alsocause the vocabulary to be too large and the results less meaningful ###Subword Tokenization</li><li>representing the rare words with the composite of frequently usedwords &gt; this will spare the space while remaining the semanticmeanings as much as possible, which is especially useful toagglutinative languages such as Turkish</li></ol><h2 id="examples"><em>examples</em>:</h2><p>Byte -level BPE: GPT-2 - WordPiece: BERT - SentencePiece or Unigram:multilingual models ## Loading and Saving - nearly the same as loadingand saving the models: use <code>from_pretrained</code> and<code>save_pretrained</code> - the things that cached - algorithms:similar to <em>architechture</em> of the models - vocabularay: similarto <em>weights</em>of the models ## Encoding - there are two steps whileencoding ### tokenization <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer=AutoTokenizer.from_pretrained(<span class="string">"modelName"</span>)</span><br><span class="line">tokens=tokenizer.tokenize(<span class="string">"sequence"</span>)</span><br><span class="line"></span><br><span class="line">print(tokens) <span class="comment"># print the results of spilting</span></span><br></pre></td></tr></table></figure> ### From tokens to input IDs - themodels can only accept the tensors as inputs, so we need to map ourtokens in to numbers - use the vocabulary to work on this, which is adictionary mapping the tokens to numbers # Handling Multiple Sequences## Models expect a batch of inputs <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">model=AutoModelForSequenceClassification.from_pretrained(<span class="string">"checkpointName"</span>)</span><br><span class="line">tokenizer=AutoTokenizer.from_pretrained(<span class="string">"checkpointName"</span>)</span><br><span class="line"></span><br><span class="line">tokens=tokenizer.tokenize(<span class="string">"raw inputs"</span>)</span><br><span class="line">token_ids=tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line">input_ids=torch.tensor(token_ids)</span><br><span class="line"></span><br><span class="line">model(input_ids)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">model([input_ids]) <span class="comment"># this line will succeed</span></span><br></pre></td></tr></table></figure> ## Padding the inputs - thebatch of inputs must have the same lengths to get through the model, forthe shape of tensor is rectangle. This is why will introduce the<code>padding_id</code> to pad the inputs. You can use the attributes<code>pad_token_id</code> of a tokenizer to get access it ## Attentionmasks - the key features of transformers is the attention layers thatcontextualize each token. As a consequence, the padding ids will alsomake a difference to the output, which is not expected. - The Attentionmasks comes for this. The value 0 represents ignoring the correspondingtokens and 1 do the opposite <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">batched_ids = [</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, <span class="number">200</span>],</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, tokenizer.pad_token_id],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">attention_mask = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))</span><br><span class="line">print(outputs.logits)</span><br></pre></td></tr></table></figure> ## Longer sequences - thereexists a limit to the length with all transformer models. If you want topass a sequence longer than the limits, you should<strong>truncate</strong> your sequence or change to a model allowinglonger inputs. # Putting it all together ## put the tokenization stepstogether - As a review, there are three steps we need to do to tokenizethe raw inputs:<code>tokenizer.tokenize()</code>--&gt;<code>tokenizer.convert_tokens_to_ids()</code>--&gt;<code>torch.tensor()</code>- For convenience, the <code>ğŸ¤— transformers</code> actually provides ahigh-level function to put all these steps together, which is<code>tokenizer()</code> itself <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">tokenizer=AutoTokenizer.from_pretrained(<span class="string">"checkpointName"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line">sequence=<span class="string">"1"</span></span><br><span class="line">input1=tokenizer(sequence) <span class="comment"># valid</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line">sequences=[<span class="string">"1"</span>,<span class="string">"2"</span>]</span><br><span class="line">input2=tokenizer(sequneces)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 pad</span></span><br><span class="line">input3=tokenizer(sequences, padding=<span class="string">"longest"</span>) <span class="comment"># pad to the maximum sequece length</span></span><br><span class="line">input4=tokenizer(sequences, padding=<span class="string">"max_length"</span>) <span class="comment"># pad to the model maximum length</span></span><br><span class="line">input5=tokenizer(sequences, padding=<span class="string">"max_length"</span>, max_length=<span class="number">8</span>) <span class="comment"># pad to the specified maximum length</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 truncate</span></span><br><span class="line">input6=tokenizer(sequences, truncation=<span class="literal">True</span>) <span class="comment"># truncate the sequences longer than the model limit</span></span><br><span class="line">input7=tokenizer(sequences, max_length=<span class="number">8</span>, truncation=<span class="literal">True</span>) <span class="comment"># truncate the sequences longer than the specified length</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5 switch the type of tensors returned</span></span><br><span class="line">input8=tokenizer(sequences, padding=<span class="literal">True</span>, return_tensors=<span class="string">"pt"</span>) <span class="comment"># pt stands for the Pytorch, tf for TensorFlow, np for NumPy</span></span><br></pre></td></tr></table></figure> ## Special words - Some modelsadd the special token such as <code>[CLS]</code> at the beginning, someadd the special token such as <code>[SEP]</code> at the end, some addboth, and some add none. ## Wrapping up: From tokenizer to model<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transfomers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"></span><br><span class="line">tokenizer=Autokenizer.form_pretrained(<span class="string">"checkpointName"</span>)</span><br><span class="line">model=AutoModel.form_pretrained(<span class="string">"checkpointName"</span>)</span><br><span class="line"></span><br><span class="line">sequence=<span class="string">"1"</span></span><br><span class="line">Input=tokenizer(sequence)</span><br><span class="line">Output=model(**Input)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> CS Learing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> ğŸ¤— </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Chapter1 Transformer Models</title>
      <link href="/2024/05/10/NLP-Course-of-Hugging-Face/"/>
      <url>/2024/05/10/NLP-Course-of-Hugging-Face/</url>
      
        <content type="html"><![CDATA[<blockquote><p>æœ€è¿‘æ‰“ç®—å…¥é—¨ NLPï¼Œåœ¨è‡ªå­¦ ğŸ¤— çš„<ahref="https://huggingface.co/learn/nlp-course/chapter1/1">NLPCourse</a>ï¼Œä½†æ˜¯æ„Ÿè§‰è‡ªå·±è¿‡äºæ‘†çƒ‚äº†ã€‚äºæ˜¯æ‰“ç®—è¾¹å­¦è¾¹åšç¬”è®°ï¼Œäº‰å–åœ¨æœŸæœ«ä¹‹å‰æŠŠæœ¬è¯¾ç¨‹å­¦å®Œ</p></blockquote><h1 id="pipeline"><code>Pipeline()</code></h1><ul><li>the most basic object in the ğŸ¤— Transformers llibrary</li><li>It can:<ul><li>feature-extraction(get a vector representing the text)</li><li>fill-task</li><li>ner(entity recognition)</li><li>question-answering</li><li>sentiment-analysis</li><li>summarization</li><li>text-generation</li><li>translation</li><li>zero-shot classification</li></ul></li></ul><h2 id="zero-shot-classification">Zero-shot Classification</h2><ul><li>you can casually assign the labels</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">classifier = pipeline(<span class="string">"zero-shot-classification"</span>)</span><br><span class="line">classifier(</span><br><span class="line">   <span class="string">"I play Genshin Impact!"</span>,</span><br><span class="line">   candidate_labels=[<span class="string">"op"</span>,<span class="string">"2-dimensional"</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="text-generation">Text Generation</h2><ul><li>involves randomness</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">generator=pipeline(<span class="string">"text-generation"</span>)</span><br><span class="line">generator(</span><br><span class="line">  <span class="string">"prompts"</span>,</span><br><span class="line">  max_length=<span class="number">15</span>, <span class="comment">#the max length of the text</span></span><br><span class="line">  num_return_sequence=<span class="number">3</span> <span class="comment">#How many texts are gonna generated</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="mask-filling">Mask filling</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">unmasker=pipeline(<span class="string">"fill-mask"</span>)</span><br><span class="line">unmasker(<span class="string">"I play &lt;mask&gt; Impact!"</span>,top_k=<span class="number">2</span>) <span class="comment">#top_k decides the time it does;&lt;mask&gt; depends on what model you are using</span></span><br></pre></td></tr></table></figure><h2 id="named-entity-recognition">Named entity recognition</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ner=pipeline(<span class="string">"ner"</span>,grouped_entities=<span class="literal">True</span>)<span class="comment">#'grouped_entities=True' is used to enable the model to put multi-words together</span></span><br></pre></td></tr></table></figure><h2 id="question-answering">Question answering</h2><ul><li>answer a question using given context</li><li>extracting answers from the context instead of generatinganswers</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">question_answerer=pipeline(<span class="string">"question-answering"</span>)</span><br><span class="line">question_answerer(</span><br><span class="line">  question=<span class="string">"where do I work?"</span>,</span><br><span class="line">  context=<span class="string">"My name is Sylvain and I work at Hugging Face in Brooklyn"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="summarization">Summarization</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">summarizer=pipeline(<span class="string">"summarization"</span>)</span><br><span class="line">summarizer(</span><br><span class="line">   <span class="string">"""</span></span><br><span class="line"><span class="string">context</span></span><br><span class="line"><span class="string">   """</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="translation">Translation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">translator=pipeline(<span class="string">"translation"</span>,model=<span class="string">"modelName"</span>)</span><br><span class="line">translator(<span class="string">"contex"</span>)</span><br></pre></td></tr></table></figure><h1 id="brief-intro-to-transformers">Brief intro to Transformers</h1><h2 id="general-categorization">General categorization</h2><ul><li>GPT-like: auto-regressive</li><li>BERT-like: auto-encoding</li><li>BART/T5-like: sequence-to-sequence</li></ul><h2 id="transfer-learning">Transfer Learning</h2><h3 id="pretraning">Pretraning</h3><ul><li>the act of traing a model from scratch <imgsrc="https://github.com/daucloud/imagecdn/raw/main/test/202404251741650.png" /></li></ul><h3 id="fine-tuning">Fine-tuning</h3><ul><li>training on the top of pretrained models with a dataset specific tothe target task <imgsrc="https://github.com/daucloud/imagecdn/raw/main/test/202404251746195.png"alt="image.png" /></li></ul><h2 id="general-architecture">General architecture</h2><ul><li>Transformers model is generally composed of two blocks:<ul><li>Encoder: receives inputs and builds representations for them</li><li>Decoder: use the outputs of encoder to generate target outputs</li></ul></li><li>Various tasks requires different blocks:<ul><li>Encoder-only: sentence classfication; NER</li><li>Decoder-only: text generation</li><li>Encoder-decoder models or sequence-to-sequence models: translationor summarization</li></ul></li></ul><h2 id="attention-layer">Attention layer</h2><ul><li>pay specific attention to certain words while ignoring others moreor less</li></ul><h2 id="original-architechture">Original Architechture</h2><ul><li>the encoder translate all the words</li><li>the decoder is only allowed to translate by the past words; butlater it can get all the outpus of encoer to better translate the word<imgsrc="https://github.com/daucloud/imagecdn/raw/main/test/202404271755969.png"alt="image.png" /></li></ul>]]></content>
      
      
      <categories>
          
          <category> CS Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> ğŸ¤— </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>æ¼«æ­¥æ‚è®°</title>
      <link href="/2024/03/17/%E6%BC%AB%E6%AD%A5%E6%9D%82%E8%AE%B0/"/>
      <url>/2024/03/17/%E6%BC%AB%E6%AD%A5%E6%9D%82%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>å–œæ¬¢åœ¨å°‘äººçš„å¤œï¼Œä¸€ä¸ªäººæ¼«æ— ç›®çš„åœ°åœ¨æ ¡å›­é‡Œèµ°èµ°åœåœã€‚ç˜«åœ¨å®¿èˆæ‰“äº†ä¸€ä¸‹åˆçš„æ¸¸æˆï¼Œå¤´è„‘æ˜æ˜æ²‰æ²‰ã€‚äºæ˜¯æ·ä¸‹æ‰‹æœºï¼ŒæŠ„èµ·å¤–å¥—ï¼Œæ‹©å®¿èˆæ¥¼è¥¿è¾¹çš„ä¸€æ¡åƒ»å¹½å°å¾„ï¼Œå¼€å§‹åœ¨å…¥å¤œçš„å›­å­é‡Œéšæ„æ¸¸è¡ã€‚æ­£å€¼å†¬æ˜¥ä¹‹äº¤ï¼ŒåŒ—äº¬çš„é£å·²ç„¶æ”¶èµ·äº†å†¬æ—¥ç¡Œäººçš„æ£±è§’ï¼Œä½ä»¥å¤œçš„äº›è®¸æ¸…å†·çš„å¯’æ„ï¼Œè¿é¢å¹æ¥ï¼Œä»¤äººæ„Ÿåˆ°èˆ’é€‚åˆæ¸…é†’ã€‚äº«å—ç€é£çš„æ„Ÿè§‰ï¼Œå¿ƒçµé€æ¸ä»æ—¥å¸¸ç”Ÿæ´»çš„ç¹çå’Œä¸–ä¿—æ„ä¹‰çš„å‹åŠ›ä¸­æŠ½ç¦»ï¼ŒæŸ”è½¯çš„è§¦è§’ä¸çŸ¥ä¸è§‰å‘å››å‘¨å»¶ä¼¸å¼€æ¥ï¼šè½å¶éšé£ç§»åŠ¨æ—¶æ‘©æ“¦åœ°é¢çš„ç°Œç°Œå£°ã€æ ¡æ²³åå°„è·¯ç¯çš„ç²¼ç²¼æ³¢å…‰ã€é“æ—å®¿èˆæ¥¼ä¸­é€šäº®çš„ç¯ã€é»‘å¤œä¸­è½»æŸ”æ‘‡åŠ¨çš„æŸ³æ¡ã€å¥‡å½¢æ€ªçŠ¶çš„å…‰ç§ƒæä¸«ã€å«è‹å¾…æ”¾çš„ç²‰è‰²è““è•¾â€¦â€¦å¿ƒçµä»¿ä½›è¤ªå»äº†è€æˆçš„å¤–å£³ï¼Œå¯¹å¸ç©ºè§æƒ¯çš„ä¸€åˆ‡é‡åˆå…´èµ·äº†å­©ç«¥èˆ¬çš„å¥½å¥‡ï¼Œç›®ä¹‹æ‰€åŠã€è€³ä¹‹æ‰€é—»éƒ½æ˜¯å†æ–°å¥‡ä¸è¿‡çš„äº‹ç‰©ã€‚</p><p>è‚†æ„çš„å¤–ç•Œä½“éªŒå”¤é†’äº†ä¸°ç›ˆçš„å†…åœ¨æ„Ÿå—ï¼Œéª¨é«“æ·±å¤„çš„å­¤ç‹¬æ„Ÿä¸€ç‚¹ç‚¹æ¸—äº†å‡ºæ¥ï¼Œæ‚„ç„¶æ¼«ä¸Šäº†å¿ƒå¤´ã€‚æˆ‘æ˜¯ä¸€ä¸ªå­¤ç‹¬çš„äººï¼Œå†…å¿ƒæ·±å¤„ï¼Œå§‹ç»ˆæœ‰ä¸€éƒ¨åˆ†æ˜¯æ— æ³•å¯¹äººæ•å¼€ã€ç”šè‡³æˆ‘è‡ªå·±ä¹Ÿä¸ç”šäº†ç„¶çš„ã€‚å¾ˆå¤šæ—¶å€™ï¼Œæˆ‘å¯¹è¿™ç§å­¤ç‹¬æ„Ÿæ˜¯å¼‚å¸¸ææƒ§çš„ï¼šåœ¨æŸäº›éš¾çœ çš„æ·±å¤œï¼Œæˆ‘ä¼šè¢«æ½®æ°´èˆ¬æ¶Œå‡ºçš„å­¤ç‹¬çª’å¡ä½å‘¼å¸ã€‚æˆ‘æ¸´æœ›æœ‰ä¸€åçœŸæ­£çš„çµé­‚çŸ¥å·±èƒ½å¤Ÿåˆ†äº«ä¸€åˆ‡ã€å€¾åæ‰€æœ‰ã€‚å¯æˆ–æ˜¯ä¸æ•¢ï¼Œæˆ–æ˜¯æ— ç¼˜ï¼Œç»ˆç©¶æ²¡èƒ½æ‰¾åˆ°ã€‚ä½†æ˜¯ï¼Œåœ¨è¿™ä¸ªéšæ„æ¼«æ­¥çš„æ™šä¸Šï¼Œæˆ‘å´é€æ¸å­¦ä¼šäº†äº«å—å­¤ç‹¬ã€‚ç‹¬è‡ªä¸€äººï¼Œå¯ä»¥éšå¿ƒæ‰€æ¬²åœ°åœæ­¥å’Œèµ·æ­¥ã€éšå¿ƒæ‰€æ¬²åœ°è¹²ä¸‹æ¥ç«¯è¯¦é“æ—çš„æ¯è‰ã€éšå¿ƒæ‰€æ¬²åœ°æ‹¾èµ·è„šè¾¹çš„ææ¡ã€‚åŸæ¥å­¤ç‹¬å¹¶ä¸æ˜¯ä»€ä¹ˆå¯æ€•çš„äº‹æƒ…ã€‚æ¯ä¸ªäººç”Ÿæ¥å­¤ç‹¬ï¼Œè¿™æ˜¯ä¸€ä¸ªäººèº«ä¸ºä»–è‡ªå·±è€Œä¸æ˜¯å…¶ä»–çš„ä»€ä¹ˆäººæ‰€å¿…ç„¶å¸¦æ¥çš„ç‹¬ä¸€æ— äºŒçš„ä½“éªŒã€‚æˆ‘å½“ç„¶ä»æ¸´æœ›è§…åˆ°ä¸€åçŸ¥å·±ï¼Œä½†æˆ‘å·²å­¦ä¼šæ‹¥æŠ±è¿™ä»½å­¤ç‹¬ã€‚</p><p>è¸±æ­¥åˆ°å¤§ç¤¼å ‚åŒ—è¾¹çš„æ²³é“æ—¶ï¼Œçªç„¶æœ‰ä¸€é˜µæçŒ›çƒˆçš„é£é¡ºç€æˆ‘å‰è¿›çš„æ–¹å‘å¹æ¥ã€‚æˆ‘ç«‹ä½ç‰‡åˆ»ï¼Œæ„Ÿå—ç€é£åœ¨è€³è¾¹çš„å‘¼å•¸å£°å’Œå¼ºé£ä¸‹ç´§è´´èº«ä½“çš„è¡£ç‰©ã€‚éšåæˆ‘è½¬è¿‡èº«ï¼Œå¼ å¼€åŒè‡‚ï¼Œé—­ç›®ä½“ä¼šå¯’å†·çš„æ°”æµå†²å‡»é¢é¢Šæ—¶çš„å¾®ç—›ã€‚æˆ‘é€†é£èµ°äº†å‡ æ­¥ï¼Œéšååˆè½¬è¿‡èº«å»ï¼Œé¡ºé£èµ°äº†èµ·æ¥ã€‚æˆ‘è¶Šèµ°è¶Šå¿«ï¼Œè¶Šèµ°è¶Šè·³è·ƒï¼Œè¶Šèµ°è¶Šè½»ç›ˆã€‚æˆ‘å¿ä¸ä½ç¬‘äº†èµ·æ¥ï¼Œä¸€å¼€å§‹æ˜¯å¾®ç¬‘ï¼Œåæ¥æ˜¯å’§å˜´ç¬‘ï¼Œæœ€åç«Ÿæ¼”å˜æˆäº†å“ˆå“ˆå¤§ç¬‘ã€‚æˆ‘ä¸€è¾¹ç¬‘ï¼Œä¸€è¾¹å¤§å£°è¯´ï¼šâ€œæˆ‘è¿˜æ´»ç€ï¼â€åæ¥é£åœäº†ï¼Œæˆ‘è¿˜åœ¨å¤§å£°ç¬‘ç€ã€‚å‘¨å›´æ²¡ä»€ä¹ˆäººï¼Œè§è¯è€…åªæœ‰æ²³æ°´ã€æŸ³æ ‘å’Œç¤¼å ‚ã€‚</p><p>å›å®¿èˆæ—¶ï¼Œæˆ‘èµ°çš„æ˜¯å­¦å ‚è·¯ã€‚å­¦å ‚è·¯äººæµä¸å°ï¼Œèµ°åœ¨é“è·¯çš„è¾¹ç¼˜ï¼Œæ—¶æ—¶æœ‰è‡ªè¡Œè½¦ä»èº«æ—é©°è¿‡ã€‚æˆ‘é¥¶æœ‰å…´è‡´åœ°çœ‹ç€å„å¼å„æ ·çš„è‡ªè¡Œè½¦è¿œå»çš„èº«å½±ï¼Œå¬ç€å¶å°”ä¼ æ¥çš„ä¸€ä¸¤å¥ç©ç¬‘å’Œè°ˆè¯å£°ï¼Œå‘æ•£çš„æ€ç»ªé‡åˆå›åˆ°äº†æ—¥å¸¸ç”Ÿæ´»ï¼Œåªæ˜¯å¤šä¸Šäº†å‡ åˆ†é‡Šç„¶ã€å¦è¡å’Œå®é™ã€‚æ¼«æ­¥çš„è¶£å‘³ï¼Œå½“çœŸè¨€ä¹‹ä¸å°½ã€‚</p><blockquote><p>æ³¨ï¼šèƒŒæ™¯å›¾ç‰‡æ¥è‡ªhttps://mapio.net/pic/p-3220490/</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> æ‚ä¸ƒæ‚å…«çš„ä¸œè¥¿ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> éšç¬” </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>å¯ä»¥åˆ¤æ–­ç´ æ•°çš„æ­£åˆ™è¡¨è¾¾å¼</title>
      <link href="/2024/02/01/%E5%8F%AF%E4%BB%A5%E5%8C%B9%E9%85%8D%E5%90%88%E6%95%B0%E7%9A%84%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
      <url>/2024/02/01/%E5%8F%AF%E4%BB%A5%E5%8C%B9%E9%85%8D%E5%90%88%E6%95%B0%E7%9A%84%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>æœ€è¿‘è¯»åˆ°ä¸€ä¸ªå¯ä»¥åˆ¤æ–­ç´ æ•°çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ˆåŒ¹é…æˆåŠŸåˆ™ä¸æ˜¯ç´ æ•°ï¼‰ï¼š<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;^1?$|^(11+?)\1+$&#x2F;</span><br></pre></td></tr></table></figure>è¿™ä¹ˆç²¾ç‚¼ï¼Œé¢‡æœ‰äº›å‡ºä¹æˆ‘çš„æ„æ–™ã€‚å› ä¸ºåœ¨æˆ‘çš„å°è±¡ä¸­å¤§å¤šæ•°æ­£åˆ™è¡¨è¾¾å¼éƒ½ååˆ†ä¸‘é™‹ï¼Œæ¯”å¦‚åŒ¹é…é‚®ç®±çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(?:[a-z0-9!#$%&amp;&#39;*+&#x2F;&#x3D;?^_&#96;&#123;|&#125;~-]+(?:\.[a-z0-9!#$%&amp;&#39;*+&#x2F;&#x3D;?^_&#96;&#123;|&#125;~-]+)*|&quot;(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*&quot;)@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.)&#123;3&#125;(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])</span><br></pre></td></tr></table></figure></p><p>ç„¶è€Œï¼Œä»”ç»†é˜…è¯»äº†ä¸€ä¸‹åŸç†åå´å‘ç°å®ƒçš„ç¡®å¯ä»¥ï¼Œä¸è¿‡éœ€è¦ä¸€æ­¥é¢„å¤„ç†ï¼Œå³æŠŠåè¿›åˆ¶æ•°å­—è½¬æ¢ä¸ºâ€œ1çš„æ•°ç»„â€ï¼š0ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œ1ä¸º1ï¼Œ2ä¸º11ï¼Œ3ä¸º111â€¦â€¦è‡³æ­¤ï¼Œå°±å¯ä»¥å¼€å§‹æˆ‘ä»¬çš„åŒ¹é…ï¼š -<code>/^1?$/</code>å¾ˆå¥½ç†è§£ï¼ŒåŒ¹é…åˆ°æ˜¯ç©ºå­—ç¬¦ä¸²ï¼ˆ0ï¼‰æˆ– 1ï¼Œè‡ªç„¶ä¸æ˜¯åˆæ•°ï¼› -<code>/^(11+?)\1+$/</code>åˆ™æ­£æ˜¯è¯¥è¡¨è¾¾å¼çš„ç²¾å¦™æ‰€åœ¨ã€‚å…¶åˆ©ç”¨åˆ°äº†æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ—¶<strong>å›æº¯</strong>çš„ç‰¹æ€§ã€‚è®©æˆ‘ä»¬å…·ä½“è§£é‡Šä¸€ä¸‹ï¼š1.<code>(11+?)</code>åŒ¹é…çš„æ˜¯<strong>è‡³å°‘ä¸¤ä¸ª1</strong>ï¼Œä½†æ˜¯å› ä¸ºè¿›è¡Œçš„æ˜¯æ‡’æƒ°åŒ¹é…ï¼Œæ‰€ä»¥æœ€å¼€å§‹åŒ¹é…åˆ°çš„æ˜¯<code>11</code>ï¼Œå¹¶è¢«æ•è·åˆ°äº†åˆ†ç»„<code>\1</code>ä¸­ã€‚åé¢éƒ¨åˆ†ä¸­ï¼Œå¦‚æœ<code>11</code>é‡å¤äº†ä¸€æ¬¡æˆ–è€…æ›´å¤šæ¬¡ï¼Œé‚£ä¹ˆå°±æ˜¯åˆæ•°ã€‚è¿™åˆæ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå…¶å®éå¸¸å®¹æ˜“ç†è§£ï¼šå¦‚æœæ•°å­—<spanclass="math inline">\(a\)</span>åŒ¹é…æˆåŠŸï¼Œæ„å‘³ç€<spanclass="math inline">\(a\)</span>åŒ–ä½œçš„â€œ1çš„æ•°ç»„â€<strong>æ°å¥½</strong>ç”±<strong>ä¸å°‘äº</strong>2ä¸ª<code>11</code>ç»„æˆï¼Œä¹Ÿå°±æ˜¯ï¼Œ<spanclass="math inline">\(\exists b&gt;1\land b \in \mathbb N,a=2b\)</span>ï¼Œè¿™è‡ªç„¶æ„å‘³ç€<spanclass="math inline">\(a\)</span>æ˜¯ä¸€ä¸ªåˆæ•°(åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ<spanclass="math inline">\(a\)</span>è¿˜æ˜¯å¶æ•°)ã€‚ 2.å¦‚æœ<code>11</code>åŒ¹é…å¤±è´¥äº†å‘¢ï¼Ÿè¿™å°±æ˜¯æœ¬æ³•æœ€æ ¸å¿ƒçš„éƒ¨åˆ†äº†ï¼šåŒ¹é…å™¨ä¼šè¿›è¡Œå›æº¯ï¼Œå¯¹ä¸‹ä¸€ä¸ªæ»¡è¶³<code>(11+?)</code>çš„<code>111</code>è¿›è¡Œ<code>\1+</code>åŒ¹é…çš„å°è¯•ã€‚åŒç†ä¸Šä¸€æ­¥ï¼Œå¦‚æœåŒ¹é…æˆåŠŸï¼Œè¯¥æ•°å­—å¤§äº3ä¸”å«æœ‰ä¸€ä¸ªå› æ•°3ï¼Œè‡ªç„¶æ˜¯åˆæ•°ã€‚3. æ¥ä¸‹æ¥ï¼Œâ€œåŒ¹é… - å›æº¯â€ä¸æ–­è¿›è¡Œã€‚å¦‚æœ<spanclass="math inline">\(n\)</span>æ˜¯åˆæ•°ï¼Œä¼šåœ¨åŒ¹é…ä¸è¶…è¿‡<spanclass="math inline">\(\sqrt{n}\)</span>æ¬¡åæˆåŠŸï¼›åä¹‹ï¼Œä¼šä¸€ç›´åŒ¹é…åˆ°ç¬¬<spanclass="math inline">\(n\)</span>æ¬¡ï¼Œæœ€ååŒ¹é…å¤±è´¥ã€‚</p><p>å¯è§ï¼Œ<code>/^1?$|^(11+?)\1+$/</code>çš„å®ç°æƒ³æ³•æ˜¯éå¸¸æœ´ç´ çš„ï¼šåˆ—ä¸¾æ¯”<spanclass="math inline">\(n\)</span>å°çš„æ‰€æœ‰æ­£æ•´æ•°<spanclass="math inline">\(i\)</span>ï¼Œåˆ¤æ–­<spanclass="math inline">\(n\)</span>æ˜¯å¦å¯ä»¥è¢«<spanclass="math inline">\(i\)</span>æ•´é™¤ã€‚æ¯”å¦‚ï¼š <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//åˆ¤æ–­ä¸€ä¸ªè‡ªç„¶æ•°æ˜¯å¦ä¸ºç´ æ•°</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isPrime</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">Â  Â  <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>)<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">Â  Â  <span class="keyword">if</span> (n == <span class="number">2</span>)<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">Â  Â  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= <span class="built_in">sqrt</span>(n); i++)<span class="keyword">if</span> (n % i == <span class="number">0</span>)<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">Â  Â  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>ä½†æ˜¯ï¼Œå¾—åˆ°è¿™ä¹ˆç²¾ç‚¼çš„è¡¨è¾¾å¼çš„å…³é”®å®åˆ™åœ¨äºå¯¹äºæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æœºåˆ¶çš„æ·±åˆ»ç†è§£(æ‡’æƒ°åŒ¹é…å’Œå›æº¯æ³•çš„ç»“åˆ)ï¼Œè€Œè¿™æ­£æ˜¯å€¼å¾—æˆ‘ä»¬æ·±æ€å’Œå­¦ä¹ çš„åœ°æ–¹ã€‚</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ</title>
      <link href="/2024/01/18/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
      <url>/2024/01/18/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ">ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ</h1><p><del>æœ¬æ–‡æ˜¯æ•°æ®ç§‘å­¦å¯¼è®ºæœŸæœ«å¤§ä½œä¸šå±•ç¤ºçš„æŠ¥å‘Šï¼Œæ‹¿æ¥éšä¾¿æ°´ä¸€ç¯‡åšå®¢hh</del></p><h2 id="å¼•è¨€">0 å¼•è¨€</h2><p>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (Generative Adversarial Network, GAN)æ˜¯ä¸€ç§ååˆ†æµè¡Œçš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚è‡ª2014å¹´IanGoodfellowç­‰äººé¦–æ¬¡æå‡ºä»¥æ¥ï¼ŒGANè¿…é€Ÿåœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œå¼•å‘äº†çƒ­çƒˆçš„åå“ï¼Œè®¸å¤šæœ‰å½±å“åŠ›çš„å·¥ä½œå±‚å‡ºä¸ç©·ã€‚å›¾ä¸€æ˜¯ç´¯ç§¯çš„GANè®ºæ–‡æ•°é‡ï¼Œå…¶ç«çƒ­ç¨‹åº¦å¯è§ä¸€æ–‘ã€‚</p><p><imgsrc="https://github.com/daucloud/imagecdn/raw/main/test/202401181914378.png" /></p><center>Figure 1: Cumulative number of GAN papers</center><p>æœ¬æ–‡å°†å¯¹GANåšç®€è¦ä»‹ç»ï¼Œä¸»è¦åŒ…æ‹¬GANåŸå§‹æ¨¡å‹ã€ç»å…¸å˜ä½“å’Œåº”ç”¨æˆå°±ã€‚</p><h2 id="åŸå§‹æ¨¡å‹">1 åŸå§‹æ¨¡å‹</h2><h3 id="åŸºæœ¬æ€æƒ³">1.1 åŸºæœ¬æ€æƒ³</h3><p>GANå…¨ç§°ä¸ºç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚é¡¾åæ€ä¹‰ï¼Œå…¶åœ¨æœ¬è´¨ä¸Šæ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒæœ€çªå‡ºçš„ç‰¹ç‚¹æ˜¯ä½¿ç”¨åˆ¤åˆ«å™¨ä¸ç”Ÿæˆå™¨è¿›è¡Œå¯¹æŠ—å¼è®­ç»ƒï¼Œåè€…è´Ÿè´£äº§ç”Ÿè´´è¿‘çœŸå®çš„æ•°æ®ï¼Œå‰è€…åˆ™è´Ÿè´£åŠªåŠ›è¾¨åˆ«ç”Ÿæˆæ•°æ®å’ŒçœŸå®æ•°æ®ã€‚å½“è®­ç»ƒè¿­ä»£è‡³ä¸€å®šæ¬¡æ•°åï¼Œç”Ÿæˆå™¨äº§ç”Ÿçš„æ•°æ®ä¾¿è¶³å¤Ÿé€¼çœŸï¼Œè®­ç»ƒç›®çš„å› è€Œå¾—åˆ°å®ç°ã€‚</p><p>å¯¹æ­¤ï¼ŒGANçš„åŸå§‹è®ºæ–‡[^1]ä¸­ç»™å‡ºäº†ä¸€ä¸ªé€šä¿—çš„è§£é‡Šï¼š</p><blockquote><p>ç”Ÿæˆå™¨æ˜¯â€œå‡é’åˆ¶é€ å›¢ä¼™â€ï¼Œè´Ÿè´£åˆ¶é€ èƒ½åœ¨å¸‚åœºä¸Šæµé€šçš„å‡é’ï¼›åˆ¤åˆ«å™¨åˆ™æ˜¯â€œè­¦å¯Ÿâ€ï¼Œè´Ÿè´£è¯†ç ´å‡é’ã€‚å‡é’åˆ¶é€ å›¢ä¼™å’Œè­¦å¯Ÿä¹‹é—´ä¸æ–­ç«äº‰ï¼Œè­¦å¯Ÿçš„é‰´åˆ«èƒ½åŠ›å’Œå›¢ä¼™çš„é€ å‡èƒ½åŠ›éƒ½ä¸æ–­æå‡ï¼Œæœ€ç»ˆï¼Œé€ å‡å›¢ä¼™åˆ¶é€ çš„å‡é’è¶³ä»¥ä»¥å‡ä¹±çœŸï¼šâ€œå‡é’å˜æˆäº†çœŸé’â€ã€‚</p></blockquote><p><strong>GANå°±æ˜¯åœ¨åˆ¶é€ è¶³ä»¥ä»¥å‡ä¹±çœŸçš„å‡é’ã€‚</strong></p><h3 id="åŸºæœ¬æ­¥éª¤">1.2 åŸºæœ¬æ­¥éª¤</h3><p>æ¦‚æ‹¬æ¥è¯´ï¼ŒGANçš„è®­ç»ƒä¸»è¦åˆ†ä¸ºä»¥ä¸‹ä¸‰æ­¥ï¼š</p><ol type="1"><li><strong>å›ºå®šç”Ÿæˆå™¨<spanclass="math inline">\(G\)</span>ï¼Œè®­ç»ƒåˆ¤åˆ«å™¨<spanclass="math inline">\(D\)</span>ï¼š</strong> ä½¿ <spanclass="math inline">\(D\)</span>å°½å¯èƒ½å‡†ç¡®åœ°è¯†åˆ«å‡ºæ ·æœ¬ç©¶ç«Ÿæ˜¯ç”±ç”Ÿæˆå™¨ç”Ÿæˆçš„ï¼ˆæ¥è‡ª <spanclass="math display">\[p_z(z)\]</span>ï¼‰è¿˜æ˜¯æ¥è‡ªçœŸå®æ•°æ®é›†ï¼ˆæ¥è‡ª <spanclass="math display">\[p_{data}(x)\]</span>ï¼‰</li><li><strong>å›ºå®šåˆ¤åˆ«å™¨<spanclass="math inline">\(D\)</span>ï¼Œè®­ç»ƒç”Ÿæˆå™¨<spanclass="math inline">\(G\)</span>ï¼š</strong> ä½¿ <spanclass="math inline">\(G\)</span> ç”Ÿæˆçš„æ ·æœ¬(<spanclass="math inline">\(G(z)\)</span>)å°½å¯èƒ½è´´è¿‘çœŸå®æ ·æœ¬ï¼Œå³è®©ç”Ÿæˆå™¨<spanclass="math inline">\(G\)</span>éª—è¿‡åˆ¤åˆ«å™¨<spanclass="math inline">\(D\)</span></li><li><strong>è¿­ä»£ï¼š</strong> å¾ªç¯1. 2.è‡³ä¸€å®šæ¬¡æ•°ï¼Œæ­¤æ—¶ç”Ÿæˆå™¨<spanclass="math inline">\(G\)</span>äº§ç”Ÿçš„æ ·æœ¬è¶³ä»¥â€œä»¥å‡ä¹±çœŸâ€ï¼Œåˆ¤åˆ«å™¨<spanclass="math inline">\(D\)</span>è¾¨è®¤çœŸå®æ ·æœ¬å’Œç”Ÿæˆæ ·æœ¬æˆåŠŸçš„æ¦‚ç‡éƒ½ä¸º<spanclass="math inline">\(\frac1 2\)</span>ï¼ŒäºŒè€…è¾¾åˆ°äº†çº³ä»€å¹³è¡¡[^2]ã€‚</li></ol><blockquote><p>è®­ç»ƒç¤ºæ„å›¾ï¼š<imgsrc="https://github.com/daucloud/imagecdn/raw/main/test/202401181914603.png" /></p><center>Figure 2: è®­ç»ƒè¿‡ç¨‹ç¤ºæ„å›¾</center><p>å›¾ä¸­ï¼Œ<spanclass="math inline">\(z\)</span>è½´æ˜¯è¾“å…¥ç”Ÿæˆå™¨çš„éšæœºå™ªå£°ï¼Œå…¶é€šè¿‡ç”Ÿæˆå™¨(ç®­å¤´)æ˜ å°„åˆ°æ ·æœ¬é›†<spanclass="math inline">\(x\)</span>è½´ã€‚é»‘è‰²æ•£ç‚¹æ˜¯çœŸå®æ•°æ®é›†ï¼›ç»¿è‰²å®çº¿æ˜¯ç”Ÿæˆæ ·æœ¬é›†ï¼Œè“è‰²è™šçº¿æ˜¯åˆ¤åˆ«å™¨ï¼ˆè¶Šè¿œç¦»<spanclass="math inline">\(x\)</span>è½´ï¼Œå…¶å€¼è¶Šæ¥è¿‘äº1ï¼Œå³å…¶è®¤ä¸ºè¯¥æ ·æœ¬æ¥è‡ªçœŸå®æ•°æ®é›†çš„æ¦‚ç‡è¶Šå¤§ï¼‰ã€‚</p><p>(a)ä¸­ï¼Œåˆ¤åˆ«å™¨æ›²çº¿(è“è‰²è™šçº¿)è¾ƒä¸ºé¢ ç°¸ï¼Œåˆ¤åˆ«æ•ˆæœè¾ƒå·®ï¼Œå› æ­¤(a)åˆ°(b)é¦–å…ˆå¯¹åˆ¤åˆ«å™¨è¿›è¡Œè®­ç»ƒï¼›(b)åˆ°(c)åˆ™å¯¹ç”Ÿæˆå™¨è¿›è¡Œè®­ç»ƒï¼Œä½¿å¾—ç”Ÿæˆæ ·æœ¬æ›²çº¿(ç»¿è‰²å®çº¿)æ›´è´´è¿‘çœŸå®æ ·æœ¬é›†(é»‘è‰²æ•£ç‚¹)ã€‚å½“è¿­ä»£æ­¤äºŒæ­¥åˆ°ä¸€å®šæ¬¡æ•°åï¼Œç»¿è‰²å®çº¿å’Œé»‘è‰²æ•£ç‚¹æ¥è¿‘é‡åˆï¼Œåˆ¤åˆ«å™¨æ›²çº¿ä¹Ÿæ’ä¸º<spanclass="math inline">\(1 \over2\)</span>ï¼Œå³æ— æ³•å°†ç”Ÿæˆæ ·æœ¬å’ŒçœŸå®æ•°æ®è¿›è¡ŒåŒºåˆ†ã€‚</p></blockquote><h3 id="å…·ä½“ç®—æ³•">1.3 å…·ä½“ç®—æ³•</h3><p><strong>è®°å·è¯´æ˜</strong></p><ul><li>ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨å‡ä¸ºå¤šå±‚ç¥ç»ç½‘ç»œï¼š<spanclass="math inline">\(G(z;\theta _g)\)</span>å’Œ<spanclass="math inline">\(D(x;\theta _d)\)</span></li><li><span class="math inline">\(G\)</span>é€šè¿‡å‚æ•°<spanclass="math inline">\(\theta_g\)</span>å°†å™ªå£°<spanclass="math inline">\(z\)</span>æ˜ å°„ä¸ºç”Ÿæˆæ ·æœ¬<spanclass="math inline">\(G(z;\theta _g)\)</span></li><li><span class="math inline">\(D\)</span>é€šè¿‡å‚æ•°<spanclass="math inline">\(\theta_d\)</span>å°†æ ·æœ¬æ˜ å°„ä¸º<spanclass="math inline">\([0,1]\)</span>çš„æ ‡é‡<spanclass="math inline">\(D(x;\theta_d)\)</span>ï¼Œå…¶å€¼è¶Šæ¥è¿‘1è¡¨ç¤ºä¸ºçœŸå®æ•°æ®çš„æ¦‚ç‡è¶Šå¤§</li><li>ç›®æ ‡ï¼Œæ±‚è§£å€¼å‡½æ•°<spanclass="math inline">\(V(D,G)\)</span>çš„æå°æå¤§å€¼ï¼Œå³ï¼š</li></ul><p><span class="math display">\[\min_G\max_D V(D,G)=\mathbb{E}_{x\simp_{data}(x)}[\log D(x)]+\mathbb{E}_{z\simp_z(z)}[\log(1-D(G(z)))]\]</span></p><p><strong>å…·ä½“æ±‚è§£æ­¥éª¤</strong></p><p>for 1 to n do<br />â€ƒâ€ƒfor 1 to k do<br />â€ƒâ€ƒâ€ƒâ€ƒä»å™ªå£°é›†å–å‡ºmä¸ªæ ·æœ¬<spanclass="math inline">\(\{z^{(1)},z^{(2)},...,z^{(m)}\}\)</span>;<br />â€ƒâ€ƒâ€ƒâ€ƒä»çœŸå®æ•°æ®é›†ä¸­å–å‡ºmä¸ªæ ·æœ¬<spanclass="math inline">\(\{x^{(1)},x^{(2)},...,x^{(m)}\}\)</span>;<br />â€ƒâ€ƒâ€ƒâ€ƒä½¿ç”¨æ¢¯åº¦ä¸Šå‡æ³•[^3]æ›´æ–°åˆ¤åˆ«å™¨çš„å‚æ•°<spanclass="math inline">\(\theta_d\)</span>ï¼š <spanclass="math display">\[\nabla_{\theta_d}\frac{1}{m}\sum_{i=1}^{m}[\logD(x^{(i)})+\log D(1-D(G(z^{(i)})))].\]</span> â€ƒâ€ƒend for<br />â€ƒâ€ƒä»å™ªå£°é›†ä¸­å–å‡ºmä¸ªæ ·æœ¬<spanclass="math inline">\(\{z^{(1)},z^{(2)},...,z^{(m)}\}\)</span>;<br />â€ƒâ€ƒä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°ç”Ÿæˆå™¨çš„å‚æ•°<spanclass="math inline">\(\theta_g\)</span> <spanclass="math display">\[\nabla_{\theta_d}\frac{1}{m}\sum_{i=1}^{m}[\log(1-D(G(z^{(i)})))].\]</span></p><p>end for</p><blockquote><p>kæ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼›ä¹‹æ‰€ä»¥æ›´æ–°kæ¬¡<spanclass="math inline">\(D\)</span>æ‰æ›´æ–°1æ¬¡<spanclass="math inline">\(G\)</span>ï¼Œæ˜¯å› ä¸ºåªæœ‰åˆ¤åˆ«å™¨è¶³å¤Ÿå¥½ï¼Œç”Ÿæˆå™¨çš„æ›´æ–°æ‰å‡†ç¡®æœ‰æ•ˆã€‚</p></blockquote><h2 id="ç»å…¸å˜ä½“">2 ç»å…¸å˜ä½“</h2><p>GANä½œä¸ºé£é¡ä¸€æ—¶çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œè‡ªç„¶å°‘ä¸äº†å„ç§æ”¹è¿›å’Œå˜ä½“ã€‚æœ¬éƒ¨åˆ†é€‰å–äº†ä¸‰ç§è¾ƒæœ‰ä»£è¡¨æ€§å’Œå½±å“åŠ›çš„å˜ä½“è¿›è¡Œä»‹ç»ï¼šCGANã€DCGANå’ŒWGANã€‚</p><h3 id="cgan4">2.1 CGAN[^4]</h3><p>åŸå§‹GANæ¨¡å‹ä¸­ï¼Œè¾“å…¥ç”Ÿæˆå™¨çš„æ˜¯éšæœºå™ªå£°ï¼Œå› æ­¤æœ€åäº§ç”Ÿçš„å›¾åƒçš„éšæœºæ€§ä¹Ÿç›¸å¯¹è¾ƒå¤§ã€‚å€˜è‹¥æˆ‘ä»¬æƒ³è¦è·å¾—å…·æœ‰æŸç§ç‰¹å¾å’Œæ ‡ç­¾çš„å›¾åƒï¼Œå°±éœ€è¦ä»ç”Ÿæˆçš„ä¸€å¤§å †æ ·æœ¬ä¸­è¿›è¡ŒæŒ‘é€‰ï¼Œè¾ƒä¸ºç¹çã€‚</p><p>CGAN(Conditional Generative AdversarialNetworks)ä¾¿æ˜¯ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜è€Œäº§ç”Ÿçš„ã€‚å…¶æ€æƒ³ä¹Ÿååˆ†æœ´ç´ ï¼Œå³å‘ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„è¾“å…¥ä¸­æ·»åŠ æ¡ä»¶ä¿¡æ¯ã€‚è¿™æ ·ä¸€æ¥ï¼Œç”Ÿæˆæ ·æœ¬ä¸ä»…éœ€è¦è¶³å¤Ÿé€¼çœŸï¼Œè¿˜è¦æ»¡è¶³ç‰¹å®šçš„æ¡ä»¶æ‰èƒ½é€šè¿‡åˆ¤åˆ«å™¨ï¼š</p><p><span class="math display">\[\min_G\max_D V(D,G)=\mathbb{E}_{x\simp_{data}(x)}[\log D(x|y)]+\mathbb{E}_{z\simp_z(z)}[\log(1-D(G(z|y)))]\]</span></p><figure><img src="https://aovoc.github.io/assets/pics/cgan-arch2.PNG"alt="Cgan,icgan" /><figcaption aria-hidden="true">Cgan,icgan</figcaption></figure><center>Figure 3: CGANå’ŒGANçš„å¯¹æ¯”</center><h3 id="dcgan5">2.2 DCGAN[^5]</h3><p>DCGAN(Deep Convolutional Generative AdversarialNetworks)å…¨ç§°ä¸ºæ·±åº¦å·ç§¯å¯¹æŠ—ç”Ÿæˆç½‘ç»œã€‚é¡¾åæ€ä¹‰ï¼Œå…¶ä¸»è¦æƒ³æ³•æ˜¯å°†å·ç§¯ç¥ç»ç½‘ç»œ(CNN)å’ŒGANè¿›è¡Œç»“åˆï¼Œåœ¨ä¸æ”¹å˜GANçš„åŸºæœ¬åŸç†çš„æƒ…å†µä¸‹è¾ƒä¸ºæœ‰æ•ˆåœ°æ”¹å–„äº†å…¶è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚</p><p>DCGANåšå‡ºçš„ä¸»è¦æ”¹å˜æœ‰ï¼š</p><ul><li><p>ä½¿ç”¨å·ç§¯å±‚å’Œè½¬ç½®å·ç§¯å±‚ï¼šå¼•å…¥äº†è½¬ç½®å·ç§¯å±‚å’Œå·ç§¯å±‚åˆ†åˆ«ä½œä¸ºç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ç½‘ç»œçš„ä¸»è¦ç»„ä»¶ã€‚</p></li><li><p>å»é™¤å…¨è¿æ¥å±‚ï¼šç”¨å…¨å·ç§¯å±‚ä»£æ›¿ã€‚</p></li><li><p>æ‰¹å½’ä¸€åŒ–(BatchNormalization)ï¼šç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨éƒ½ä½¿ç”¨BNå±‚ã€‚</p></li><li><p>ä¿®æ”¹æ¿€æ´»å‡½æ•°ï¼šç”Ÿæˆå™¨è¾“å‡ºå±‚ä½¿ç”¨Tanhï¼Œå…¶ä½™å±‚ä½¿ç”¨ReLUï¼›åˆ¤åˆ«å™¨å‡ä½¿ç”¨LeakyReLU</p></li></ul><p><imgsrc="https://github.com/daucloud/imagecdn/raw/main/test/202401181914603.png" /></p><center>Figure 4: ç”Ÿæˆå™¨è½¬ç½®å·ç§¯å±‚ç¤ºæ„å›¾</center><h3 id="wgan6">2.3 WGAN[^6]</h3><p>WGAN(Wasserstein Generative AdversarialNetworks)å¼•å…¥äº†Wassersteinè·ç¦»ä»£æ›¿åŸæ¥çš„JSæ•£åº¦ä½œä¸ºGANçš„æŸå¤±å‡½æ•°ï¼Œå½»åº•è§£å†³äº†GANè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œæ˜¯GANå‘å±•å²ä¸Šé‡Œç¨‹ç¢‘å¼çš„å·¥ä½œä¹‹ä¸€ã€‚</p><p><img src="https://pic1.zhimg.com/80/v2-6be6e2ef3d15c4b10c2a943e9bf4db70_1440w.jpg#width=60%" alt="img" style="zoom:67%;" /></p><center>Figure 5: WGANç®—æ³•</center><p>å¯è§ï¼ŒWGANåšå‡ºçš„æœ€ä¸»è¦æ”¹åŠ¨ï¼Œå³æ˜¯å¯¹æŸå¤±å‡½æ•°çš„æ›´æ¢ã€‚æ­¤å¤–ï¼Œå…¶è¿˜åšå‡ºäº†å¦‚ä¸‹æ”¹å˜ï¼š</p><ul><li>åˆ¤åˆ«å™¨æœ€åä¸€å±‚å»æ‰sigmoid</li><li>æ¯æ¬¡æ›´æ–°åˆ¤åˆ«å™¨çš„å‚æ•°ä¹‹åæŠŠå®ƒä»¬çš„ç»å¯¹å€¼æˆªæ–­åˆ°ä¸è¶…è¿‡ä¸€ä¸ªå›ºå®šå¸¸æ•°c</li><li>ä¸è¦ç”¨åŸºäºåŠ¨é‡çš„ä¼˜åŒ–ç®—æ³•ï¼ˆåŒ…æ‹¬momentumå’ŒAdamï¼‰ï¼Œæ¨èRMSPropï¼ŒSGDä¹Ÿè¡Œ</li></ul><p>æ”¹è¿›è™½ç„¶ç®€å•ï¼Œä½†æ˜¯æˆæ•ˆå·¨å¤§ã€‚</p><h2 id="åº”ç”¨ä¸¾ä¾‹">3 åº”ç”¨ä¸¾ä¾‹</h2><h3 id="edmond-de-belamy">3.1 ã€ŒEdmond de Belamyã€</h3><p>åœ¨2018å¹´æœ«çš„ä½³å£«å¾—çº½çº¦æ‹å–åœºä¸Šï¼Œæ¥è‡ªå·´é»çš„è‰ºæœ¯å›¢é˜Ÿ<em>Obvious</em>ä½¿ç”¨GANç”Ÿæˆçš„ç”»ä½œã€ŒEdmonddeBelamyã€ä»¥è¶…å‡ºä¼°ä»·40å€çš„43.25ä¸‡ç¾å…ƒæˆäº¤ï¼Œå…¶å³ä¸‹è§’ä¾¿å°æœ‰çºµæ¨ªGANé¢†åŸŸçš„è‘—åå…¬å¼ï¼š<span class="math display">\[\min_G\max_D \mathbb{E}_{x}[\logD(x)]+\mathbb{E}_{z}[\log(1-D(G(z)))]\]</span></p><p>1968å¹´ï¼Œæ¯•åŠ ç´¢æ›¾è¯´ï¼šâ€è®¡ç®—æœºæ˜¯æ²¡æœ‰ç”¨çš„ã€‚å®ƒä»¬åªä¼šå‘Šè¯‰ä½ ç­”æ¡ˆâ€ã€‚ä½†åœ¨åŒä¸€åœºæ‹å–ä¼šä¸Šï¼Œæ²¡æœ‰ä¸€å¹…æ¯•åŠ ç´¢çš„ç”»ä½œæˆäº¤ä»·æ ¼è¶…è¿‡äº†ã€ŒEdmondde Belamyã€ï¼Œè¿™ä¸ç¦ä»¤äººå”å˜˜ä¸å·²ã€‚</p><p><imgsrc="https://github.com/daucloud/imagecdn/raw/main/test/202401181922436.png" /></p><center>Figure 6: ã€ŒEdmond de Belamyã€</center><h3 id="this-person-does-not-exist">3.2 This Person Does Not Exist</h3><p><ahref="https://thispersondoesnotexist.com/">thispersondoesnotexist.com</a>æ¯æ¬¡è¿›å…¥è¯¥ç½‘å€ï¼Œéƒ½ä¼šç”Ÿæˆä¸€å¼ ä¸–ç•Œä¸Šå¹¶ä¸å­˜åœ¨çš„äººè„¸ï¼Œè€Œè¿™æ­£æ˜¯ä½¿ç”¨GANè¿›è¡Œç”Ÿæˆçš„ã€‚</p><p><imgsrc="https://github.com/daucloud/imagecdn/raw/main/test/202401181917230.png" /></p><center>Figure 7: This Person Does Not Exist</center><h3 id="äºŒæ¬¡å…ƒå¤´åƒç”Ÿæˆ7">3.3 äºŒæ¬¡å…ƒå¤´åƒç”Ÿæˆ[^7]</h3><p>ä½¿ç”¨GANï¼Œä½ å¯ä»¥éšå¿ƒæ‰€æ¬²ç”ŸæˆäºŒæ¬¡å…ƒ<del>è€å©†</del>å¤´åƒï¼š</p><p><imgsrc="https://github.com/daucloud/imagecdn/raw/main/test/202401181917189.png" /></p><center>Figure 8: GANç”Ÿæˆçš„äºŒæ¬¡å…ƒå¤´åƒ</center><h2 id="æ€»ç»“">4 æ€»ç»“</h2><p>GANæ˜¯ä¸€ç§åº”ç”¨å¹¿æ³›ã€æ½œåŠ›å·¨å¤§çš„ç”Ÿæˆæ¨¡å‹ã€‚å®ƒä½¿ç”¨åˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨è¿›è¡Œå¯¹æŠ—æ€§è®­ç»ƒï¼Œå¹¶æœ€ç»ˆäº§ç”Ÿè¶³å¤Ÿé€¼çœŸçš„å›¾åƒã€‚ä½†GANåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šå¸¸å­˜åœ¨ç”Ÿæˆæ ·æœ¬è¿‡äºéšæœºã€è®­ç»ƒä¸ç¨³å®šç­‰ç¼ºç‚¹ï¼Œå› æ­¤æ¶Œç°å‡ºäº†è¯¸å¤šå˜ä½“å¯¹å…¶è¿›è¡Œæ”¹è¿›ï¼šCGANã€DCGANã€WGANâ€¦â€¦</p><p>æˆ‘ä»¬æœŸå¾…åœ¨å°†æ¥èƒ½çœ‹åˆ°æ›´å…·æ½œåŠ›çš„GANæ¨¡å‹å’Œæ›´å¯Œä»·å€¼çš„GANåº”ç”¨ï¼</p><hr /><p>Footnotes:</p><p>[1]:Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B.,Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014, June10). <em>Generative Adversarial Networks</em>. arXiv.org.https://arxiv.org/abs/1406.2661 [2]:äº‹å®ä¸Šï¼Œè¿™ç§ â€œçº³ä»€å¹³è¡¡â€æ˜¯ä¸€ç§ç†æƒ³çŠ¶æ€ï¼Œå®é™…è®­ç»ƒéš¾ä»¥è¾¾åˆ°ã€‚ [3]:å…³äºæ¢¯åº¦ä¸‹é™/ä¸Šå‡æ³•ï¼Œå¯å‚è€ƒ<ahref="https://www.zhihu.com/question/305638940/answer/1639782992">ä»€ä¹ˆæ˜¯æ¢¯åº¦ä¸‹é™æ³•ï¼Ÿ- é©¬åŒå­¦çš„å›ç­” - çŸ¥ä¹</a> [4]:Mirza, M., &amp; Osindero, S. (2014,November 6). <em>Conditional generative adversarial nets</em>.arXiv.org. https://arxiv.org/abs/1411.1784 [5]:Radford, A., Metz, L.,&amp; Chintala, S. (2016, January 7). <em>Unsupervised representationlearning with deep convolutional generative Adversarial Networks</em>.arXiv.org. https://arxiv.org/abs/1511.06434 [6]:<ahref="https://zhuanlan.zhihu.com/p/25071913">ä»¤äººæ‹æ¡ˆå«ç»çš„WassersteinGAN - éƒ‘åæ»¨çš„æ–‡ç«  - çŸ¥ä¹</a> [7]:Jin, Y., Zhang, J., Li, M., Tian, Y.,Zhu, H., &amp; Fang, Z. (2017, August 18). <em>Towards the automaticanime characters creation with Generative Adversarial Networks</em>.arXiv.org. https://arxiv.org/abs/1708.05509</p>]]></content>
      
      
      <categories>
          
          <category> cs learning </category>
          
          <category> intro to data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>é›ª</title>
      <link href="/2023/12/11/%E9%9B%AA/"/>
      <url>/2023/12/11/%E9%9B%AA/</url>
      
        <content type="html"><![CDATA[<h1 id="é›ª">é›ª</h1><p>æœ¬æ¥æ˜¯ä¸å–œæ¬¢å†¬å¤©çš„ã€‚</p><p>åŸæ¥é¡¶å–œæ¬¢çš„å­£èŠ‚æ˜¯ç§‹å¤©ã€‚æ¼«æ­¥è¡—å¤´ï¼Œæ€»èƒ½ä¸ä¸€ç‰‡é£˜ç„¶å äºèº«å‰çš„è½å¶ä¸æœŸè€Œé‡ã€‚çˆ±æäº†è¿™ç§æ„æ–™ä¹‹å¤–çš„é‚‚é€…ã€‚</p><p>åæ¥å¬è¿‡lunaçš„<ahref="https://www.bilibili.com/video/BV1ka4y1E7ik/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bd539b5a62c295726bece82272cc6c5a">ã‚ã®å¤ã®ã„ã¤ã‹ã¯(åœ¨é‚£å€‹å¤æ—¥çš„æŸå¤©)</a>ï¼Œå¤å¤©åœ¨æˆ‘å¿ƒä¸­çš„åœ°ä½æ¸æ¸å˜å¾—ä¸ç§‹å¤©ä¸åˆ†ä¼¯ä»²äº†ã€‚æ¯æ¬¡çœ‹è¿™é¦–æ›²å­çš„pvï¼Œéƒ½æ·±ä¸ºå¤çš„çƒ­çƒˆæ¿€åŠ¨ä¸å·²ï¼šâ€œæ­£æ˜¯æ— æ•°ä¸ªå¾®å°ç‰‡åˆ»ï¼Œæ±‡é›†åœ¨ä¸€èµ·é€ å°±è¿™çƒ­çƒˆçš„æˆ‘â€ã€‚ç”Ÿå‘½æœ¬è¯¥å¦‚æ­¤ã€‚</p><p>ä½†æ˜¯ï¼Œæåˆ°â€œå†¬â€è¿™ä¸ªå­—ï¼Œæµ®ç°åœ¨è„‘æµ·ä¸­çš„æ€»æ˜¯å‡›å†½çš„é£ã€å‡‹é›¶çš„æ ‘ã€èœ·ç¼©çš„äººâ€¦â€¦è®¨åŒè¿™ç§ä¸‡äº‹ä¸‡ç‰©éƒ½è¢«å‹æŠ‘çš„æ„Ÿè§‰ï¼šç”Ÿå‘½ä¼¼ä¹è¢«ä¸¥é…·çš„å†¬å†°å°äº†èµ·æ¥ï¼Œå¾…åˆ°æ¥å¹´æ˜¥æš–èŠ±å¼€çš„æ—¶èŠ‚ï¼Œæ‰èƒ½é‡ç„•ç”Ÿæœºã€‚å†¬å¤©å°‘äº†ç‚¹çµé­‚ã€‚</p><p>åæ¥æ‰æ¸æ¸å‘ç°æˆ‘é”™äº†ã€‚å†¬ä¸æ˜¯æ²¡æœ‰çµé­‚ï¼›ç›¸åï¼Œå†¬çš„çµé­‚ç”šè‡³æ¯”å…¶ä»–ä¸‰ä¸ªå­£èŠ‚éƒ½æ›´åŠ å¯çˆ±â€”â€”è¿™æ­£æ˜¯é‚£åä¸ºâ€œé›ªâ€çš„ç²¾çµã€‚é›ªæ´ç™½ã€è½»ç›ˆã€å¤çµç²¾æ€ªï¼Œå†¬å¤©çš„æ²‰é—·å› ä¸ºå¥¹çš„åˆ°æ¥ä¸€æ‰«è€Œç©ºã€‚â€œå¿½å¦‚ä¸€å¤œæ˜¥é£æ¥ï¼Œåƒæ ‘ä¸‡æ ‘æ¢¨èŠ±å¼€ã€‚â€è¢«é›ªç‚¹ç¼€åçš„ä¸–ç•Œï¼Œç„•å‘å‡ºä¸äºšäºæ˜¥çš„å‹ƒå‹ƒç”Ÿæœºã€‚äººä»¬ä¸å†èœ·ç¼©åœ¨è¢«çªï¼Œæ ¡å›­é‡Œéšå¤„å¯è§é£èˆçš„é›ªçƒã€å¥‡å½¢æ€ªçŠ¶çš„é›ªäººå’Œåˆ›æ„ç™¾å‡ºçš„é›ªåœ°ä¸Šçš„å›¾æ¡ˆã€‚å†¬å¤©åŸæ¥æ˜¯è¿™æ ·ä¸€ä¸ªè¶£å‘³ç›ç„¶çš„å­£èŠ‚ã€‚</p><p>ç‹¬èº«éª‘è¡Œåœ¨æ¸…åå›­å†…ï¼Œèµ°èµ°åœåœï¼Œåœåœèµ°èµ°ï¼Œä¸€æ—¶å°†è¯¸å¤šddlæŠ›åœ¨è„‘åï¼Œåªè§‰å¿ƒçµä¹Ÿå› è¿™ç™½èŒ«èŒ«çš„é›ªçš„ä¸–ç•Œå˜å¾—ä¸€ç‰‡é€šé€ã€‚ç»ˆäºæ˜ç™½åŸæ¥æ²¡æœ‰ä¸€ä¸ªå­£èŠ‚æ˜¯ä¸å€¼å¾—å–œçˆ±çš„ï¼Œç”Ÿå‘½ä¸­ä¹Ÿæ²¡æœ‰ä¸€ä¸ªæ—¶åˆ»æ˜¯ä¸å€¼å¾—çƒ­çˆ±çš„ã€‚äººæ´»åœ¨ä¸–ï¼Œæœ¬è¯¥å¦‚æ­¤ã€‚</p><p>æœ¬è¯¥è¿™æ ·å–œæ¬¢å†¬å¤©å•Šã€‚</p><figure><img src="/img/2023-12-11-é›ª/snow1.jpg" alt="snow1" /><figcaption aria-hidden="true">snow1</figcaption></figure><figure><img src="/img/2023-12-11-é›ª/snow2.jpg" alt="snow2" /><figcaption aria-hidden="true">snow2</figcaption></figure><figure><img src="/img/2023-12-11-é›ª/snow.jpg" alt="snow" /><figcaption aria-hidden="true">snow</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> æ‚ä¸ƒæ‚å…«çš„ä¸œè¥¿ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> éšç¬” </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello,World!</title>
      <link href="/2023/12/09/Hello-World/"/>
      <url>/2023/12/09/Hello-World/</url>
      
        <content type="html"><![CDATA[<p>å»ºå¥½äº†åšå®¢ï¼Œç¬¬ä¸€ä»¶äº‹å½“ç„¶æ˜¯å‘ä¸€ç¯‡ Hello,world! (bushi</p><p><del>å®é™…ä¸Šæ²¡æƒ³å¥½è¦å†™å•¥ï¼Œæ‰€ä»¥éšä¾¿æ°´ä¸€ç¯‡åšå®¢</del></p><p>æ”¾é¦–DTï¼š<iframe allow="autoplay *; encrypted-media *; fullscreen *; clipboard-write" frameborder="0" height="175" style="width:100%;max-width:660px;overflow:hidden;border-radius:10px;" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-storage-access-by-user-activation allow-top-navigation-by-user-activation" src="https://embed.music.apple.com/cn/album/%E6%B5%81%E6%B2%99/1416149926?i=1416149940"></iframe></p>]]></content>
      
      
      <categories>
          
          <category> æ‚ä¸ƒæ‚å…«çš„ä¸œè¥¿ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> éšç¬” </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
