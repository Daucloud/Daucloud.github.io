<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>é«˜ä»£é€‰è®²æœŸæœ«é‡ç‚¹æ•´ç†</title>
      <link href="/2024/06/18/%E9%AB%98%E4%BB%A3%E9%80%89%E8%AE%B2%E6%9C%9F%E6%9C%AB%E9%87%8D%E7%82%B9%E6%95%B4%E7%90%86/"/>
      <url>/2024/06/18/%E9%AB%98%E4%BB%A3%E9%80%89%E8%AE%B2%E6%9C%9F%E6%9C%AB%E9%87%8D%E7%82%B9%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<blockquote><p>æ­¤æ–‡ä¸ºå®—æ­£å®‡è€å¸ˆåœ¨æœ€åä¸€èŠ‚è¯¾æ‰€åˆ’å…­ä¸ªé‡ç‚¹çš„æ•´ç†ï¼Œæƒä½œæœŸæœ«é¢„ä¹ (x)</p></blockquote><h1 id="æ±‚ç‰¹å¾å¤šé¡¹å¼å’Œæå°å¤šé¡¹å¼"><a href="#æ±‚ç‰¹å¾å¤šé¡¹å¼å’Œæå°å¤šé¡¹å¼" class="headerlink" title="æ±‚ç‰¹å¾å¤šé¡¹å¼å’Œæå°å¤šé¡¹å¼"></a>æ±‚ç‰¹å¾å¤šé¡¹å¼å’Œæå°å¤šé¡¹å¼</h1><h2 id="ç‰¹å¾å¤šé¡¹å¼"><a href="#ç‰¹å¾å¤šé¡¹å¼" class="headerlink" title="ç‰¹å¾å¤šé¡¹å¼"></a>ç‰¹å¾å¤šé¡¹å¼</h2><p>$f(\lambda)=\left|\lambda I-A\right|$</p><h2 id="æå°å¤šé¡¹å¼"><a href="#æå°å¤šé¡¹å¼" class="headerlink" title="æå°å¤šé¡¹å¼"></a>æå°å¤šé¡¹å¼</h2><h3 id="æ–¹æ³•-1"><a href="#æ–¹æ³•-1" class="headerlink" title="æ–¹æ³• 1"></a>æ–¹æ³• 1</h3><p><strong>å®šç† 7.4</strong></p><script type="math/tex; mode=display">æå°å¤šé¡¹å¼m(\lambda)å’Œç‰¹å¾å¤šé¡¹å¼f(\lambda)å«æœ‰å®Œå…¨ç›¸åŒçš„æ ¹é›†å’Œä¸å¯çº¦å› å­é›†</script><p>ä¸€èˆ¬æ¥è¯´è€ƒè¯•æ¶‰åŠçš„çŸ©é˜µå¹¶ä¸å¤æ‚ï¼Œç»´æ•°ä¹Ÿè¾ƒå°ï¼Œå› æ­¤å¯ä»¥æ ¹æ®å®šç† 7.4ï¼Œä»å°åˆ°å¤§å°è¯•$f(\lambda)$çš„å› å¼$a(\lambda)$æ˜¯å¦æ»¡è¶³$a(A)=O$ä»¥ç¡®å®šæœ€å°å¤šé¡¹å¼</p><h3 id="æ–¹æ³•-2"><a href="#æ–¹æ³•-2" class="headerlink" title="æ–¹æ³• 2"></a>æ–¹æ³• 2</h3><p>æ±‚å‡ºçŸ©é˜µ$A$çš„ Jordan æ ‡å‡†å‹ï¼Œåˆ™$(\lambda-\lambda_i)$é¡¹åœ¨$m(\lambda)$ä¸­çš„é‡æ•°å³ä¸ºç‰¹å¾å€¼ä¸º$\lambda_i$çš„ Jordan å—çš„æœ€å¤§é˜¶æ•°ï¼›å¯¹äºå¹¿ä¹‰ Jordan å—ï¼Œå¯ä»¥ç›´æ¥å–ä¸ºç›¸åº”çš„ä¸å¯çº¦å› å¼</p><h1 id="è®¡ç®—å¤æ–¹é˜µçš„-Jordan-æ ‡å‡†å‹åŠå¯é€†çŸ©é˜µ-P-s-t-P-1-AP-J"><a href="#è®¡ç®—å¤æ–¹é˜µçš„-Jordan-æ ‡å‡†å‹åŠå¯é€†çŸ©é˜µ-P-s-t-P-1-AP-J" class="headerlink" title="è®¡ç®—å¤æ–¹é˜µçš„ Jordan æ ‡å‡†å‹åŠå¯é€†çŸ©é˜µ$P,s.t.P^{-1}AP=J$"></a>è®¡ç®—å¤æ–¹é˜µçš„ Jordan æ ‡å‡†å‹åŠå¯é€†çŸ©é˜µ$P,s.t.P^{-1}AP=J$</h1><h2 id="Jordan-æ ‡å‡†å‹è®¡ç®—"><a href="#Jordan-æ ‡å‡†å‹è®¡ç®—" class="headerlink" title="Jordan æ ‡å‡†å‹è®¡ç®—"></a>Jordan æ ‡å‡†å‹è®¡ç®—</h2><h3 id="æ–¹æ³•-1-1"><a href="#æ–¹æ³•-1-1" class="headerlink" title="æ–¹æ³• 1"></a>æ–¹æ³• 1</h3><ol><li>æ±‚å‡ºç‰¹å¾å¤šé¡¹å¼$f(\lambda)$\</li><li>åˆ™$\lambda_i$çš„ä¸ªæ•°ä¸º$r_i=n-rank(A-\lambda_iI)$ï¼Œä¹Ÿå³$f(\lambda)$ä¸­$(\lambda-\lambda_i)$çš„æ¬¡æ•°</li><li>é˜¶æ•°ä¸º$t$çš„ Jordan å—çš„ä¸ªæ•°ä¸º$r_i(t)=rank(A-\lambda_iI)^{t+1}+rank(A-\lambda_iI)^{t-1}-2rank(A-\lambda_iI)^t$<blockquote><p>è§£é‡Šï¼šæ³¨æ„åˆ°$rank(A-\lambda_iI)^m$ä¸­ä»…å«æœ‰åŸæœ¬é˜¶æ•°å¤§äº$m$çš„$\lambda_i$çš„ Jordan å—ï¼Œæ•…$rank(A-\lambda_iI)^m-rank(A-\lambda_iI)^{m+1}$æ°å¥½ä¸ºé˜¶æ•°å¤§äº$m$çš„$\lambda_i$çš„ Jordan å—çš„ä¸ªæ•°ï¼Œå› æ­¤$\left(rank(A-\lambda_iI)^{m-1}-rank(A-\lambda_iI)^m\right)-\left(rank(A-\lambda_iI)^m-rank(A-\lambda_iI)^{m+1}\right)$ä¸ºé˜¶æ•°å¤§äº$m-1$çš„$\lambda_i$çš„ Jordan å—çš„ä¸ªæ•°å’Œé˜¶æ•°å¤§äº$m$çš„ Jordan å—çš„ä¸ªæ•°ä¹‹å·®ï¼Œæ¢è¨€ä¹‹å³ä¸ºé˜¶æ•°ä¸º$m$çš„ Jordan å—çš„ä¸ªæ•°</p></blockquote></li></ol><ul><li>ä¸€äº› tricks:<ol><li>ç”±äºè€ƒè¯•æ¶‰åŠçš„ Jordan å—é€šå¸¸ç»´æ•°ä¸å¤§ï¼Œè€Œ$m(\lambda_i)$åˆ™ç¡®å®šäº†å„$\lambda_i$çš„ Jordan å—çš„æœ€å¤§é˜¶æ•°ï¼Œé€šè¿‡æ­¤å¾€å¾€å°±å¯æ±‚å‡º Jordan æ ‡å‡†å‹</li><li>å¯ä»¥é€šè¿‡è®¡ç®—$dim Ker(\lambda_iI-A)$æ¥è·çŸ¥$\lambda_i$çš„ Jordan å—çš„ä¸ªæ•°ï¼ˆ$Ker(\lambda_iI-A)$ä¸­çš„æ¯ä¸ªå‘é‡éƒ½å¯ä»¥ä½œä¸ºä¸€æ¡ Jordan é“¾çš„ç»ˆç»“ï¼‰</li></ol></li></ul><h3 id="æ–¹æ³•-2-1"><a href="#æ–¹æ³•-2-1" class="headerlink" title="æ–¹æ³• 2"></a>æ–¹æ³• 2</h3><p>æ±‚å‡º$A$çš„åˆç­‰å› å­ç»„ï¼Œåˆ™ Jordan æ ‡å‡†å‹ä¸º</p><script type="math/tex; mode=display">J=\begin{bmatrix}J\left(p_1^{k_1}\right)&&\\&\ddots&\\&&J\left(p_s^{k_s}\right)\end{bmatrix}</script><blockquote><p>æ³¨ï¼šè¯¥æ–¹é˜µç§°ä¸ºç¬¬ä¸‰ç§ç›¸ä¼¼æ ‡å‡†å‹ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰ç¬¬ä¸€ç±»ç›¸ä¼¼æ ‡å‡†å‹ï¼ˆæœ‰ç†æ ‡å‡†å‹ï¼‰å’Œç¬¬äºŒç±»ç›¸ä¼¼æ ‡å‡†å‹ï¼ˆåˆç­‰å› å­å‹é˜µå‹ï¼‰:</p></blockquote><script type="math/tex; mode=display">C=\begin{bmatrix}C\left(d_1\right)&&\\&\ddots&\\&&C\left(d_r\right)\end{bmatrix}\\G=\begin{bmatrix}C\left(p_1^{k_1}\right)&&\\&\ddots&\\&&C\left(p_s^{k_s}\right)\end{bmatrix}</script><h2 id="å¯é€†çŸ©é˜µ-P-çš„æ±‚å–"><a href="#å¯é€†çŸ©é˜µ-P-çš„æ±‚å–" class="headerlink" title="å¯é€†çŸ©é˜µ P çš„æ±‚å–"></a>å¯é€†çŸ©é˜µ P çš„æ±‚å–</h2><h3 id="æ–¹æ³•-1-2"><a href="#æ–¹æ³•-1-2" class="headerlink" title="æ–¹æ³• 1"></a>æ–¹æ³• 1</h3><ol><li>æ±‚å‡º$Ker(A-\lambda_iI)$ï¼Œå…¶ä¸­çš„å‘é‡å¯ä»¥ä½œä¸º$\lambda_i$çš„ Jordan é“¾çš„ç»ˆç»“</li><li>ä½†æ˜¯éœ€è¦æ³¨æ„ï¼Œå¹¶ä¸æ˜¯æ¯ä¸ªå‘é‡éƒ½å¯ä»¥å½¢æˆé˜¶æ•°æ­£ç¡®çš„ Jordan å—ã€‚è­¬å¦‚ï¼Œè®¾$Ker\left(A-\lambda_iI\right)$ç”±$\alpha_1$å’Œ$\alpha_2$å¼ æˆã€‚å¦‚æœä¸€ä¸ª Jordan å—ä¸ºäºŒé˜¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦è®¾æ­¤ Jordan é“¾çš„ç»ˆç»“ä¸º$s\alpha_1+t\alpha_2$ï¼Œå¹¶ç¡®ä¿$\left(A-\lambda_iI\right)x=s\alpha_1+t\alpha_2$æœ‰è§£ï¼ˆé€šè¿‡è€ƒè™‘å¢å¹¿çŸ©é˜µ$\begin{bmatrix}A&amp;s\alpha_1+t\alpha_2\end{bmatrix}$ï¼‰</li></ol><blockquote><p>ä¸¾ä¸€é“ä¾‹é¢˜ï¼Œä¾¿äºç†è§£<br><em>é—®ï¼šæ±‚æ­¤å¤æ•°åŸŸä¸Šæ–¹é˜µçš„ Jordan æ ‡å‡†å‹å’Œå¯é€†çŸ©é˜µ Pï¼š</em></p><script type="math/tex; mode=display">\begin{bmatrix}1&-3& 0&3\\-2&-6& 0&13\\0&-3& 1&3\\-1&-4& 0&8\\\end{bmatrix}</script><p><em>è§£:</em></p><p>$$f(\lambda)=(\lambda-1)^4,m(\lambda)=(\lambda-1)^3,æ•… Jordan æ ‡å‡†å‹ä¸º\begin{bmatrix}</p></blockquote><pre><code>1&amp;0&amp;0&amp;0\\1&amp;1&amp;0&amp;0\\0&amp;1&amp;1&amp;0\\0&amp;0&amp;0&amp;1\\</code></pre><p>\end{bmatrix}$$</p><blockquote><p>$$Ker(A-I)=span\left(\begin{bmatrix}</p></blockquote><pre><code>3\\1\\0\\1</code></pre><p>\end{bmatrix},\begin{bmatrix}<br>0\0\1\0<br>\end{bmatrix}\right)$$</p><blockquote><p>$$å–\alpha_4=\begin{bmatrix}</p></blockquote><pre><code>3\\1\\0\\1</code></pre><p>\end{bmatrix}ï¼Œè®¾\alpha_3=\begin{bmatrix}<br>3s\s\t\s<br>\end{bmatrix}$$</p><blockquote><p>$$æ¬²ä½¿(A-I)\alpha_2=\alpha_3 æœ‰è§£ï¼Œè€ƒè™‘å¢å¹¿çŸ©é˜µ\begin{bmatrix}</p></blockquote><pre><code>0&amp;-3&amp;0&amp;3&amp;3s\\-2&amp;-7&amp;0&amp;13&amp;s\\0&amp;-3&amp;0&amp;3&amp;t\\-1&amp;-4&amp;0&amp;7&amp;s\\</code></pre><p>\end{bmatrix}ï¼Œå¯çŸ¥ 3s=t$$</p><blockquote><p>$$ä¸å¦¨å– s=1ï¼Œæ•…\alpha_3=\begin{bmatrix}</p></blockquote><pre><code>3\\1\\3\\1</code></pre><p>\end{bmatrix}ï¼Œå¹¶è§£å¾—\alpha_2=\begin{bmatrix}<br>3u+3\v-1\0\0<br>\end{bmatrix}$$</p><blockquote><script type="math/tex; mode=display">å†ä»¤(A-I)\alpha_1=\alpha_2æœ‰è§£</script><script type="math/tex; mode=display">è€ƒè™‘å¢å¹¿çŸ©é˜µ\begin{bmatrix}</script></blockquote><pre><code>0&amp;-3&amp;0&amp;3&amp;3u+3\\-2&amp;-7&amp;0&amp;13&amp;u-1\\0&amp;-3&amp;0&amp;3&amp;v\\-1&amp;-4&amp;0&amp;7&amp;w\\</code></pre><p>\end{bmatrix}ï¼Œå› æ­¤æœ‰ 3u+3=v,2w-\frac{v}{3}=u-1$$</p><blockquote><script type="math/tex; mode=display">ä¸å¦¨å–u=1,v=6,w=1æ•…æœ‰\alpha_2=\begin{bmatrix}</script></blockquote><pre><code>6\\0\\6\\1</code></pre><p>\end{bmatrix},\alpha_1=\begin{bmatrix}<br>7\-2\0\0<br>\end{bmatrix}$$</p><blockquote><p>$$æ•…çŸ¥ P=\begin{bmatrix}</p></blockquote><pre><code>7&amp;6&amp;3&amp;3\\-2&amp;0&amp;1&amp;1\\0&amp;6&amp;3&amp;0\\0&amp;1&amp;1&amp;1</code></pre><p>\end{bmatrix}$$</p><h3 id="æ–¹æ³•-2-2"><a href="#æ–¹æ³•-2-2" class="headerlink" title="æ–¹æ³• 2"></a>æ–¹æ³• 2</h3><p>ç”±äº$\lambda I-A$å’Œ$\lambda I-J$æœ‰ç›¸åŒçš„ Smith æ ‡å‡†å‹ï¼Œè€Œ Smith æ ‡å‡†å‹æœ‰å›ºå®šçš„ç®—æ³•æ±‚è§£ï¼Œå› æ­¤å¯ä»¥æ±‚$P_1^{-1}\left(\lambda I-A\right)P_1=P_2^{-1}\left(\lambda I-J\right)P_2=S$ï¼Œåˆ™$P=P_2P_1^{-1}$</p><h1 id="lambda-çŸ©é˜µå¯¹è§’åŒ–ï¼›åˆç­‰å› å­ã€ä¸å˜å› å­å’Œ-Jordan-æ ‡å‡†å‹çš„å…³ç³»"><a href="#lambda-çŸ©é˜µå¯¹è§’åŒ–ï¼›åˆç­‰å› å­ã€ä¸å˜å› å­å’Œ-Jordan-æ ‡å‡†å‹çš„å…³ç³»" class="headerlink" title="$\lambda-$çŸ©é˜µå¯¹è§’åŒ–ï¼›åˆç­‰å› å­ã€ä¸å˜å› å­å’Œ Jordan æ ‡å‡†å‹çš„å…³ç³»"></a>$\lambda-$çŸ©é˜µå¯¹è§’åŒ–ï¼›åˆç­‰å› å­ã€ä¸å˜å› å­å’Œ Jordan æ ‡å‡†å‹çš„å…³ç³»</h1><h2 id="lambda-çŸ©é˜µå¯¹è§’åŒ–"><a href="#lambda-çŸ©é˜µå¯¹è§’åŒ–" class="headerlink" title="$\lambda-$çŸ©é˜µå¯¹è§’åŒ–"></a>$\lambda-$çŸ©é˜µå¯¹è§’åŒ–</h2><p>è¯¾æœ¬æœ‰è¯¦å°½çš„ç®—æ³•ã€‚ç®€è€Œè¨€ä¹‹å³æ˜¯ä¸æ–­é™ä½$a_{11}$çš„æ¬¡æ•°<br><img src="https://img.picgo.net/2024/06/18/2024061812322236777062ff2476a40.jpg" alt="3663133d2ba182b3d2aca82db4134e2.jpg"><br><img src="https://img.picgo.net/2024/06/18/202406181233913c52ac3b6584e7b8d.jpg" alt="9b55172bde8fb4579a9083312526854.jpg"></p><h2 id="åˆç­‰å› å­ã€ä¸å˜å› å­ä¸-Jordan-æ ‡å‡†å‹çš„å…³ç³»"><a href="#åˆç­‰å› å­ã€ä¸å˜å› å­ä¸-Jordan-æ ‡å‡†å‹çš„å…³ç³»" class="headerlink" title="åˆç­‰å› å­ã€ä¸å˜å› å­ä¸ Jordan æ ‡å‡†å‹çš„å…³ç³»"></a>åˆç­‰å› å­ã€ä¸å˜å› å­ä¸ Jordan æ ‡å‡†å‹çš„å…³ç³»</h2><h3 id="ä¸‰ç§åŸºæœ¬å› å­çš„æ¦‚å¿µ"><a href="#ä¸‰ç§åŸºæœ¬å› å­çš„æ¦‚å¿µ" class="headerlink" title="ä¸‰ç§åŸºæœ¬å› å­çš„æ¦‚å¿µ"></a>ä¸‰ç§åŸºæœ¬å› å­çš„æ¦‚å¿µ</h3><ul><li>å¯¹äº$\lambda-$çŸ©é˜µ 1. ä¸å˜å› å­ï¼šSmith æ ‡å‡†å‹å¯¹è§’çº¿ä¸Šçš„æ‰€æœ‰éé›¶å…ƒç´ ï¼š$d_1(\lambda),â€¦,d_r(\lambda)$ 2. è¡Œåˆ—å¼å› å­ï¼šæ‰€æœ‰ k é˜¶éé›¶å­è¡Œåˆ—å¼çš„é¦– 1 æœ€å¤§å…¬å› å­ï¼š$D_1,â€¦,D_r$ 3. åˆç­‰å› å­ï¼šä¸å˜å› å­çš„å‡†ç´ å› å­å…¨ä½“ï¼ˆ1 ä¸è®¡å…¥ï¼‰<blockquote><p>ä¸å˜å› å­å’Œè¡Œåˆ—å¼å› å­äº’ç›¸å†³å®šï¼š$D<em>k=\prod</em>{i=1}^kd<em>i,d_k=\frac{D_k}{D</em>{k-1}}$</p></blockquote></li><li>å¯¹äºæ•°å­—çŸ©é˜µï¼Œå…¶ä¸‰ç§åŸºæœ¬å› å­æŒ‡çš„æ˜¯$\lambda I-A$çš„ä¸‰ç§åŸºæœ¬å› å­ï¼ˆä½†æ˜¯ 1 å‡ä¸è®¡å…¥ï¼‰</li></ul><h3 id="ä»åˆç­‰å› å­æ±‚-Jordan-æ ‡å‡†å‹"><a href="#ä»åˆç­‰å› å­æ±‚-Jordan-æ ‡å‡†å‹" class="headerlink" title="ä»åˆç­‰å› å­æ±‚ Jordan æ ‡å‡†å‹"></a>ä»åˆç­‰å› å­æ±‚ Jordan æ ‡å‡†å‹</h3><script type="math/tex; mode=display">J=\begin{bmatrix}J\left(p_1^{k_1}\right)&&\\&\ddots&\\&&J\left(p_s^{k_s}\right)\end{bmatrix}</script><h1 id="Gram-Schmidt-æ­£äº¤åŒ–"><a href="#Gram-Schmidt-æ­£äº¤åŒ–" class="headerlink" title="Gram-Schmidt æ­£äº¤åŒ–"></a>Gram-Schmidt æ­£äº¤åŒ–</h1><p>è®¾$A=\begin{bmatrix}v_1&amp;v_2&amp;\cdots&amp;v_n\end{bmatrix}$<br>åˆ™å…¶æ­£äº¤åŒ–ç»“æœ$Q=\begin{bmatrix}w_1&amp;w_2&amp;\cdots&amp;w_n\end{bmatrix}$<br>å…¶ä¸­ï¼š</p><script type="math/tex; mode=display">\begin{aligned}&w_1=v_1\\&w_2=v_2-\frac{\left<w_1,v_2\right>}{\left<w_1,w_1\right>}w_1\\&w_3=v_3-\frac{\left<w_1,v_3\right>}{\left<w_1,w_1\right>}w_1-\frac{\left<w_2,v_3\right>}{\left<w_2,w_2\right>}w_2\\&\vdots\\&w_n=v_n-\frac{\left<w_1,v_n\right>}{\left<w_1,w_1\right>}w_1-\frac{\left<w_2,v_n\right>}{\left<w_2,w_2\right>}w_2-\cdots-\frac{\left<w_n,v_n\right>}{\left<w_n,w_n\right>}w_n\\\end{aligned}</script><p>ä¸Šå­¦æœŸçš„å†…å®¹ï¼Œä¸è¿‡å¤šèµ˜è¿°</p><h1 id="è§„èŒƒæ–¹é˜µè°±åˆ†è§£"><a href="#è§„èŒƒæ–¹é˜µè°±åˆ†è§£" class="headerlink" title="è§„èŒƒæ–¹é˜µè°±åˆ†è§£"></a>è§„èŒƒæ–¹é˜µè°±åˆ†è§£</h1><ol><li>æ±‚å‡ºè§„èŒƒæ–¹é˜µ A çš„ç‰¹å¾å¤šé¡¹å¼ï¼Œå¹¶è§£å‡ºç‰¹å¾å€¼åˆ™$U^{-1}AU=diag(\lambda_{1},\cdots,\lambda_n)$</li><li>æ±‚å‡º$Ker(\lambda_iI-A)$çš„ä¸€ç»„åŸºï¼Œå¯¹å…¶æ–½è¡Œ Gram-Schimidt æ­£äº¤åŒ–</li><li>å°†ä¸Šä¸€æ­¥å¾—åˆ°çš„æ‰€æœ‰å‘é‡æ’åˆ—èµ·æ¥ï¼Œå³å¾—åˆ°é…‰æ–¹é˜µ$U$</li></ol><h1 id="Jordan-æ ‡å‡†å‹çš„åº”ç”¨å’Œå¯å¯¹è§’åŒ–æ¡ä»¶"><a href="#Jordan-æ ‡å‡†å‹çš„åº”ç”¨å’Œå¯å¯¹è§’åŒ–æ¡ä»¶" class="headerlink" title="Jordan æ ‡å‡†å‹çš„åº”ç”¨å’Œå¯å¯¹è§’åŒ–æ¡ä»¶"></a>Jordan æ ‡å‡†å‹çš„åº”ç”¨å’Œå¯å¯¹è§’åŒ–æ¡ä»¶</h1><p>è®¾ A ä¸º n é˜¶å¤æ–¹é˜µï¼Œåˆ™å¦‚ä¸‹å‘½é¢˜ç­‰ä»·ï¼š</p><ol><li>A åœ¨$\mathbb{C}$å¯å¯¹è§’åŒ–</li><li>A çš„å‡ ä½•é‡æ•°ä¸º nï¼ˆæœ‰ n ä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡ï¼‰</li><li>A åœ¨å¤æ•°åŸŸä¸Šçš„åˆç­‰å› å­å‡ä¸º 1 æ¬¡ï¼ˆå³åˆç­‰å› å­å‡æ— é‡æ ¹ï¼‰</li><li>A çš„ä¸å˜å› å­å‡æ— é‡æ ¹</li><li>A çš„æå°å¤šé¡¹å¼æ— é‡æ ¹</li><li>$\forall c\in\mathbb{C},rank(cI-A)=rank\left((cI-A)^2\right)$<br>æ­¤å¤–ï¼Œä»¥ä¸Šå‘½é¢˜çš„ä¸€ä¸ªå……åˆ†æ¡ä»¶æ˜¯$f(\lambda)$æ— é‡æ ¹</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Chapter2 Using ğŸ¤— Transformers</title>
      <link href="/2024/05/14/NLP-Course-of-Hugging-Face/"/>
      <url>/2024/05/14/NLP-Course-of-Hugging-Face/</url>
      
        <content type="html"><![CDATA[<h1 id="Behind-the-pipeline"><a href="#Behind-the-pipeline" class="headerlink" title="Behind the pipeline"></a>Behind the pipeline</h1><ul><li><code>pipeline()</code> groups the â€œpreprocessing-pass the inputs through the model-postprocessingâ€ together<br><img src="https://cdn.jsdelivr.net/gh/Daucloud/imagecdn/test/202404291430930.png" alt="|525"><h2 id="Preprocessing-with-a-tokenizer"><a href="#Preprocessing-with-a-tokenizer" class="headerlink" title="Preprocessing with a tokenizer"></a>Preprocessing with a tokenizer</h2></li><li>convert the raw context into vectors with a tokenizer:<ol><li>splitting the input into tokens(words, subwords, symbols like punctuation)</li><li>each token to an integer</li><li>adding additional inputs that maybe useful to the model<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">checkpoint=<span class="string">"checkoutName"</span></span><br><span class="line">tokenizer=AutoTokenizer.from_pretrained(checkpoint)<span class="comment"># all the preprocessing needs to be done exactly the same way as when the model was pretrained</span></span><br><span class="line">raw_input=<span class="string">"context"</span></span><br><span class="line">inputs=tokenizer(raw_inputs,padding=<span class="literal">True</span>,truncation=<span class="literal">True</span>,return_tensors=<span class="string">"pt"</span>)</span><br><span class="line"><span class="comment"># as for the return_tensors, pt: Pytorch, tf: TensorFlow, np: Numpy, jax: JAX</span></span><br><span class="line">print(inputs)</span><br><span class="line"><span class="comment">### the results, is a dictionary, which contains two key-value pair</span></span><br><span class="line">&#123;</span><br><span class="line"> <span class="string">"input_ids"</span>: tensor([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]), <span class="comment"># the unique identifiers for each token</span></span><br><span class="line"> <span class="string">"attention_mask"</span>: tensor([<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Going-through-the-model"><a href="#Going-through-the-model" class="headerlink" title="Going through the model"></a>Going through the model</h2></li></ol></li><li>convert input IDs into logits<br><img src="https://raw.githubusercontent.com/Daucloud/imagecdn/main/test/202405140923963.png" alt="image.png"></li><li>ğŸ¤— provides the <code>AutoModel</code> class, which corresponds to the hidden states step<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel</span><br><span class="line">checkout=<span class="string">"checkoutname"</span></span><br><span class="line">model=AutoModel.from_pretrained(checkout)</span><br><span class="line">outputs=(**inputs)</span><br><span class="line"><span class="comment"># the Automodel converts the inputs(seen last part) into a three-dimensional vector:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">1. batch size: the number of sequences processed at a time</span></span><br><span class="line"><span class="string">2. sequence length</span></span><br><span class="line"><span class="string">3. Hidden size: usually very large(768, 3072, even more)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print(outputs.last_hidden_state.size) <span class="comment"># torch.Size([2,16,768])</span></span><br></pre></td></tr></table></figure></li><li>There are also some other architectures for specific tasks. You can understand them as <code>AutoModel</code> followed by the head<ul><li>ForCausalLM</li><li>ForMaskedLM</li><li>ForMultipleChoice</li><li>ForQuestionAnswering</li><li>ForSequenceClassification</li><li>ForTokenClassification</li><li>other ğŸ¤—<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line">checkout=<span class="string">"checkoutName"</span></span><br><span class="line">model=AutoModelForSequenceClassification.from_pretrained(checkout)</span><br><span class="line">outputs=(**inputs)</span><br><span class="line">print(outputs.logits.shape) <span class="comment">#torch.Size([2,2]), the size is much smaller than the results of AutoModel</span></span><br></pre></td></tr></table></figure><h2 id="Postprocessing-the-output"><a href="#Postprocessing-the-output" class="headerlink" title="Postprocessing the output"></a>Postprocessing the output</h2></li></ul></li><li>Convert the logits into predictions , aka,  convert the raw, unnormalized scores to probabilties which can be understood by human</li><li>Usually we will use a SoftMax Layer to work on this<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">predictions=torch.nn.functional.softmax(outputs.logits, dim=<span class="number">-1</span>)</span><br><span class="line">print(predictions)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[5.0e-2,9.5e-1],[1.0e-1,9.0-1]],grad_fn=&lt;SoftmaxBackward&gt;)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># we can inspect the id2label the following way:</span></span><br><span class="line">model.config.id2label <span class="comment"># &#123;0:'NEGATIVE',1:'POSITIVE'&#125;</span></span><br></pre></td></tr></table></figure><h1 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h1></li><li><code>AutoClass</code> and it relatives mentioned above are simple wrappers which can automatically guess the architechture from your checkpoint<h2 id="Creating-A-Transformer"><a href="#Creating-A-Transformer" class="headerlink" title="Creating A Transformer"></a>Creating A Transformer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertConfig, BertModel, </span><br><span class="line">config=BertConfig()</span><br><span class="line">model=BertModel(config)<span class="comment"># in this way, you will get a randomly initialized bert model, which will output gibberish</span></span><br><span class="line">model=BertModel.from_pretrained(<span class="string">"bert-base-cased"</span>)<span class="comment"># the model is instantiate with the checkpoint trained by the bert team, which is less time-consuming and more environment-friendly; you can also replace the BertModel with AutoClass. Actually, it is more suggested to use AutoModel rather than a specific model, which will also work even if the architechture is different</span></span><br></pre></td></tr></table></figure></li><li>once you used the checkpoint, the weights are downloaded and cached to <code>~/.cache/huggingface/transformers</code></li><li>you can use the <code>save_pretrained</code> method to save the model to your disk:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.save_pretrained(<span class="string">"path/to/directory"</span>)</span><br><span class="line">!ls path/to/directory</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">config.json pytorch_model.bin</span></span><br><span class="line"><span class="string"># the two files go hand in hand, the `config.json` contains the attributes necessary to build the architechture, and the `pytorch_model.bin` contains the weights(checkpoints) which are the parameters of your model</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h1 id="Tokenizers"><a href="#Tokenizers" class="headerlink" title="Tokenizers"></a>Tokenizers</h1><h2 id="Algorithms-for-tokenization"><a href="#Algorithms-for-tokenization" class="headerlink" title="Algorithms for tokenization"></a>Algorithms for tokenization</h2><h3 id="Word-Based"><a href="#Word-Based" class="headerlink" title="Word-Based"></a>Word-Based</h3></li></ul><ol><li>split the the words according to the given marks, such as spaces, punctuationsâ€¦</li><li>map each word to an ID. The ID is determined through the vocabulary, which is usually very large. For example, the size of a English vocabulary may be as large as 500,000</li><li>As for the words that are not in the vocabulary, they are often represented by the unkown token such as <code>[UNK]</code> or  <code>&lt;unk&gt;</code>. However, as you can imagine, this will lose imformation, so it is sensible to avoid the unknown tokens as much as possible<h3 id="Character-Based"><a href="#Character-Based" class="headerlink" title="Character-Based"></a>Character-Based</h3></li><li>split the sentences by the characters<blockquote><p>This method will defintely reduce the amount of unknown tokens. However, is will also cause the vocabulary to be too large and the results less meaningful</p><h3 id="Subword-Tokenization"><a href="#Subword-Tokenization" class="headerlink" title="Subword Tokenization"></a>Subword Tokenization</h3></blockquote></li><li>representing the rare words with the composite of frequently used words<blockquote><p>this will spare the space while remaining the semantic meanings as much as possible, which is especially useful to agglutinative languages such as Turkish</p></blockquote></li></ol><p><em>examples</em>:<br>-<br>Byte -level BPE: GPT-2</p><ul><li>WordPiece: BERT</li><li>SentencePiece or Unigram: multilingual models<h2 id="Loading-and-Saving"><a href="#Loading-and-Saving" class="headerlink" title="Loading and Saving"></a>Loading and Saving</h2></li><li>nearly the same as loading and saving the models: use <code>from_pretrained</code> and <code>save_pretrained</code></li><li>the things that cached<ul><li>algorithms: similar to <em>architechture</em> of the models</li><li>vocabularay: similar to <em>weights</em>of the models<h2 id="Encoding"><a href="#Encoding" class="headerlink" title="Encoding"></a>Encoding</h2></li></ul></li><li>there are two steps while encoding<h3 id="tokenization"><a href="#tokenization" class="headerlink" title="tokenization"></a>tokenization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer=AutoTokenizer.from_pretrained(<span class="string">"modelName"</span>)</span><br><span class="line">tokens=tokenizer.tokenize(<span class="string">"sequence"</span>)</span><br><span class="line"></span><br><span class="line">print(tokens) <span class="comment"># print the results of spilting</span></span><br></pre></td></tr></table></figure><h3 id="From-tokens-to-input-IDs"><a href="#From-tokens-to-input-IDs" class="headerlink" title="From tokens to input IDs"></a>From tokens to input IDs</h3></li><li>the models can only accept the tensors as inputs, so we need to map our tokens in to numbers</li><li>use the vocabulary to work on this, which is a dictionary mapping the tokens to numbers<h1 id="Handling-Multiple-Sequences"><a href="#Handling-Multiple-Sequences" class="headerlink" title="Handling Multiple Sequences"></a>Handling Multiple Sequences</h1><h2 id="Models-expect-a-batch-of-inputs"><a href="#Models-expect-a-batch-of-inputs" class="headerlink" title="Models expect a batch of inputs"></a>Models expect a batch of inputs</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">model=AutoModelForSequenceClassification.from_pretrained(<span class="string">"checkpointName"</span>)</span><br><span class="line">tokenizer=AutoTokenizer.from_pretrained(<span class="string">"checkpointName"</span>)</span><br><span class="line"></span><br><span class="line">tokens=tokenizer.tokenize(<span class="string">"raw inputs"</span>)</span><br><span class="line">token_ids=tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line">input_ids=torch.tensor(token_ids)</span><br><span class="line"></span><br><span class="line">model(input_ids)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">model([input_ids]) <span class="comment"># this line will succeed</span></span><br></pre></td></tr></table></figure><h2 id="Padding-the-inputs"><a href="#Padding-the-inputs" class="headerlink" title="Padding the inputs"></a>Padding the inputs</h2></li><li>the batch of inputs must have the same lengths to get through the model, for the shape of tensor is rectangle. This is why will introduce the <code>padding_id</code> to pad the inputs. You can use the attributes <code>pad_token_id</code> of a tokenizer to get access it<h2 id="Attention-masks"><a href="#Attention-masks" class="headerlink" title="Attention masks"></a>Attention masks</h2></li><li>the key features of transformers is the attention layers that contextualize each token. As a consequence, the padding ids will also make a difference to the output, which is not expected.</li><li>The Attention masks comes for this. The value 0 represents ignoring the corresponding tokens and 1 do the opposite<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">batched_ids = [</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, <span class="number">200</span>],</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, tokenizer.pad_token_id],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">attention_mask = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))</span><br><span class="line">print(outputs.logits)</span><br></pre></td></tr></table></figure><h2 id="Longer-sequences"><a href="#Longer-sequences" class="headerlink" title="Longer sequences"></a>Longer sequences</h2></li><li>there exists a limit to the length with all transformer models. If you want to pass a sequence longer than the limits, you should <strong>truncate</strong> your sequence or change to a model allowing longer inputs.<h1 id="Putting-it-all-together"><a href="#Putting-it-all-together" class="headerlink" title="Putting it all together"></a>Putting it all together</h1><h2 id="put-the-tokenization-steps-together"><a href="#put-the-tokenization-steps-together" class="headerlink" title="put the tokenization steps together"></a>put the tokenization steps together</h2></li><li>As a review, there are three steps we need to do to tokenize the raw inputs: <code>tokenizer.tokenize()</code>â€”&gt;<code>tokenizer.convert_tokens_to_ids()</code>â€”&gt;<code>torch.tensor()</code></li><li>For convenience, the <code>ğŸ¤— transformers</code> actually provides a high-level function to put all these steps together, which is <code>tokenizer()</code> itself<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">tokenizer=AutoTokenizer.from_pretrained(<span class="string">"checkpointName"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line">sequence=<span class="string">"1"</span></span><br><span class="line">input1=tokenizer(sequence) <span class="comment"># valid</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line">sequences=[<span class="string">"1"</span>,<span class="string">"2"</span>]</span><br><span class="line">input2=tokenizer(sequneces)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 pad</span></span><br><span class="line">input3=tokenizer(sequences, padding=<span class="string">"longest"</span>) <span class="comment"># pad to the maximum sequece length</span></span><br><span class="line">input4=tokenizer(sequences, padding=<span class="string">"max_length"</span>) <span class="comment"># pad to the model maximum length</span></span><br><span class="line">input5=tokenizer(sequences, padding=<span class="string">"max_length"</span>, max_length=<span class="number">8</span>) <span class="comment"># pad to the specified maximum length</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 truncate</span></span><br><span class="line">input6=tokenizer(sequences, truncation=<span class="literal">True</span>) <span class="comment"># truncate the sequences longer than the model limit</span></span><br><span class="line">input7=tokenizer(sequences, max_length=<span class="number">8</span>, truncation=<span class="literal">True</span>) <span class="comment"># truncate the sequences longer than the specified length</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5 switch the type of tensors returned</span></span><br><span class="line">input8=tokenizer(sequences, padding=<span class="literal">True</span>, return_tensors=<span class="string">"pt"</span>) <span class="comment"># pt stands for the Pytorch, tf for TensorFlow, np for NumPy</span></span><br></pre></td></tr></table></figure><h2 id="Special-words"><a href="#Special-words" class="headerlink" title="Special words"></a>Special words</h2></li><li>Some models add the special token such as <code>[CLS]</code> at the beginning, some add the special token such as <code>[SEP]</code> at the end, some add both, and some add none.<h2 id="Wrapping-up-From-tokenizer-to-model"><a href="#Wrapping-up-From-tokenizer-to-model" class="headerlink" title="Wrapping up: From tokenizer to model"></a>Wrapping up: From tokenizer to model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transfomers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"></span><br><span class="line">tokenizer=Autokenizer.form_pretrained(<span class="string">"checkpointName"</span>)</span><br><span class="line">model=AutoModel.form_pretrained(<span class="string">"checkpointName"</span>)</span><br><span class="line"></span><br><span class="line">sequence=<span class="string">"1"</span></span><br><span class="line">Input=tokenizer(sequence)</span><br><span class="line">Output=model(**Input)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> CS Learing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> ğŸ¤— </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Chapter1 Transformer Models</title>
      <link href="/2024/05/10/NLP-Course-of-Hugging-Face/"/>
      <url>/2024/05/10/NLP-Course-of-Hugging-Face/</url>
      
        <content type="html"><![CDATA[<blockquote><p>æœ€è¿‘æ‰“ç®—å…¥é—¨ NLPï¼Œåœ¨è‡ªå­¦ ğŸ¤— çš„<a href="https://huggingface.co/learn/nlp-course/chapter1/1" target="_blank" rel="noopener">NLP Course</a>ï¼Œä½†æ˜¯æ„Ÿè§‰è‡ªå·±è¿‡äºæ‘†çƒ‚äº†ã€‚äºæ˜¯æ‰“ç®—è¾¹å­¦è¾¹åšç¬”è®°ï¼Œäº‰å–åœ¨æœŸæœ«ä¹‹å‰æŠŠæœ¬è¯¾ç¨‹å­¦å®Œ</p></blockquote><h1 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline()"></a><code>Pipeline()</code></h1><ul><li>the most basic object in the ğŸ¤— Transformers llibrary</li><li>It can:<ul><li>feature-extraction(get a vector representing the text)</li><li>fill-task</li><li>ner(entity recognition)</li><li>question-answering</li><li>sentiment-analysis</li><li>summarization</li><li>text-generation</li><li>translation</li><li>zero-shot classification</li></ul></li></ul><h2 id="Zero-shot-Classification"><a href="#Zero-shot-Classification" class="headerlink" title="Zero-shot Classification"></a>Zero-shot Classification</h2><ul><li>you can casually assign the labels</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">classifier = pipeline(<span class="string">"zero-shot-classification"</span>)</span><br><span class="line">classifier(</span><br><span class="line">   <span class="string">"I play Genshin Impact!"</span>,</span><br><span class="line">   candidate_labels=[<span class="string">"op"</span>,<span class="string">"2-dimensional"</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="Text-Generation"><a href="#Text-Generation" class="headerlink" title="Text Generation"></a>Text Generation</h2><ul><li>involves randomness</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">generator=pipeline(<span class="string">"text-generation"</span>)</span><br><span class="line">generator(</span><br><span class="line">  <span class="string">"prompts"</span>,</span><br><span class="line">  max_length=<span class="number">15</span>, <span class="comment">#the max length of the text</span></span><br><span class="line">  num_return_sequence=<span class="number">3</span> <span class="comment">#How many texts are gonna generated</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="Mask-filling"><a href="#Mask-filling" class="headerlink" title="Mask filling"></a>Mask filling</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">unmasker=pipeline(<span class="string">"fill-mask"</span>)</span><br><span class="line">unmasker(<span class="string">"I play &lt;mask&gt; Impact!"</span>,top_k=<span class="number">2</span>) <span class="comment">#top_k decides the time it does;&lt;mask&gt; depends on what model you are using</span></span><br></pre></td></tr></table></figure><h2 id="Named-entity-recognition"><a href="#Named-entity-recognition" class="headerlink" title="Named entity recognition"></a>Named entity recognition</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ner=pipeline(<span class="string">"ner"</span>,grouped_entities=<span class="literal">True</span>)<span class="comment">#'grouped_entities=True' is used to enable the model to put multi-words together</span></span><br></pre></td></tr></table></figure><h2 id="Question-answering"><a href="#Question-answering" class="headerlink" title="Question answering"></a>Question answering</h2><ul><li>answer a question using given context</li><li>extracting answers from the context instead of generating answers</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">question_answerer=pipeline(<span class="string">"question-answering"</span>)</span><br><span class="line">question_answerer(</span><br><span class="line">  question=<span class="string">"where do I work?"</span>,</span><br><span class="line">  context=<span class="string">"My name is Sylvain and I work at Hugging Face in Brooklyn"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="Summarization"><a href="#Summarization" class="headerlink" title="Summarization"></a>Summarization</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">summarizer=pipeline(<span class="string">"summarization"</span>)</span><br><span class="line">summarizer(</span><br><span class="line">   <span class="string">"""</span></span><br><span class="line"><span class="string">context</span></span><br><span class="line"><span class="string">   """</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="Translation"><a href="#Translation" class="headerlink" title="Translation"></a>Translation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">translator=pipeline(<span class="string">"translation"</span>,model=<span class="string">"modelName"</span>)</span><br><span class="line">translator(<span class="string">"contex"</span>)</span><br></pre></td></tr></table></figure><h1 id="Brief-intro-to-Transformers"><a href="#Brief-intro-to-Transformers" class="headerlink" title="Brief intro to Transformers"></a>Brief intro to Transformers</h1><h2 id="General-categorization"><a href="#General-categorization" class="headerlink" title="General categorization"></a>General categorization</h2><ul><li>GPT-like: auto-regressive</li><li>BERT-like: auto-encoding</li><li>BART/T5-like: sequence-to-sequence</li></ul><h2 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h2><h3 id="Pretraning"><a href="#Pretraning" class="headerlink" title="Pretraning"></a>Pretraning</h3><ul><li>the act of traing a model from scratch<br><img src="https://github.com/daucloud/imagecdn/raw/main/test/202404251741650.png" alt=""></li></ul><h3 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a>Fine-tuning</h3><ul><li>training on the top of pretrained models with a dataset specific to the target task<br><img src="https://github.com/daucloud/imagecdn/raw/main/test/202404251746195.png" alt="image.png"></li></ul><h2 id="General-architecture"><a href="#General-architecture" class="headerlink" title="General architecture"></a>General architecture</h2><ul><li>Transformers model is generally composed of two blocks:<ul><li>Encoder: receives inputs and builds representations for them</li><li>Decoder: use the outputs of encoder to generate target outputs</li></ul></li><li>Various tasks requires different blocks:<ul><li>Encoder-only: sentence classfication; NER</li><li>Decoder-only: text generation</li><li>Encoder-decoder models or sequence-to-sequence models: translation or summarization</li></ul></li></ul><h2 id="Attention-layer"><a href="#Attention-layer" class="headerlink" title="Attention layer"></a>Attention layer</h2><ul><li>pay specific attention to certain words while ignoring others more or less</li></ul><h2 id="Original-Architechture"><a href="#Original-Architechture" class="headerlink" title="Original Architechture"></a>Original Architechture</h2><ul><li>the encoder translate all the words</li><li>the decoder is only allowed to translate by the past words; but later it can get all the outpus of encoer to better translate the word<br><img src="https://github.com/daucloud/imagecdn/raw/main/test/202404271755969.png" alt="image.png"></li></ul>]]></content>
      
      
      <categories>
          
          <category> CS Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> ğŸ¤— </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>æ¼«æ­¥æ‚è®°</title>
      <link href="/2024/03/17/%E6%BC%AB%E6%AD%A5%E6%9D%82%E8%AE%B0/"/>
      <url>/2024/03/17/%E6%BC%AB%E6%AD%A5%E6%9D%82%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>å–œæ¬¢åœ¨å°‘äººçš„å¤œï¼Œä¸€ä¸ªäººæ¼«æ— ç›®çš„åœ°åœ¨æ ¡å›­é‡Œèµ°èµ°åœåœã€‚ç˜«åœ¨å®¿èˆæ‰“äº†ä¸€ä¸‹åˆçš„æ¸¸æˆï¼Œå¤´è„‘æ˜æ˜æ²‰æ²‰ã€‚äºæ˜¯æ·ä¸‹æ‰‹æœºï¼ŒæŠ„èµ·å¤–å¥—ï¼Œæ‹©å®¿èˆæ¥¼è¥¿è¾¹çš„ä¸€æ¡åƒ»å¹½å°å¾„ï¼Œå¼€å§‹åœ¨å…¥å¤œçš„å›­å­é‡Œéšæ„æ¸¸è¡ã€‚æ­£å€¼å†¬æ˜¥ä¹‹äº¤ï¼ŒåŒ—äº¬çš„é£å·²ç„¶æ”¶èµ·äº†å†¬æ—¥ç¡Œäººçš„æ£±è§’ï¼Œä½ä»¥å¤œçš„äº›è®¸æ¸…å†·çš„å¯’æ„ï¼Œè¿é¢å¹æ¥ï¼Œä»¤äººæ„Ÿåˆ°èˆ’é€‚åˆæ¸…é†’ã€‚äº«å—ç€é£çš„æ„Ÿè§‰ï¼Œå¿ƒçµé€æ¸ä»æ—¥å¸¸ç”Ÿæ´»çš„ç¹çå’Œä¸–ä¿—æ„ä¹‰çš„å‹åŠ›ä¸­æŠ½ç¦»ï¼ŒæŸ”è½¯çš„è§¦è§’ä¸çŸ¥ä¸è§‰å‘å››å‘¨å»¶ä¼¸å¼€æ¥ï¼šè½å¶éšé£ç§»åŠ¨æ—¶æ‘©æ“¦åœ°é¢çš„ç°Œç°Œå£°ã€æ ¡æ²³åå°„è·¯ç¯çš„ç²¼ç²¼æ³¢å…‰ã€é“æ—å®¿èˆæ¥¼ä¸­é€šäº®çš„ç¯ã€é»‘å¤œä¸­è½»æŸ”æ‘‡åŠ¨çš„æŸ³æ¡ã€å¥‡å½¢æ€ªçŠ¶çš„å…‰ç§ƒæä¸«ã€å«è‹å¾…æ”¾çš„ç²‰è‰²è““è•¾â€¦â€¦å¿ƒçµä»¿ä½›è¤ªå»äº†è€æˆçš„å¤–å£³ï¼Œå¯¹å¸ç©ºè§æƒ¯çš„ä¸€åˆ‡é‡åˆå…´èµ·äº†å­©ç«¥èˆ¬çš„å¥½å¥‡ï¼Œç›®ä¹‹æ‰€åŠã€è€³ä¹‹æ‰€é—»éƒ½æ˜¯å†æ–°å¥‡ä¸è¿‡çš„äº‹ç‰©ã€‚</p><p>è‚†æ„çš„å¤–ç•Œä½“éªŒå”¤é†’äº†ä¸°ç›ˆçš„å†…åœ¨æ„Ÿå—ï¼Œéª¨é«“æ·±å¤„çš„å­¤ç‹¬æ„Ÿä¸€ç‚¹ç‚¹æ¸—äº†å‡ºæ¥ï¼Œæ‚„ç„¶æ¼«ä¸Šäº†å¿ƒå¤´ã€‚æˆ‘æ˜¯ä¸€ä¸ªå­¤ç‹¬çš„äººï¼Œå†…å¿ƒæ·±å¤„ï¼Œå§‹ç»ˆæœ‰ä¸€éƒ¨åˆ†æ˜¯æ— æ³•å¯¹äººæ•å¼€ã€ç”šè‡³æˆ‘è‡ªå·±ä¹Ÿä¸ç”šäº†ç„¶çš„ã€‚å¾ˆå¤šæ—¶å€™ï¼Œæˆ‘å¯¹è¿™ç§å­¤ç‹¬æ„Ÿæ˜¯å¼‚å¸¸ææƒ§çš„ï¼šåœ¨æŸäº›éš¾çœ çš„æ·±å¤œï¼Œæˆ‘ä¼šè¢«æ½®æ°´èˆ¬æ¶Œå‡ºçš„å­¤ç‹¬çª’å¡ä½å‘¼å¸ã€‚æˆ‘æ¸´æœ›æœ‰ä¸€åçœŸæ­£çš„çµé­‚çŸ¥å·±èƒ½å¤Ÿåˆ†äº«ä¸€åˆ‡ã€å€¾åæ‰€æœ‰ã€‚å¯æˆ–æ˜¯ä¸æ•¢ï¼Œæˆ–æ˜¯æ— ç¼˜ï¼Œç»ˆç©¶æ²¡èƒ½æ‰¾åˆ°ã€‚ä½†æ˜¯ï¼Œåœ¨è¿™ä¸ªéšæ„æ¼«æ­¥çš„æ™šä¸Šï¼Œæˆ‘å´é€æ¸å­¦ä¼šäº†äº«å—å­¤ç‹¬ã€‚ç‹¬è‡ªä¸€äººï¼Œå¯ä»¥éšå¿ƒæ‰€æ¬²åœ°åœæ­¥å’Œèµ·æ­¥ã€éšå¿ƒæ‰€æ¬²åœ°è¹²ä¸‹æ¥ç«¯è¯¦é“æ—çš„æ¯è‰ã€éšå¿ƒæ‰€æ¬²åœ°æ‹¾èµ·è„šè¾¹çš„ææ¡ã€‚åŸæ¥å­¤ç‹¬å¹¶ä¸æ˜¯ä»€ä¹ˆå¯æ€•çš„äº‹æƒ…ã€‚æ¯ä¸ªäººç”Ÿæ¥å­¤ç‹¬ï¼Œè¿™æ˜¯ä¸€ä¸ªäººèº«ä¸ºä»–è‡ªå·±è€Œä¸æ˜¯å…¶ä»–çš„ä»€ä¹ˆäººæ‰€å¿…ç„¶å¸¦æ¥çš„ç‹¬ä¸€æ— äºŒçš„ä½“éªŒã€‚æˆ‘å½“ç„¶ä»æ¸´æœ›è§…åˆ°ä¸€åçŸ¥å·±ï¼Œä½†æˆ‘å·²å­¦ä¼šæ‹¥æŠ±è¿™ä»½å­¤ç‹¬ã€‚</p><p>è¸±æ­¥åˆ°å¤§ç¤¼å ‚åŒ—è¾¹çš„æ²³é“æ—¶ï¼Œçªç„¶æœ‰ä¸€é˜µæçŒ›çƒˆçš„é£é¡ºç€æˆ‘å‰è¿›çš„æ–¹å‘å¹æ¥ã€‚æˆ‘ç«‹ä½ç‰‡åˆ»ï¼Œæ„Ÿå—ç€é£åœ¨è€³è¾¹çš„å‘¼å•¸å£°å’Œå¼ºé£ä¸‹ç´§è´´èº«ä½“çš„è¡£ç‰©ã€‚éšåæˆ‘è½¬è¿‡èº«ï¼Œå¼ å¼€åŒè‡‚ï¼Œé—­ç›®ä½“ä¼šå¯’å†·çš„æ°”æµå†²å‡»é¢é¢Šæ—¶çš„å¾®ç—›ã€‚æˆ‘é€†é£èµ°äº†å‡ æ­¥ï¼Œéšååˆè½¬è¿‡èº«å»ï¼Œé¡ºé£èµ°äº†èµ·æ¥ã€‚æˆ‘è¶Šèµ°è¶Šå¿«ï¼Œè¶Šèµ°è¶Šè·³è·ƒï¼Œè¶Šèµ°è¶Šè½»ç›ˆã€‚æˆ‘å¿ä¸ä½ç¬‘äº†èµ·æ¥ï¼Œä¸€å¼€å§‹æ˜¯å¾®ç¬‘ï¼Œåæ¥æ˜¯å’§å˜´ç¬‘ï¼Œæœ€åç«Ÿæ¼”å˜æˆäº†å“ˆå“ˆå¤§ç¬‘ã€‚æˆ‘ä¸€è¾¹ç¬‘ï¼Œä¸€è¾¹å¤§å£°è¯´ï¼šâ€œæˆ‘è¿˜æ´»ç€ï¼â€åæ¥é£åœäº†ï¼Œæˆ‘è¿˜åœ¨å¤§å£°ç¬‘ç€ã€‚å‘¨å›´æ²¡ä»€ä¹ˆäººï¼Œè§è¯è€…åªæœ‰æ²³æ°´ã€æŸ³æ ‘å’Œç¤¼å ‚ã€‚</p><p>å›å®¿èˆæ—¶ï¼Œæˆ‘èµ°çš„æ˜¯å­¦å ‚è·¯ã€‚å­¦å ‚è·¯äººæµä¸å°ï¼Œèµ°åœ¨é“è·¯çš„è¾¹ç¼˜ï¼Œæ—¶æ—¶æœ‰è‡ªè¡Œè½¦ä»èº«æ—é©°è¿‡ã€‚æˆ‘é¥¶æœ‰å…´è‡´åœ°çœ‹ç€å„å¼å„æ ·çš„è‡ªè¡Œè½¦è¿œå»çš„èº«å½±ï¼Œå¬ç€å¶å°”ä¼ æ¥çš„ä¸€ä¸¤å¥ç©ç¬‘å’Œè°ˆè¯å£°ï¼Œå‘æ•£çš„æ€ç»ªé‡åˆå›åˆ°äº†æ—¥å¸¸ç”Ÿæ´»ï¼Œåªæ˜¯å¤šä¸Šäº†å‡ åˆ†é‡Šç„¶ã€å¦è¡å’Œå®é™ã€‚æ¼«æ­¥çš„è¶£å‘³ï¼Œå½“çœŸè¨€ä¹‹ä¸å°½ã€‚</p><blockquote><p>æ³¨ï¼šèƒŒæ™¯å›¾ç‰‡æ¥è‡ª<a href="https://mapio.net/pic/p-3220490/" target="_blank" rel="noopener">https://mapio.net/pic/p-3220490/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> æ‚ä¸ƒæ‚å…«çš„ä¸œè¥¿ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> éšç¬” </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>å¯ä»¥åˆ¤æ–­ç´ æ•°çš„æ­£åˆ™è¡¨è¾¾å¼</title>
      <link href="/2024/02/01/%E5%8F%AF%E4%BB%A5%E5%8C%B9%E9%85%8D%E5%90%88%E6%95%B0%E7%9A%84%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
      <url>/2024/02/01/%E5%8F%AF%E4%BB%A5%E5%8C%B9%E9%85%8D%E5%90%88%E6%95%B0%E7%9A%84%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>æœ€è¿‘è¯»åˆ°ä¸€ä¸ªå¯ä»¥åˆ¤æ–­ç´ æ•°çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ˆåŒ¹é…æˆåŠŸåˆ™ä¸æ˜¯ç´ æ•°ï¼‰ï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;^1?$|^(11+?)\1+$&#x2F;</span><br></pre></td></tr></table></figure><br>è¿™ä¹ˆç²¾ç‚¼ï¼Œé¢‡æœ‰äº›å‡ºä¹æˆ‘çš„æ„æ–™ã€‚å› ä¸ºåœ¨æˆ‘çš„å°è±¡ä¸­å¤§å¤šæ•°æ­£åˆ™è¡¨è¾¾å¼éƒ½ååˆ†ä¸‘é™‹ï¼Œæ¯”å¦‚åŒ¹é…é‚®ç®±çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(?:[a-z0-9!#$%&amp;&#39;*+&#x2F;&#x3D;?^_&#96;&#123;|&#125;~-]+(?:\.[a-z0-9!#$%&amp;&#39;*+&#x2F;&#x3D;?^_&#96;&#123;|&#125;~-]+)*|&quot;(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*&quot;)@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.)&#123;3&#125;(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])</span><br></pre></td></tr></table></figure></p><p>ç„¶è€Œï¼Œä»”ç»†é˜…è¯»äº†ä¸€ä¸‹åŸç†åå´å‘ç°å®ƒçš„ç¡®å¯ä»¥ï¼Œä¸è¿‡éœ€è¦ä¸€æ­¥é¢„å¤„ç†ï¼Œå³æŠŠåè¿›åˆ¶æ•°å­—è½¬æ¢ä¸ºâ€œ1çš„æ•°ç»„â€ï¼š0ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œ1ä¸º1ï¼Œ2ä¸º11ï¼Œ3ä¸º111â€¦â€¦<br>è‡³æ­¤ï¼Œå°±å¯ä»¥å¼€å§‹æˆ‘ä»¬çš„åŒ¹é…ï¼š</p><ul><li><code>/^1?$/</code>å¾ˆå¥½ç†è§£ï¼ŒåŒ¹é…åˆ°æ˜¯ç©ºå­—ç¬¦ä¸²ï¼ˆ0ï¼‰æˆ– 1ï¼Œè‡ªç„¶ä¸æ˜¯åˆæ•°ï¼›</li><li><code>/^(11+?)\1+$/</code>åˆ™æ­£æ˜¯è¯¥è¡¨è¾¾å¼çš„ç²¾å¦™æ‰€åœ¨ã€‚å…¶åˆ©ç”¨åˆ°äº†æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ—¶<strong>å›æº¯</strong>çš„ç‰¹æ€§ã€‚è®©æˆ‘ä»¬å…·ä½“è§£é‡Šä¸€ä¸‹ï¼š</li></ul><ol><li><code>(11+?)</code>åŒ¹é…çš„æ˜¯<strong>è‡³å°‘ä¸¤ä¸ª1</strong>ï¼Œä½†æ˜¯å› ä¸ºè¿›è¡Œçš„æ˜¯æ‡’æƒ°åŒ¹é…ï¼Œæ‰€ä»¥æœ€å¼€å§‹åŒ¹é…åˆ°çš„æ˜¯<code>11</code>ï¼Œå¹¶è¢«æ•è·åˆ°äº†åˆ†ç»„<code>\1</code>ä¸­ã€‚åé¢éƒ¨åˆ†ä¸­ï¼Œå¦‚æœ<code>11</code>é‡å¤äº†ä¸€æ¬¡æˆ–è€…æ›´å¤šæ¬¡ï¼Œé‚£ä¹ˆå°±æ˜¯åˆæ•°ã€‚è¿™åˆæ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå…¶å®éå¸¸å®¹æ˜“ç†è§£ï¼šå¦‚æœæ•°å­—$a$åŒ¹é…æˆåŠŸï¼Œæ„å‘³ç€$a$åŒ–ä½œçš„â€œ1çš„æ•°ç»„â€<strong>æ°å¥½</strong>ç”±<strong>ä¸å°‘äº</strong>2ä¸ª<code>11</code>ç»„æˆï¼Œä¹Ÿå°±æ˜¯ï¼Œ$\exists b&gt;1\land b \in \mathbb N, a=2b$ï¼Œè¿™è‡ªç„¶æ„å‘³ç€$a$æ˜¯ä¸€ä¸ªåˆæ•°(åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ$a$è¿˜æ˜¯å¶æ•°)ã€‚</li><li>å¦‚æœ<code>11</code>åŒ¹é…å¤±è´¥äº†å‘¢ï¼Ÿè¿™å°±æ˜¯æœ¬æ³•æœ€æ ¸å¿ƒçš„éƒ¨åˆ†äº†ï¼šåŒ¹é…å™¨ä¼šè¿›è¡Œå›æº¯ï¼Œå¯¹ä¸‹ä¸€ä¸ªæ»¡è¶³<code>(11+?)</code>çš„<code>111</code>è¿›è¡Œ<code>\1+</code>åŒ¹é…çš„å°è¯•ã€‚åŒç†ä¸Šä¸€æ­¥ï¼Œå¦‚æœåŒ¹é…æˆåŠŸï¼Œè¯¥æ•°å­—å¤§äº3ä¸”å«æœ‰ä¸€ä¸ªå› æ•°3ï¼Œè‡ªç„¶æ˜¯åˆæ•°ã€‚</li><li>æ¥ä¸‹æ¥ï¼Œâ€œåŒ¹é… - å›æº¯â€ä¸æ–­è¿›è¡Œã€‚å¦‚æœ$n$æ˜¯åˆæ•°ï¼Œä¼šåœ¨åŒ¹é…ä¸è¶…è¿‡$\sqrt{n}$æ¬¡åæˆåŠŸï¼›åä¹‹ï¼Œä¼šä¸€ç›´åŒ¹é…åˆ°ç¬¬$n$æ¬¡ï¼Œæœ€ååŒ¹é…å¤±è´¥ã€‚</li></ol><p>å¯è§ï¼Œ<code>/^1?$|^(11+?)\1+$/</code>çš„å®ç°æƒ³æ³•æ˜¯éå¸¸æœ´ç´ çš„ï¼šåˆ—ä¸¾æ¯”$n$å°çš„æ‰€æœ‰æ­£æ•´æ•°$i$ï¼Œåˆ¤æ–­$n$æ˜¯å¦å¯ä»¥è¢«$i$æ•´é™¤ã€‚æ¯”å¦‚ï¼š<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//åˆ¤æ–­ä¸€ä¸ªè‡ªç„¶æ•°æ˜¯å¦ä¸ºç´ æ•°</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isPrime</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">Â  Â  <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>)<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">Â  Â  <span class="keyword">if</span> (n == <span class="number">2</span>)<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">Â  Â  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= <span class="built_in">sqrt</span>(n); i++)<span class="keyword">if</span> (n % i == <span class="number">0</span>)<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">Â  Â  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>ä½†æ˜¯ï¼Œå¾—åˆ°è¿™ä¹ˆç²¾ç‚¼çš„è¡¨è¾¾å¼çš„å…³é”®å®åˆ™åœ¨äºå¯¹äºæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æœºåˆ¶çš„æ·±åˆ»ç†è§£(æ‡’æƒ°åŒ¹é…å’Œå›æº¯æ³•çš„ç»“åˆ)ï¼Œè€Œè¿™æ­£æ˜¯å€¼å¾—æˆ‘ä»¬æ·±æ€å’Œå­¦ä¹ çš„åœ°æ–¹ã€‚</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ</title>
      <link href="/2024/01/18/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
      <url>/2024/01/18/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ"><a href="#ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ" class="headerlink" title="ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ"></a>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ</h1><p><del>æœ¬æ–‡æ˜¯æ•°æ®ç§‘å­¦å¯¼è®ºæœŸæœ«å¤§ä½œä¸šå±•ç¤ºçš„æŠ¥å‘Šï¼Œæ‹¿æ¥éšä¾¿æ°´ä¸€ç¯‡åšå®¢hh</del></p><h2 id="0-å¼•è¨€"><a href="#0-å¼•è¨€" class="headerlink" title="0    å¼•è¨€"></a>0    å¼•è¨€</h2><p>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (Generative Adversarial Network, GAN) æ˜¯ä¸€ç§ååˆ†æµè¡Œçš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚è‡ª2014å¹´Ian Goodfellowç­‰äººé¦–æ¬¡æå‡ºä»¥æ¥ï¼ŒGANè¿…é€Ÿåœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œå¼•å‘äº†çƒ­çƒˆçš„åå“ï¼Œè®¸å¤šæœ‰å½±å“åŠ›çš„å·¥ä½œå±‚å‡ºä¸ç©·ã€‚å›¾ä¸€æ˜¯ç´¯ç§¯çš„GANè®ºæ–‡æ•°é‡ï¼Œå…¶ç«çƒ­ç¨‹åº¦å¯è§ä¸€æ–‘ã€‚</p><p><img src="https://github.com/daucloud/imagecdn/raw/main/test/202401181914378.png" alt=""></p><center>Figure 1: Cumulative number of GAN papers</center><p>æœ¬æ–‡å°†å¯¹GANåšç®€è¦ä»‹ç»ï¼Œä¸»è¦åŒ…æ‹¬GANåŸå§‹æ¨¡å‹ã€ç»å…¸å˜ä½“å’Œåº”ç”¨æˆå°±ã€‚</p><h2 id="1-åŸå§‹æ¨¡å‹"><a href="#1-åŸå§‹æ¨¡å‹" class="headerlink" title="1    åŸå§‹æ¨¡å‹"></a>1    åŸå§‹æ¨¡å‹</h2><h3 id="1-1-åŸºæœ¬æ€æƒ³"><a href="#1-1-åŸºæœ¬æ€æƒ³" class="headerlink" title="1.1    åŸºæœ¬æ€æƒ³"></a>1.1    åŸºæœ¬æ€æƒ³</h3><p>GANå…¨ç§°ä¸ºç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚é¡¾åæ€ä¹‰ï¼Œå…¶åœ¨æœ¬è´¨ä¸Šæ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒæœ€çªå‡ºçš„ç‰¹ç‚¹æ˜¯ä½¿ç”¨åˆ¤åˆ«å™¨ä¸ç”Ÿæˆå™¨è¿›è¡Œå¯¹æŠ—å¼è®­ç»ƒï¼Œåè€…è´Ÿè´£äº§ç”Ÿè´´è¿‘çœŸå®çš„æ•°æ®ï¼Œå‰è€…åˆ™è´Ÿè´£åŠªåŠ›è¾¨åˆ«ç”Ÿæˆæ•°æ®å’ŒçœŸå®æ•°æ®ã€‚å½“è®­ç»ƒè¿­ä»£è‡³ä¸€å®šæ¬¡æ•°åï¼Œç”Ÿæˆå™¨äº§ç”Ÿçš„æ•°æ®ä¾¿è¶³å¤Ÿé€¼çœŸï¼Œè®­ç»ƒç›®çš„å› è€Œå¾—åˆ°å®ç°ã€‚</p><p>å¯¹æ­¤ï¼ŒGANçš„åŸå§‹è®ºæ–‡<sup><a href="#fn_1" id="reffn_1">1</a></sup>ä¸­ç»™å‡ºäº†ä¸€ä¸ªé€šä¿—çš„è§£é‡Šï¼š</p><blockquote><p>ç”Ÿæˆå™¨æ˜¯â€œå‡é’åˆ¶é€ å›¢ä¼™â€ï¼Œè´Ÿè´£åˆ¶é€ èƒ½åœ¨å¸‚åœºä¸Šæµé€šçš„å‡é’ï¼›åˆ¤åˆ«å™¨åˆ™æ˜¯â€œè­¦å¯Ÿâ€ï¼Œè´Ÿè´£è¯†ç ´å‡é’ã€‚å‡é’åˆ¶é€ å›¢ä¼™å’Œè­¦å¯Ÿä¹‹é—´ä¸æ–­ç«äº‰ï¼Œè­¦å¯Ÿçš„é‰´åˆ«èƒ½åŠ›å’Œå›¢ä¼™çš„é€ å‡èƒ½åŠ›éƒ½ä¸æ–­æå‡ï¼Œæœ€ç»ˆï¼Œé€ å‡å›¢ä¼™åˆ¶é€ çš„å‡é’è¶³ä»¥ä»¥å‡ä¹±çœŸï¼šâ€œå‡é’å˜æˆäº†çœŸé’â€ã€‚</p></blockquote><p><strong>GANå°±æ˜¯åœ¨åˆ¶é€ è¶³ä»¥ä»¥å‡ä¹±çœŸçš„å‡é’ã€‚</strong></p><h3 id="1-2-åŸºæœ¬æ­¥éª¤"><a href="#1-2-åŸºæœ¬æ­¥éª¤" class="headerlink" title="1.2    åŸºæœ¬æ­¥éª¤"></a>1.2    åŸºæœ¬æ­¥éª¤</h3><p>æ¦‚æ‹¬æ¥è¯´ï¼ŒGANçš„è®­ç»ƒä¸»è¦åˆ†ä¸ºä»¥ä¸‹ä¸‰æ­¥ï¼š</p><ol><li><strong>å›ºå®šç”Ÿæˆå™¨$G$ï¼Œè®­ç»ƒåˆ¤åˆ«å™¨$D$ï¼š</strong> ä½¿ $D$ å°½å¯èƒ½å‡†ç¡®åœ°è¯†åˆ«å‡ºæ ·æœ¬ç©¶ç«Ÿæ˜¯ç”±ç”Ÿæˆå™¨ç”Ÿæˆçš„ï¼ˆæ¥è‡ª <script type="math/tex">p_z(z)</script>ï¼‰è¿˜æ˜¯æ¥è‡ªçœŸå®æ•°æ®é›†ï¼ˆæ¥è‡ª <script type="math/tex">p_{data}(x)</script>ï¼‰</li><li><strong>å›ºå®šåˆ¤åˆ«å™¨$D$ï¼Œè®­ç»ƒç”Ÿæˆå™¨$G$ï¼š</strong> ä½¿ $G$ ç”Ÿæˆçš„æ ·æœ¬($G(z)$)å°½å¯èƒ½è´´è¿‘çœŸå®æ ·æœ¬ï¼Œå³è®©ç”Ÿæˆå™¨$G$éª—è¿‡åˆ¤åˆ«å™¨$D$</li><li><strong>è¿­ä»£ï¼š</strong> å¾ªç¯1. 2.è‡³ä¸€å®šæ¬¡æ•°ï¼Œæ­¤æ—¶ç”Ÿæˆå™¨$G$äº§ç”Ÿçš„æ ·æœ¬è¶³ä»¥â€œä»¥å‡ä¹±çœŸâ€ï¼Œåˆ¤åˆ«å™¨$D$è¾¨è®¤çœŸå®æ ·æœ¬å’Œç”Ÿæˆæ ·æœ¬æˆåŠŸçš„æ¦‚ç‡éƒ½ä¸º$\frac1 2$ï¼ŒäºŒè€…è¾¾åˆ°äº†çº³ä»€å¹³è¡¡<sup><a href="#fn_2" id="reffn_2">2</a></sup>ã€‚</li></ol><blockquote><p>è®­ç»ƒç¤ºæ„å›¾ï¼š<img src="https://github.com/daucloud/imagecdn/raw/main/test/202401181914603.png" alt=""></p><center>Figure 2: è®­ç»ƒè¿‡ç¨‹ç¤ºæ„å›¾</center><p>å›¾ä¸­ï¼Œ$z$è½´æ˜¯è¾“å…¥ç”Ÿæˆå™¨çš„éšæœºå™ªå£°ï¼Œå…¶é€šè¿‡ç”Ÿæˆå™¨(ç®­å¤´)æ˜ å°„åˆ°æ ·æœ¬é›†$x$è½´ã€‚é»‘è‰²æ•£ç‚¹æ˜¯çœŸå®æ•°æ®é›†ï¼›ç»¿è‰²å®çº¿æ˜¯ç”Ÿæˆæ ·æœ¬é›†ï¼Œè“è‰²è™šçº¿æ˜¯åˆ¤åˆ«å™¨ï¼ˆè¶Šè¿œç¦»$x$è½´ï¼Œå…¶å€¼è¶Šæ¥è¿‘äº1ï¼Œå³å…¶è®¤ä¸ºè¯¥æ ·æœ¬æ¥è‡ªçœŸå®æ•°æ®é›†çš„æ¦‚ç‡è¶Šå¤§ï¼‰ã€‚</p><p>(a)ä¸­ï¼Œåˆ¤åˆ«å™¨æ›²çº¿(è“è‰²è™šçº¿)è¾ƒä¸ºé¢ ç°¸ï¼Œåˆ¤åˆ«æ•ˆæœè¾ƒå·®ï¼Œå› æ­¤(a)åˆ°(b)é¦–å…ˆå¯¹åˆ¤åˆ«å™¨è¿›è¡Œè®­ç»ƒï¼›(b)åˆ°(c)åˆ™å¯¹ç”Ÿæˆå™¨è¿›è¡Œè®­ç»ƒï¼Œä½¿å¾—ç”Ÿæˆæ ·æœ¬æ›²çº¿(ç»¿è‰²å®çº¿)æ›´è´´è¿‘çœŸå®æ ·æœ¬é›†(é»‘è‰²æ•£ç‚¹)ã€‚å½“è¿­ä»£æ­¤äºŒæ­¥åˆ°ä¸€å®šæ¬¡æ•°åï¼Œç»¿è‰²å®çº¿å’Œé»‘è‰²æ•£ç‚¹æ¥è¿‘é‡åˆï¼Œåˆ¤åˆ«å™¨æ›²çº¿ä¹Ÿæ’ä¸º$1 \over 2$ï¼Œå³æ— æ³•å°†ç”Ÿæˆæ ·æœ¬å’ŒçœŸå®æ•°æ®è¿›è¡ŒåŒºåˆ†ã€‚</p></blockquote><h3 id="1-3-å…·ä½“ç®—æ³•"><a href="#1-3-å…·ä½“ç®—æ³•" class="headerlink" title="1.3    å…·ä½“ç®—æ³•"></a>1.3    å…·ä½“ç®—æ³•</h3><p><strong>è®°å·è¯´æ˜</strong></p><ul><li>ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨å‡ä¸ºå¤šå±‚ç¥ç»ç½‘ç»œï¼š$G(z;\theta _g)$å’Œ$D(x;\theta _d)$</li><li>$G$é€šè¿‡å‚æ•°$\theta_g$å°†å™ªå£°$z$æ˜ å°„ä¸ºç”Ÿæˆæ ·æœ¬$G(z;\theta _g)$</li><li>$D$é€šè¿‡å‚æ•°$\theta_d$å°†æ ·æœ¬æ˜ å°„ä¸º$[0,1]$çš„æ ‡é‡$D(x;\theta _d)$ï¼Œå…¶å€¼è¶Šæ¥è¿‘1è¡¨ç¤ºä¸ºçœŸå®æ•°æ®çš„æ¦‚ç‡è¶Šå¤§</li><li>ç›®æ ‡ï¼Œæ±‚è§£å€¼å‡½æ•°$V(D,G)$çš„æå°æå¤§å€¼ï¼Œå³ï¼š</li></ul><script type="math/tex; mode=display">\min_G\max_D V(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[\log D(x)]+\mathbb{E}_{z\sim p_z(z)}[\log(1-D(G(z)))]</script><p><strong>å…·ä½“æ±‚è§£æ­¥éª¤</strong></p><p>for 1 to n do<br>&emsp;&emsp;for 1 to k do<br>&emsp;&emsp;&emsp;&emsp;ä»å™ªå£°é›†å–å‡ºmä¸ªæ ·æœ¬${z^{(1)},z^{(2)},â€¦,z^{(m)}}$;<br>&emsp;&emsp;&emsp;&emsp;ä»çœŸå®æ•°æ®é›†ä¸­å–å‡ºmä¸ªæ ·æœ¬${x^{(1)},x^{(2)},â€¦,x^{(m)}}$;<br>&emsp;&emsp;&emsp;&emsp;ä½¿ç”¨æ¢¯åº¦ä¸Šå‡æ³•<sup><a href="#fn_3" id="reffn_3">3</a></sup>æ›´æ–°åˆ¤åˆ«å™¨çš„å‚æ•°$\theta_d$ï¼š</p><script type="math/tex; mode=display">\nabla_{\theta_d}\frac{1}{m}\sum_{i=1}^{m}[\log D(x^{(i)})+\log D(1-D(G(z^{(i)})))].</script><p>&emsp;&emsp;end for<br>&emsp;&emsp;ä»å™ªå£°é›†ä¸­å–å‡ºmä¸ªæ ·æœ¬${z^{(1)},z^{(2)},â€¦,z^{(m)}}$;<br>&emsp;&emsp;ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°ç”Ÿæˆå™¨çš„å‚æ•°$\theta_g$</p><script type="math/tex; mode=display">\nabla_{\theta_d}\frac{1}{m}\sum_{i=1}^{m}[\log (1-D(G(z^{(i)})))].</script><p>end for</p><blockquote><p>kæ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼›ä¹‹æ‰€ä»¥æ›´æ–°kæ¬¡$D$æ‰æ›´æ–°1æ¬¡$G$ï¼Œæ˜¯å› ä¸ºåªæœ‰åˆ¤åˆ«å™¨è¶³å¤Ÿå¥½ï¼Œç”Ÿæˆå™¨çš„æ›´æ–°æ‰å‡†ç¡®æœ‰æ•ˆã€‚</p></blockquote><h2 id="2-ç»å…¸å˜ä½“"><a href="#2-ç»å…¸å˜ä½“" class="headerlink" title="2    ç»å…¸å˜ä½“"></a>2    ç»å…¸å˜ä½“</h2><p>GANä½œä¸ºé£é¡ä¸€æ—¶çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œè‡ªç„¶å°‘ä¸äº†å„ç§æ”¹è¿›å’Œå˜ä½“ã€‚æœ¬éƒ¨åˆ†é€‰å–äº†ä¸‰ç§è¾ƒæœ‰ä»£è¡¨æ€§å’Œå½±å“åŠ›çš„å˜ä½“è¿›è¡Œä»‹ç»ï¼šCGANã€DCGANå’ŒWGANã€‚</p><h3 id="2-1-CGAN4"><a href="#2-1-CGAN4" class="headerlink" title="2.1    CGAN4"></a>2.1    CGAN<sup><a href="#fn_4" id="reffn_4">4</a></sup></h3><p>åŸå§‹GANæ¨¡å‹ä¸­ï¼Œè¾“å…¥ç”Ÿæˆå™¨çš„æ˜¯éšæœºå™ªå£°ï¼Œå› æ­¤æœ€åäº§ç”Ÿçš„å›¾åƒçš„éšæœºæ€§ä¹Ÿç›¸å¯¹è¾ƒå¤§ã€‚å€˜è‹¥æˆ‘ä»¬æƒ³è¦è·å¾—å…·æœ‰æŸç§ç‰¹å¾å’Œæ ‡ç­¾çš„å›¾åƒï¼Œå°±éœ€è¦ä»ç”Ÿæˆçš„ä¸€å¤§å †æ ·æœ¬ä¸­è¿›è¡ŒæŒ‘é€‰ï¼Œè¾ƒä¸ºç¹çã€‚</p><p>CGAN(Conditional Generative Adversarial Networks)ä¾¿æ˜¯ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜è€Œäº§ç”Ÿçš„ã€‚å…¶æ€æƒ³ä¹Ÿååˆ†æœ´ç´ ï¼Œå³å‘ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„è¾“å…¥ä¸­æ·»åŠ æ¡ä»¶ä¿¡æ¯ã€‚è¿™æ ·ä¸€æ¥ï¼Œç”Ÿæˆæ ·æœ¬ä¸ä»…éœ€è¦è¶³å¤Ÿé€¼çœŸï¼Œè¿˜è¦æ»¡è¶³ç‰¹å®šçš„æ¡ä»¶æ‰èƒ½é€šè¿‡åˆ¤åˆ«å™¨ï¼š</p><script type="math/tex; mode=display">\min_G\max_D V(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[\log D(x|y)]+\mathbb{E}_{z\sim p_z(z)}[\log(1-D(G(z|y)))]</script><p><img src="https://aovoc.github.io/assets/pics/cgan-arch2.PNG" alt="Cgan,icgan"></p><center>Figure 3: CGANå’ŒGANçš„å¯¹æ¯”</center><h3 id="2-2-DCGAN5"><a href="#2-2-DCGAN5" class="headerlink" title="2.2    DCGAN5"></a>2.2    DCGAN<sup><a href="#fn_5" id="reffn_5">5</a></sup></h3><p>DCGAN(Deep Convolutional Generative Adversarial Networks)å…¨ç§°ä¸ºæ·±åº¦å·ç§¯å¯¹æŠ—ç”Ÿæˆç½‘ç»œã€‚é¡¾åæ€ä¹‰ï¼Œå…¶ä¸»è¦æƒ³æ³•æ˜¯å°†å·ç§¯ç¥ç»ç½‘ç»œ(CNN)å’ŒGANè¿›è¡Œç»“åˆï¼Œåœ¨ä¸æ”¹å˜GANçš„åŸºæœ¬åŸç†çš„æƒ…å†µä¸‹è¾ƒä¸ºæœ‰æ•ˆåœ°æ”¹å–„äº†å…¶è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚</p><p>DCGANåšå‡ºçš„ä¸»è¦æ”¹å˜æœ‰ï¼š</p><ul><li><p>ä½¿ç”¨å·ç§¯å±‚å’Œè½¬ç½®å·ç§¯å±‚ï¼šå¼•å…¥äº†è½¬ç½®å·ç§¯å±‚å’Œå·ç§¯å±‚åˆ†åˆ«ä½œä¸ºç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ç½‘ç»œçš„ä¸»è¦ç»„ä»¶ã€‚</p></li><li><p>å»é™¤å…¨è¿æ¥å±‚ï¼šç”¨å…¨å·ç§¯å±‚ä»£æ›¿ã€‚</p></li><li>æ‰¹å½’ä¸€åŒ–(Batch Normalization)ï¼šç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨éƒ½ä½¿ç”¨BNå±‚ã€‚</li><li>ä¿®æ”¹æ¿€æ´»å‡½æ•°ï¼šç”Ÿæˆå™¨è¾“å‡ºå±‚ä½¿ç”¨Tanhï¼Œå…¶ä½™å±‚ä½¿ç”¨ReLUï¼›åˆ¤åˆ«å™¨å‡ä½¿ç”¨LeakyReLU</li></ul><p><img src="https://github.com/daucloud/imagecdn/raw/main/test/202401181914603.png" alt=""></p><center>Figure 4: ç”Ÿæˆå™¨è½¬ç½®å·ç§¯å±‚ç¤ºæ„å›¾</center><h3 id="2-3-WGAN6"><a href="#2-3-WGAN6" class="headerlink" title="2.3    WGAN6"></a>2.3    WGAN<sup><a href="#fn_6" id="reffn_6">6</a></sup></h3><p>WGAN(Wasserstein Generative Adversarial Networks)å¼•å…¥äº†Wassersteinè·ç¦»ä»£æ›¿åŸæ¥çš„JSæ•£åº¦ä½œä¸ºGANçš„æŸå¤±å‡½æ•°ï¼Œå½»åº•è§£å†³äº†GANè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œæ˜¯GANå‘å±•å²ä¸Šé‡Œç¨‹ç¢‘å¼çš„å·¥ä½œä¹‹ä¸€ã€‚</p><p><img src="https://pic1.zhimg.com/80/v2-6be6e2ef3d15c4b10c2a943e9bf4db70_1440w.jpg#width=60%" alt="img" style="zoom:67%;" /></p><center>Figure 5: WGANç®—æ³•</center><p>å¯è§ï¼ŒWGANåšå‡ºçš„æœ€ä¸»è¦æ”¹åŠ¨ï¼Œå³æ˜¯å¯¹æŸå¤±å‡½æ•°çš„æ›´æ¢ã€‚æ­¤å¤–ï¼Œå…¶è¿˜åšå‡ºäº†å¦‚ä¸‹æ”¹å˜ï¼š</p><ul><li>åˆ¤åˆ«å™¨æœ€åä¸€å±‚å»æ‰sigmoid</li><li>æ¯æ¬¡æ›´æ–°åˆ¤åˆ«å™¨çš„å‚æ•°ä¹‹åæŠŠå®ƒä»¬çš„ç»å¯¹å€¼æˆªæ–­åˆ°ä¸è¶…è¿‡ä¸€ä¸ªå›ºå®šå¸¸æ•°c</li><li>ä¸è¦ç”¨åŸºäºåŠ¨é‡çš„ä¼˜åŒ–ç®—æ³•ï¼ˆåŒ…æ‹¬momentumå’ŒAdamï¼‰ï¼Œæ¨èRMSPropï¼ŒSGDä¹Ÿè¡Œ</li></ul><p>æ”¹è¿›è™½ç„¶ç®€å•ï¼Œä½†æ˜¯æˆæ•ˆå·¨å¤§ã€‚</p><h2 id="3-åº”ç”¨ä¸¾ä¾‹"><a href="#3-åº”ç”¨ä¸¾ä¾‹" class="headerlink" title="3    åº”ç”¨ä¸¾ä¾‹"></a>3    åº”ç”¨ä¸¾ä¾‹</h2><h3 id="3-1-ã€ŒEdmond-de-Belamyã€"><a href="#3-1-ã€ŒEdmond-de-Belamyã€" class="headerlink" title="3.1    ã€ŒEdmond de Belamyã€"></a>3.1    ã€ŒEdmond de Belamyã€</h3><p>åœ¨2018å¹´æœ«çš„ä½³å£«å¾—çº½çº¦æ‹å–åœºä¸Šï¼Œæ¥è‡ªå·´é»çš„è‰ºæœ¯å›¢é˜Ÿ<em>Obvious</em>ä½¿ç”¨GANç”Ÿæˆçš„ç”»ä½œã€ŒEdmond de Belamyã€ä»¥è¶…å‡ºä¼°ä»·40å€çš„43.25ä¸‡ç¾å…ƒæˆäº¤ï¼Œå…¶å³ä¸‹è§’ä¾¿å°æœ‰çºµæ¨ªGANé¢†åŸŸçš„è‘—åå…¬å¼ï¼š <script type="math/tex">\min_G\max_D \mathbb{E}_{x}[\log D(x)]+\mathbb{E}_{z}[\log(1-D(G(z)))]</script></p><p>1968å¹´ï¼Œæ¯•åŠ ç´¢æ›¾è¯´ï¼šâ€è®¡ç®—æœºæ˜¯æ²¡æœ‰ç”¨çš„ã€‚å®ƒä»¬åªä¼šå‘Šè¯‰ä½ ç­”æ¡ˆâ€ã€‚ä½†åœ¨åŒä¸€åœºæ‹å–ä¼šä¸Šï¼Œæ²¡æœ‰ä¸€å¹…æ¯•åŠ ç´¢çš„ç”»ä½œæˆäº¤ä»·æ ¼è¶…è¿‡äº†ã€ŒEdmond de Belamyã€ï¼Œè¿™ä¸ç¦ä»¤äººå”å˜˜ä¸å·²ã€‚</p><p><img src="https://github.com/daucloud/imagecdn/raw/main/test/202401181922436.png" alt=""></p><center>Figure 6: ã€ŒEdmond de Belamyã€</center><h3 id="3-2-This-Person-Does-Not-Exist"><a href="#3-2-This-Person-Does-Not-Exist" class="headerlink" title="3.2    This Person Does Not Exist"></a>3.2    This Person Does Not Exist</h3><p><a href="https://thispersondoesnotexist.com/" target="_blank" rel="noopener">thispersondoesnotexist.com </a>æ¯æ¬¡è¿›å…¥è¯¥ç½‘å€ï¼Œéƒ½ä¼šç”Ÿæˆä¸€å¼ ä¸–ç•Œä¸Šå¹¶ä¸å­˜åœ¨çš„äººè„¸ï¼Œè€Œè¿™æ­£æ˜¯ä½¿ç”¨GANè¿›è¡Œç”Ÿæˆçš„ã€‚</p><p><img src="https://github.com/daucloud/imagecdn/raw/main/test/202401181917230.png" alt=""></p><center>Figure 7: This Person Does Not Exist</center><h3 id="3-3-äºŒæ¬¡å…ƒå¤´åƒç”Ÿæˆ7"><a href="#3-3-äºŒæ¬¡å…ƒå¤´åƒç”Ÿæˆ7" class="headerlink" title="3.3    äºŒæ¬¡å…ƒå¤´åƒç”Ÿæˆ7"></a>3.3    äºŒæ¬¡å…ƒå¤´åƒç”Ÿæˆ<sup><a href="#fn_7" id="reffn_7">7</a></sup></h3><p>ä½¿ç”¨GANï¼Œä½ å¯ä»¥éšå¿ƒæ‰€æ¬²ç”ŸæˆäºŒæ¬¡å…ƒ<del>è€å©†</del>å¤´åƒï¼š</p><p><img src="https://github.com/daucloud/imagecdn/raw/main/test/202401181917189.png" alt=""></p><center>Figure 8: GANç”Ÿæˆçš„äºŒæ¬¡å…ƒå¤´åƒ</center><h2 id="4-æ€»ç»“"><a href="#4-æ€»ç»“" class="headerlink" title="4    æ€»ç»“"></a>4    æ€»ç»“</h2><p>GANæ˜¯ä¸€ç§åº”ç”¨å¹¿æ³›ã€æ½œåŠ›å·¨å¤§çš„ç”Ÿæˆæ¨¡å‹ã€‚å®ƒä½¿ç”¨åˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨è¿›è¡Œå¯¹æŠ—æ€§è®­ç»ƒï¼Œå¹¶æœ€ç»ˆäº§ç”Ÿè¶³å¤Ÿé€¼çœŸçš„å›¾åƒã€‚ä½†GANåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šå¸¸å­˜åœ¨ç”Ÿæˆæ ·æœ¬è¿‡äºéšæœºã€è®­ç»ƒä¸ç¨³å®šç­‰ç¼ºç‚¹ï¼Œå› æ­¤æ¶Œç°å‡ºäº†è¯¸å¤šå˜ä½“å¯¹å…¶è¿›è¡Œæ”¹è¿›ï¼šCGANã€DCGANã€WGAN â€¦â€¦ </p><p>æˆ‘ä»¬æœŸå¾…åœ¨å°†æ¥èƒ½çœ‹åˆ°æ›´å…·æ½œåŠ›çš„GANæ¨¡å‹å’Œæ›´å¯Œä»·å€¼çš„GANåº”ç”¨ï¼</p><hr><p>Footnotes: </p><p>[1]:Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014, June 10). <em>Generative Adversarial Networks</em>. arXiv.org. <a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">https://arxiv.org/abs/1406.2661</a><br>[2]:äº‹å®ä¸Šï¼Œè¿™ç§ â€œçº³ä»€å¹³è¡¡â€ æ˜¯ä¸€ç§ç†æƒ³çŠ¶æ€ï¼Œå®é™…è®­ç»ƒéš¾ä»¥è¾¾åˆ°ã€‚<br>[3]:å…³äºæ¢¯åº¦ä¸‹é™/ä¸Šå‡æ³•ï¼Œå¯å‚è€ƒ<a href="https://www.zhihu.com/question/305638940/answer/1639782992" target="_blank" rel="noopener">ä»€ä¹ˆæ˜¯æ¢¯åº¦ä¸‹é™æ³•ï¼Ÿ - é©¬åŒå­¦çš„å›ç­” - çŸ¥ä¹</a><br>[4]:Mirza, M., &amp; Osindero, S. (2014, November 6). <em>Conditional generative adversarial nets</em>. arXiv.org. <a href="https://arxiv.org/abs/1411.1784" target="_blank" rel="noopener">https://arxiv.org/abs/1411.1784</a><br>[5]:Radford, A., Metz, L., &amp; Chintala, S. (2016, January 7). <em>Unsupervised representation learning with deep convolutional generative Adversarial Networks</em>. arXiv.org. <a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="noopener">https://arxiv.org/abs/1511.06434</a><br>[6]:<a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="noopener">ä»¤äººæ‹æ¡ˆå«ç»çš„Wasserstein GAN - éƒ‘åæ»¨çš„æ–‡ç«  - çŸ¥ä¹</a><br>[7]:Jin, Y., Zhang, J., Li, M., Tian, Y., Zhu, H., &amp; Fang, Z. (2017, August 18). <em>Towards the automatic anime characters creation with Generative Adversarial Networks</em>. arXiv.org. <a href="https://arxiv.org/abs/1708.05509" target="_blank" rel="noopener">https://arxiv.org/abs/1708.05509</a></p>]]></content>
      
      
      <categories>
          
          <category> cs learning </category>
          
          <category> intro to data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>é›ª</title>
      <link href="/2023/12/11/%E9%9B%AA/"/>
      <url>/2023/12/11/%E9%9B%AA/</url>
      
        <content type="html"><![CDATA[<h1 id="é›ª"><a href="#é›ª" class="headerlink" title="é›ª"></a>é›ª</h1><p>æœ¬æ¥æ˜¯ä¸å–œæ¬¢å†¬å¤©çš„ã€‚</p><p>åŸæ¥é¡¶å–œæ¬¢çš„å­£èŠ‚æ˜¯ç§‹å¤©ã€‚æ¼«æ­¥è¡—å¤´ï¼Œæ€»èƒ½ä¸ä¸€ç‰‡é£˜ç„¶å äºèº«å‰çš„è½å¶ä¸æœŸè€Œé‡ã€‚çˆ±æäº†è¿™ç§æ„æ–™ä¹‹å¤–çš„é‚‚é€…ã€‚</p><p>åæ¥å¬è¿‡lunaçš„<a href="https://www.bilibili.com/video/BV1ka4y1E7ik/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bd539b5a62c295726bece82272cc6c5a" target="_blank" rel="noopener"> ã‚ã®å¤ã®ã„ã¤ã‹ã¯ (åœ¨é‚£å€‹å¤æ—¥çš„æŸå¤©)</a>ï¼Œå¤å¤©åœ¨æˆ‘å¿ƒä¸­çš„åœ°ä½æ¸æ¸å˜å¾—ä¸ç§‹å¤©ä¸åˆ†ä¼¯ä»²äº†ã€‚æ¯æ¬¡çœ‹è¿™é¦–æ›²å­çš„pvï¼Œéƒ½æ·±ä¸ºå¤çš„çƒ­çƒˆæ¿€åŠ¨ä¸å·²ï¼šâ€œæ­£æ˜¯æ— æ•°ä¸ªå¾®å°ç‰‡åˆ»ï¼Œæ±‡é›†åœ¨ä¸€èµ·é€ å°±è¿™çƒ­çƒˆçš„æˆ‘â€ã€‚ç”Ÿå‘½æœ¬è¯¥å¦‚æ­¤ã€‚</p><p>ä½†æ˜¯ï¼Œæåˆ°â€œå†¬â€è¿™ä¸ªå­—ï¼Œæµ®ç°åœ¨è„‘æµ·ä¸­çš„æ€»æ˜¯å‡›å†½çš„é£ã€å‡‹é›¶çš„æ ‘ã€èœ·ç¼©çš„äººâ€¦â€¦è®¨åŒè¿™ç§ä¸‡äº‹ä¸‡ç‰©éƒ½è¢«å‹æŠ‘çš„æ„Ÿè§‰ï¼šç”Ÿå‘½ä¼¼ä¹è¢«ä¸¥é…·çš„å†¬å†°å°äº†èµ·æ¥ï¼Œå¾…åˆ°æ¥å¹´æ˜¥æš–èŠ±å¼€çš„æ—¶èŠ‚ï¼Œæ‰èƒ½é‡ç„•ç”Ÿæœºã€‚å†¬å¤©å°‘äº†ç‚¹çµé­‚ã€‚</p><p>åæ¥æ‰æ¸æ¸å‘ç°æˆ‘é”™äº†ã€‚å†¬ä¸æ˜¯æ²¡æœ‰çµé­‚ï¼›ç›¸åï¼Œå†¬çš„çµé­‚ç”šè‡³æ¯”å…¶ä»–ä¸‰ä¸ªå­£èŠ‚éƒ½æ›´åŠ å¯çˆ±â€”â€”è¿™æ­£æ˜¯é‚£åä¸ºâ€œé›ªâ€çš„ç²¾çµã€‚é›ªæ´ç™½ã€è½»ç›ˆã€å¤çµç²¾æ€ªï¼Œå†¬å¤©çš„æ²‰é—·å› ä¸ºå¥¹çš„åˆ°æ¥ä¸€æ‰«è€Œç©ºã€‚â€œå¿½å¦‚ä¸€å¤œæ˜¥é£æ¥ï¼Œåƒæ ‘ä¸‡æ ‘æ¢¨èŠ±å¼€ã€‚â€è¢«é›ªç‚¹ç¼€åçš„ä¸–ç•Œï¼Œç„•å‘å‡ºä¸äºšäºæ˜¥çš„å‹ƒå‹ƒç”Ÿæœºã€‚äººä»¬ä¸å†èœ·ç¼©åœ¨è¢«çªï¼Œæ ¡å›­é‡Œéšå¤„å¯è§é£èˆçš„é›ªçƒã€å¥‡å½¢æ€ªçŠ¶çš„é›ªäººå’Œåˆ›æ„ç™¾å‡ºçš„é›ªåœ°ä¸Šçš„å›¾æ¡ˆã€‚å†¬å¤©åŸæ¥æ˜¯è¿™æ ·ä¸€ä¸ªè¶£å‘³ç›ç„¶çš„å­£èŠ‚ã€‚</p><p>ç‹¬èº«éª‘è¡Œåœ¨æ¸…åå›­å†…ï¼Œèµ°èµ°åœåœï¼Œåœåœèµ°èµ°ï¼Œä¸€æ—¶å°†è¯¸å¤šddlæŠ›åœ¨è„‘åï¼Œåªè§‰å¿ƒçµä¹Ÿå› è¿™ç™½èŒ«èŒ«çš„é›ªçš„ä¸–ç•Œå˜å¾—ä¸€ç‰‡é€šé€ã€‚ç»ˆäºæ˜ç™½åŸæ¥æ²¡æœ‰ä¸€ä¸ªå­£èŠ‚æ˜¯ä¸å€¼å¾—å–œçˆ±çš„ï¼Œç”Ÿå‘½ä¸­ä¹Ÿæ²¡æœ‰ä¸€ä¸ªæ—¶åˆ»æ˜¯ä¸å€¼å¾—çƒ­çˆ±çš„ã€‚äººæ´»åœ¨ä¸–ï¼Œæœ¬è¯¥å¦‚æ­¤ã€‚</p><p>æœ¬è¯¥è¿™æ ·å–œæ¬¢å†¬å¤©å•Šã€‚</p><p><img src="/img/2023-12-11-é›ª/snow1.jpg" alt="snow1"></p><p><img src="/img/2023-12-11-é›ª/snow2.jpg" alt="snow2"></p><p><img src="/img/2023-12-11-é›ª/snow.jpg" alt="snow"></p>]]></content>
      
      
      <categories>
          
          <category> æ‚ä¸ƒæ‚å…«çš„ä¸œè¥¿ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> éšç¬” </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello,World!</title>
      <link href="/2023/12/09/Hello-World/"/>
      <url>/2023/12/09/Hello-World/</url>
      
        <content type="html"><![CDATA[<p>å»ºå¥½äº†åšå®¢ï¼Œç¬¬ä¸€ä»¶äº‹å½“ç„¶æ˜¯å‘ä¸€ç¯‡ Hello,world! (bushi</p><p><del>å®é™…ä¸Šæ²¡æƒ³å¥½è¦å†™å•¥ï¼Œæ‰€ä»¥éšä¾¿æ°´ä¸€ç¯‡åšå®¢</del></p><p>æ”¾é¦–DTï¼š</p><iframe allow="autoplay *; encrypted-media *; fullscreen *; clipboard-write" frameborder="0" height="175" style="width:100%;max-width:660px;overflow:hidden;border-radius:10px;" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-storage-access-by-user-activation allow-top-navigation-by-user-activation" src="https://embed.music.apple.com/cn/album/%E6%B5%81%E6%B2%99/1416149926?i=1416149940"></iframe>]]></content>
      
      
      <categories>
          
          <category> æ‚ä¸ƒæ‚å…«çš„ä¸œè¥¿ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> éšç¬” </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
